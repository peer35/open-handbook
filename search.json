[
  {
    "objectID": "topics.html",
    "href": "topics.html",
    "title": "Topics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nBAZIS HPC\n\n\n \n\n\n\n\nData Citation\n\n\n \n\n\n\n\nData Collection\n\n\n \n\n\n\n\nData Management Plan (DMP)\n\n\n \n\n\n\n\nData Protection\n\n\n \n\n\n\n\nData Publication\n\n\n \n\n\n\n\nData Storage\n\n\n \n\n\n\n\nFAIR Principles\n\n\n \n\n\n\n\nFinding Existing Data\n\n\n \n\n\n\n\nPersistent identifier\n\n\n \n\n\n\n\nResearch Data Management (RDM)\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "topics/persistent-identifier.html",
    "href": "topics/persistent-identifier.html",
    "title": "Persistent identifier",
    "section": "",
    "text": "Persistent identifier\nA persistent identifier (PID) is a durable reference to a digital dataset document, website or other object. It is a kind of ISBN for digital files. By using a persistent identifier, you make sure that your dataset will be findable well into the future. A DOI or Handle are the commonly used PIDs. The data archiving options at the VU commonly offer DOIs.\nMost data archives or repositories offer a persistent identifier and generate this automatically when research data are archived. For example, this is the case for DataverseNL at the VU. In Yoda at the VU, assigning a PID is possible, but does not happen automatically. Please get in touch with the RDM Support Desk if you have questions about assigning a PID when you archive data in Yoda."
  },
  {
    "objectID": "topics/fair-principles.html",
    "href": "topics/fair-principles.html",
    "title": "FAIR Principles",
    "section": "",
    "text": "This page discusses what the FAIR principles (Wilkinson et al. 2016) are, why they are important and how you can work in line with these principles at VU.\n\n\nThe FAIR principles were formulated in 2016 to guide researchers in increasing the Findability, Accessibility, Interoperability and Reusability of their data (see the publication in the journal Scientific Data and the summary of the principles). The goal is to ensure that scholarly data can be used as widely as possible – accelerating scientific discoveries and benefiting society in the process.\nA lot of good resources exist already that explain the FAIR principles very well: - GO FAIR provides a clear overview of the FAIR principles - The Turing Way has a great information page about FAIR, containing a lot of references to other useful sources - The story A FAIRy tale explains all principles in an understable way\nThe FAIR principles were rapidly adopted by Dutch and European funding agencies. If you receive a research grant from NWO, ZonMw, or the European Commission, you will be asked to make your data FAIR.\n\n\n\nYou do not need to apply all FAIR principles at once to start benefiting from making your data FAIR. Applying even just some of the principles will increase the visibility and impact of your data, leading to: - Increased citations of the datasets themselves and your research - Improved reproducibility of your research - Compliance with funder and publisher requirements\nMaking your data FAIR will also make it possible for you to easily find, access and reuse your own data in the future. You may be the first and most important beneficiary of making your own data FAIR.\n\n\n\n\n\nA DMP is a living document in which you specify what kinds of data you will use in your project, and how you will process, store and archive them. Preparing a data management plan should be your first step in the process to make data FAIR. The DMP template will ask questions that enable you to systematically address the things that need to be done to make your data FAIR. Writing a DMP [link to handbook page about DMPs] is also a requirement from funding agencies and some faculties at the VU. At the VU, you can use DMPonline to create and share DMPs.\n\n\n\nTo be findable, data need to be described with appropriate metadata. Metadata can include keywords, references to related papers, the researchers’ ORCID identifiers, and the codes for the grants that supported the research. You will need to provide such metadata when you are uploading data to a repository (see below). You increase findability by filling out as many metadata fields as possible and by providing rich descriptions in terminology that is common in your field.\nTo be reusable, data need to be accompanied by documentation describing how the data was created, structured, processed, and so on. It is good practice to integrate writing documentation during the research process. It will be easier and take less time compared to when you try to do this at the end. Having documentation on the research process will also help you to redo parts of your data cleaning actions or data analysis if necessary.\nIf you have questions about metadata and documentation, contact the RDM Support Desk and we will be happy to help you and to provide advice.\n\n\n\nIf you choose a repository that: assigns a persistent identifier to both the data and the metadata; attaches metadata to the data according to standard metadata schemas; releases data with a license; and provides access to the data and metadata via an open and standard communication protocol (such as http) – then your data will meet many, if not most, of the FAIR principles.\nThe VU provides three repositories which meets all of these conditions: [link to pages about repositories]\n\nDataverseNL\nYoda - Yoda information page and Yoda publication platform\nOpen Science Framework (OSF)\n\nCosts for using these repositories for datasets up to 500 GB are covered by the faculty. There are costs involved for you department or project if a datasets is larger than 500 GB. See the storage cost model for details.\n\n\n\n\nData do not need to be open to be FAIR. The FAIR principles allow for controlled access, which can be important for certain types of data, such as personal data, medical data, competitive company data. The guiding principle is always that data should be as “as open as possible, as closed as necessary”. If data cannot be openly shared, because they are too sensitive, then “the FAIR approach would be to make the metadata publicly available and provide information about the conditions for accessing the data itself.”"
  },
  {
    "objectID": "topics/fair-principles.html#what-are-the-fair-principles",
    "href": "topics/fair-principles.html#what-are-the-fair-principles",
    "title": "FAIR Principles",
    "section": "",
    "text": "The FAIR principles were formulated in 2016 to guide researchers in increasing the Findability, Accessibility, Interoperability and Reusability of their data (see the publication in the journal Scientific Data and the summary of the principles). The goal is to ensure that scholarly data can be used as widely as possible – accelerating scientific discoveries and benefiting society in the process.\nA lot of good resources exist already that explain the FAIR principles very well: - GO FAIR provides a clear overview of the FAIR principles - The Turing Way has a great information page about FAIR, containing a lot of references to other useful sources - The story A FAIRy tale explains all principles in an understable way\nThe FAIR principles were rapidly adopted by Dutch and European funding agencies. If you receive a research grant from NWO, ZonMw, or the European Commission, you will be asked to make your data FAIR."
  },
  {
    "objectID": "topics/fair-principles.html#how-can-you-benefit-from-working-in-line-with-the-fair-principles",
    "href": "topics/fair-principles.html#how-can-you-benefit-from-working-in-line-with-the-fair-principles",
    "title": "FAIR Principles",
    "section": "",
    "text": "You do not need to apply all FAIR principles at once to start benefiting from making your data FAIR. Applying even just some of the principles will increase the visibility and impact of your data, leading to: - Increased citations of the datasets themselves and your research - Improved reproducibility of your research - Compliance with funder and publisher requirements\nMaking your data FAIR will also make it possible for you to easily find, access and reuse your own data in the future. You may be the first and most important beneficiary of making your own data FAIR."
  },
  {
    "objectID": "topics/fair-principles.html#making-data-fair-how-to-get-started-in-three-easy-steps",
    "href": "topics/fair-principles.html#making-data-fair-how-to-get-started-in-three-easy-steps",
    "title": "FAIR Principles",
    "section": "",
    "text": "A DMP is a living document in which you specify what kinds of data you will use in your project, and how you will process, store and archive them. Preparing a data management plan should be your first step in the process to make data FAIR. The DMP template will ask questions that enable you to systematically address the things that need to be done to make your data FAIR. Writing a DMP [link to handbook page about DMPs] is also a requirement from funding agencies and some faculties at the VU. At the VU, you can use DMPonline to create and share DMPs.\n\n\n\nTo be findable, data need to be described with appropriate metadata. Metadata can include keywords, references to related papers, the researchers’ ORCID identifiers, and the codes for the grants that supported the research. You will need to provide such metadata when you are uploading data to a repository (see below). You increase findability by filling out as many metadata fields as possible and by providing rich descriptions in terminology that is common in your field.\nTo be reusable, data need to be accompanied by documentation describing how the data was created, structured, processed, and so on. It is good practice to integrate writing documentation during the research process. It will be easier and take less time compared to when you try to do this at the end. Having documentation on the research process will also help you to redo parts of your data cleaning actions or data analysis if necessary.\nIf you have questions about metadata and documentation, contact the RDM Support Desk and we will be happy to help you and to provide advice.\n\n\n\nIf you choose a repository that: assigns a persistent identifier to both the data and the metadata; attaches metadata to the data according to standard metadata schemas; releases data with a license; and provides access to the data and metadata via an open and standard communication protocol (such as http) – then your data will meet many, if not most, of the FAIR principles.\nThe VU provides three repositories which meets all of these conditions: [link to pages about repositories]\n\nDataverseNL\nYoda - Yoda information page and Yoda publication platform\nOpen Science Framework (OSF)\n\nCosts for using these repositories for datasets up to 500 GB are covered by the faculty. There are costs involved for you department or project if a datasets is larger than 500 GB. See the storage cost model for details."
  },
  {
    "objectID": "topics/fair-principles.html#what-if-i-cannot-share-my-data",
    "href": "topics/fair-principles.html#what-if-i-cannot-share-my-data",
    "title": "FAIR Principles",
    "section": "",
    "text": "Data do not need to be open to be FAIR. The FAIR principles allow for controlled access, which can be important for certain types of data, such as personal data, medical data, competitive company data. The guiding principle is always that data should be as “as open as possible, as closed as necessary”. If data cannot be openly shared, because they are too sensitive, then “the FAIR approach would be to make the metadata publicly available and provide information about the conditions for accessing the data itself.”"
  },
  {
    "objectID": "topics/data-publication.html",
    "href": "topics/data-publication.html",
    "title": "Data Publication",
    "section": "",
    "text": "Open Access publishing means that you make your publication freely accessible online to everyone without restrictions. The Vrije Universiteit believes that government-funded research should be available free of charge to as many people as possible. To that end, the VU Library offers a guide on how to go about Open Access Publishing.\nOpen Access publishing is one component of Open Science. UNESCO defines Open Science as\n\na set of principles and practices that aim to make scientific research from all fields accessible to everyone for the benefits of scientists and society as a whole. Open science is about making sure not only that scientific knowledge is accessible but also that the production of that knowledge itself is inclusive, equitable and sustainable\n\nThis includes making openly available research data, methods, and documentation where possible. As such, RDM and the practices outlined in this LibGuide are a precondition of Open Science.\nYou can read more about Open Science in the Netherlands on the website of the Nationaal Programma Open Science and join the Open Science Community Amsterdam, the community of VU employees interested in Open Science (joint with the University of Amsterdam and Amsterdam University of Applied Sciences).\n\n\n\nIn addition to archiving research data in a data repository, you may choose to publish an article about your data collection. This is not necessarily common for all disciplines. Some examples of data journals where you can publish your data and dataset, are:\n\nScientific Data - Nature\nGeoscience Data Journal\nGigascience\nDataset Papers in Science\nJournal of Physical and Chemical Reference Data\nEarth System Science Data\nJournal of Open Archaeology Data\nJournal of Open Health Data\nJournal of Open Psychology Data"
  },
  {
    "objectID": "topics/data-publication.html#open-access-and-open-science",
    "href": "topics/data-publication.html#open-access-and-open-science",
    "title": "Data Publication",
    "section": "",
    "text": "Open Access publishing means that you make your publication freely accessible online to everyone without restrictions. The Vrije Universiteit believes that government-funded research should be available free of charge to as many people as possible. To that end, the VU Library offers a guide on how to go about Open Access Publishing.\nOpen Access publishing is one component of Open Science. UNESCO defines Open Science as\n\na set of principles and practices that aim to make scientific research from all fields accessible to everyone for the benefits of scientists and society as a whole. Open science is about making sure not only that scientific knowledge is accessible but also that the production of that knowledge itself is inclusive, equitable and sustainable\n\nThis includes making openly available research data, methods, and documentation where possible. As such, RDM and the practices outlined in this LibGuide are a precondition of Open Science.\nYou can read more about Open Science in the Netherlands on the website of the Nationaal Programma Open Science and join the Open Science Community Amsterdam, the community of VU employees interested in Open Science (joint with the University of Amsterdam and Amsterdam University of Applied Sciences)."
  },
  {
    "objectID": "topics/data-publication.html#publishing-your-data-in-a-data-journal",
    "href": "topics/data-publication.html#publishing-your-data-in-a-data-journal",
    "title": "Data Publication",
    "section": "",
    "text": "In addition to archiving research data in a data repository, you may choose to publish an article about your data collection. This is not necessarily common for all disciplines. Some examples of data journals where you can publish your data and dataset, are:\n\nScientific Data - Nature\nGeoscience Data Journal\nGigascience\nDataset Papers in Science\nJournal of Physical and Chemical Reference Data\nEarth System Science Data\nJournal of Open Archaeology Data\nJournal of Open Health Data\nJournal of Open Psychology Data"
  },
  {
    "objectID": "topics/data-publication.html#licensing-the-data",
    "href": "topics/data-publication.html#licensing-the-data",
    "title": "Data Publication",
    "section": "Licensing the data",
    "text": "Licensing the data\nA data licence agreement is a legal instrument that lets others know what they can and cannot do with your research data (and any documentation. scripts and metadata that are published with the data). It is important to consider what kind of limitations are relevant. An important component can be a guideline on how people should cite the dataset. Other components could be:\n\nCan people make copies or even distribute copies\nWho should be contacted if you need access to re-use data\nEtc.\n\nIn principle, Dataverse allows you to choose your terms of use. Some data repositories require you to use a certain licence if you want to deposit your data with them. At Dryad, for example, all datasets are published under the terms of Creative Commons Zero to minimise legal barriers and to maximise the impact for research and education. Some funders may also require that you publish the data as open data. Open data are data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and share alike (Open Knowledge International definition). If you need help with drawing up license agreements, you can contact the IXA office.\nAdditional websites and tools:\n\nExplanation about copyrights and licences by a professor from Leiden University (English subtitles available)\nThe Guide to Creative Commons for Scholarly Publishing and Educational Resources by NWO, VSNU and the University and Royal Libraries\nDCC how-to guide on licensing research data, a guide that links to the Creative Commons website, where many terms are explained\nOpen Data Commons Public Domain Dedication and License (PDDL)\nEUDAT B2SHARE license selection wizard, which Pawel Kamocki (et al) released under an open source license."
  },
  {
    "objectID": "topics/data-publication.html#licensing-software",
    "href": "topics/data-publication.html#licensing-software",
    "title": "Data Publication",
    "section": "Licensing software",
    "text": "Licensing software\nAs with data, it is important to license any software that you write for your research. Some basic information about software licensing can be found here, but it is also important to keep the following in mind:\n\nFor software, choosing a licence usually happens right when you start developing the software or when you put it in a public repository, rather than when the software is finished and fully baked. This makes sense: immediately when the software appears on a public repository, the software can be found and seen by others. Giving it a licence helps others to figure out what they can and can’t do with it.\nIf you are reusing software or libraries written by someone else, you must stick to the clauses of the licence given to the original software/library;\nIf you used proprietary software to write your code (e.g. SPSS syntax), your options in choosing a licence will probably be limited;\nWhen choosing a licence, do not just think about what others may do with the software, but also what you might want to do with the software in the future.\n\nThere are several tools that can help you to choose a suitable sotware licence. If you have questions or need guidance in choosing a licence for your software, get in touch with the RDM Support Desk."
  },
  {
    "objectID": "topics/data-management-plan.html",
    "href": "topics/data-management-plan.html",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "A Data Management Plan (DMP) is a document outlining how research data will be handled throughout the research lifecycle. A DMP is a structured way to address data collection, organization, storage, sharing, and preservation. It also outlines the measures taken to ensure data security and addresses how data will be preserved and made available for future use.\n\n\n\nVU Amsterdam offers the online tool DMPonline for writing Data Management Plans. DMPonline is a platform that offers a range of templates, ensuring that researchers can create DMPs to meet the standards of diverse funders and institutions associated with their projects. DMPonline makes it easy to work on a DMP together with colleagues, advisors, or other stakeholders. VU Amsterdam researchers can use the request feedback function of DMPonline to get their DMP reviewed by a faculty data steward or RDM Support Desk colleague.\nIf you have questions about DMPonline, or encounter problems when using the tool, please get in touch with rdm@vu.nl.\n\n\n\nVarious templates exist in which you can set up your DMP. We strongly recommend that you use the VU template, which is called VU DMP template 2021 (NWO & ZonMw certified) v1.4. Below you’ll find an explanation of how to access this template. If you need to write a DMP for funding agencies NWO, ZonMw or ERC, you can use the VU template as well.\n\n\nYou can find the VU DMP template in DMPonline. It includes concise guidance on how to complete your DMP.\nYou can select the VU template by taking the following steps (see also the picture below).\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nFor the question on primary funding organisation, select the check box on the right, saying that no funder is associated with your plan.\n\nNote: Follow these steps as well if you receive funding from NWO or ZonMw (see also below).\n\nIf you’re aiming to write a full DMP based on the VU DMP template, please make sure you don’t select the GDPR registration form.\n\n\n\n\nWe recommend researchers to use the VU DMP template whenever possible, especially for researchers who work with personal data. The VU DMP template includes questions that serve as input for the GDPR record of processing activities. This means that when you write a DMP based on the VU DMP template, you simultaneousely comply with the VU requirement to register the personal data you use in your research.\nHowever, it is also possible to use other templates in DMPonline. If your funder or partner organization requires you to use a certain template, it is possible to select that template in DMPonline. Please follow the steps below to select a funder’s template.\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nIn the field under Select the primary funding organisation, start typing the name of your funder and select their template.\n\n\nResearchers who don’t work with personal data and who wish to use another DMP template than the VU template, can also follow the steps above.\n\n\n\n\n\n\nIf your research is subject to the GDPR, then you need to register information on your research in a central VU registry. This central registry lists all personal data processing activities carried out at the VU. The registry indicates why and how personal data are processed, and with whom they are shared. The registry helps the VU demonstrate compliance with the GDPR and in the case of a data breach, the registry helps with monitoring and acting swiftly to inform all relevant stakeholders.\nFor research projects, the VU registers data processing via DMPonline. You can create your registration by logging into DMPonline and following the following instructions:\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nFor the question on primary funding organisation, select the check box on the right, saying that no funder is associated with your plan.\n\n\nOnce you get to the two VU templates, you can fill in the VU DMP template 2021 v1.4 if you need to write a DMP anyway; the information you include in this DMP template will be used for the registry. If you don’t need to write a (new) DMP, you can use the separate VU GDPR registration form for research v1.1. Your faculty’s Privacy Champion can help you with your registration.\nIf your research is primarily led by Amsterdam UMC, location VUmc, your research will be registered using their own separate system.\n\n\n\nIf you use personal data in your research, you should register your data processing activities before you start data collection. If you are not sure whether your research data are subject to the GDPR, contact your faculty’s Privacy Champion. Your privacy champion can also assist you if your research is already running, but has not yet been registered.\n\n\n\n\nResearch data is any information that has been collected, observed, generated or created to validate original research findings. Examples of data could be interview recordings, experiment results, physical measurement, notes from focus group’s meetings, notes from fieldwork, observations captured in photographs, film or audio, text files extracted from a corpus, image of archival items or artworks, scraped websites, responses to survey questions. Algorithms, simulations, code, scripts and software are often also considered as research data. There is also physical data: (biological) samples, collections, artifacts etc.\nAdministrative documents, like informed consent forms and key files should be acknowledged as important elements of research data as well.\n\n\n\nAt the VU, we sometimes use the term ‘Data Assets’. You can think of data assets as small ‘parcels’ of data that can change form or format throughout the research. For example, if you’re sending out surveys for your research, the survey responses are considered a data asset. If, in addition to the surveys, you’re also holding focus groups, the data collected from the focus group are also considered a data asset, separate from the survey results. Most projects will have more than one data asset per data stage. It is common to provide data assets based on the data stage such as raw, processed, or analysed. Raw Data refers to original data collected, Processed Data is data that has undergone some level of transformation or organisation. Processing involves cleaning, formatting, and structuring raw data to make them more understandable and suitable for analysis. Analysed Data usually results from statistical methods, detailed examination or interpretation.\nHere are some examples of data assets in research data management:\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nType of data\nFormat\n\n\n\n\nRaw data\nInterviews\nAudio files\nMP3\n\n\n\nSpectographic analysis\nText files\nCSV\n\n\nProcessed data\nTranscription of interviews\nText files\nDocx\n\n\n\nData spreadsheet\nSPSS files\nSAV\n\n\nAnalysed data\nRegression graphic\nGraph\nPNG\n\n\n\nData table\nWord file\nDocx\n\n\nOther\nPoster presentation\nPowerpoint\nPPS\n\n\n\nProject Website\nHTML\n\n\n\n\nAnalysis code\nText files\nPython\n\n\n\nNote that these data assets also change in the different phases of the research! While the interview data are audio files in the raw stage, they are transcribed and become text files in the processed stage.\n\n\n\nThe VU DMP template consists of seven sections with questions. In DMPonline, there is guidance available for all sections, as well as example answers. When you are writing your DMP, you can consult this information directly in DMPonline. Below we provide references to information and support available for various RDM-related aspects.\n\n\nIf you have questions about working with personal data in research, please get in touch with the Privacy Champion of your faculty. The overview of Privacy Champions can be found on the VU website. Make sure to contact your Privacy Champion in the following situations:\n\nIf you need to carry out a DPIA, or if you’re unsure if you need to do one\nIf you work with special category personal data, or otherwise very sensitive data\nIf you are collaborating with other parties\nIf you need software for which no licence is set up on behalf of the VU\nIf you wish to reuse existing data containing personal data\n\nIt is impossible to provide an overview of tasks to be carried out to ensure compliance with the GDPR that fits all research projects. For that reason, it is important to contact your Privacy Champion. They will be able to identify what needs to be arranged to adhere to the GDPR.\nEthical aspects of research should be addressed in the ethics procedure of your faculty. Each faculty has their own ethics committee. The webpages of all committees are listed below. Please go to the page of the ethics committee of your faculty to find instructions for ethical review procedures for your study.\n\nACTA: ACTA Institutional Review Board (IRB)\nBeta: Research ethics review committee Faculty of Science (BETHCIE)\nFGB: Scientific and Ethical Review Board (VCWE)\nFGW: Ethische Toetsingscommissie Onderzoek (EtCO)\nFSW: Research Ethics Review Committee (RERC)\nRCH (Faculty of Law): Ethics Committee\nSBE: Research Ethics and Integrity\nVUmc: METc (Medical Ethical Review Committee)\n\n\n\n\nAn overview of storage facilities at the VU is available in the Data Storage Finder. You can use this as a starting point to navigate storage solutions.\nIf you have questions about data storage and backup, send an email to rdm@vu.nl.\n\n\n\nIf your research data contains personal data and you’re unsure about which data may be published, please contact your Privacy Champion."
  },
  {
    "objectID": "topics/data-management-plan.html#what-is-a-dmp",
    "href": "topics/data-management-plan.html#what-is-a-dmp",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "A Data Management Plan (DMP) is a document outlining how research data will be handled throughout the research lifecycle. A DMP is a structured way to address data collection, organization, storage, sharing, and preservation. It also outlines the measures taken to ensure data security and addresses how data will be preserved and made available for future use."
  },
  {
    "objectID": "topics/data-management-plan.html#dmponline",
    "href": "topics/data-management-plan.html#dmponline",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "VU Amsterdam offers the online tool DMPonline for writing Data Management Plans. DMPonline is a platform that offers a range of templates, ensuring that researchers can create DMPs to meet the standards of diverse funders and institutions associated with their projects. DMPonline makes it easy to work on a DMP together with colleagues, advisors, or other stakeholders. VU Amsterdam researchers can use the request feedback function of DMPonline to get their DMP reviewed by a faculty data steward or RDM Support Desk colleague.\nIf you have questions about DMPonline, or encounter problems when using the tool, please get in touch with rdm@vu.nl."
  },
  {
    "objectID": "topics/data-management-plan.html#choosing-the-right-template",
    "href": "topics/data-management-plan.html#choosing-the-right-template",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "Various templates exist in which you can set up your DMP. We strongly recommend that you use the VU template, which is called VU DMP template 2021 (NWO & ZonMw certified) v1.4. Below you’ll find an explanation of how to access this template. If you need to write a DMP for funding agencies NWO, ZonMw or ERC, you can use the VU template as well.\n\n\nYou can find the VU DMP template in DMPonline. It includes concise guidance on how to complete your DMP.\nYou can select the VU template by taking the following steps (see also the picture below).\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nFor the question on primary funding organisation, select the check box on the right, saying that no funder is associated with your plan.\n\nNote: Follow these steps as well if you receive funding from NWO or ZonMw (see also below).\n\nIf you’re aiming to write a full DMP based on the VU DMP template, please make sure you don’t select the GDPR registration form.\n\n\n\n\nWe recommend researchers to use the VU DMP template whenever possible, especially for researchers who work with personal data. The VU DMP template includes questions that serve as input for the GDPR record of processing activities. This means that when you write a DMP based on the VU DMP template, you simultaneousely comply with the VU requirement to register the personal data you use in your research.\nHowever, it is also possible to use other templates in DMPonline. If your funder or partner organization requires you to use a certain template, it is possible to select that template in DMPonline. Please follow the steps below to select a funder’s template.\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nIn the field under Select the primary funding organisation, start typing the name of your funder and select their template.\n\n\nResearchers who don’t work with personal data and who wish to use another DMP template than the VU template, can also follow the steps above."
  },
  {
    "objectID": "topics/data-management-plan.html#register-your-processing-activities",
    "href": "topics/data-management-plan.html#register-your-processing-activities",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "If your research is subject to the GDPR, then you need to register information on your research in a central VU registry. This central registry lists all personal data processing activities carried out at the VU. The registry indicates why and how personal data are processed, and with whom they are shared. The registry helps the VU demonstrate compliance with the GDPR and in the case of a data breach, the registry helps with monitoring and acting swiftly to inform all relevant stakeholders.\nFor research projects, the VU registers data processing via DMPonline. You can create your registration by logging into DMPonline and following the following instructions:\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nFor the question on primary funding organisation, select the check box on the right, saying that no funder is associated with your plan.\n\n\nOnce you get to the two VU templates, you can fill in the VU DMP template 2021 v1.4 if you need to write a DMP anyway; the information you include in this DMP template will be used for the registry. If you don’t need to write a (new) DMP, you can use the separate VU GDPR registration form for research v1.1. Your faculty’s Privacy Champion can help you with your registration.\nIf your research is primarily led by Amsterdam UMC, location VUmc, your research will be registered using their own separate system.\n\n\n\nIf you use personal data in your research, you should register your data processing activities before you start data collection. If you are not sure whether your research data are subject to the GDPR, contact your faculty’s Privacy Champion. Your privacy champion can also assist you if your research is already running, but has not yet been registered."
  },
  {
    "objectID": "topics/data-management-plan.html#what-is-data",
    "href": "topics/data-management-plan.html#what-is-data",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "Research data is any information that has been collected, observed, generated or created to validate original research findings. Examples of data could be interview recordings, experiment results, physical measurement, notes from focus group’s meetings, notes from fieldwork, observations captured in photographs, film or audio, text files extracted from a corpus, image of archival items or artworks, scraped websites, responses to survey questions. Algorithms, simulations, code, scripts and software are often also considered as research data. There is also physical data: (biological) samples, collections, artifacts etc.\nAdministrative documents, like informed consent forms and key files should be acknowledged as important elements of research data as well."
  },
  {
    "objectID": "topics/data-management-plan.html#data-assets",
    "href": "topics/data-management-plan.html#data-assets",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "At the VU, we sometimes use the term ‘Data Assets’. You can think of data assets as small ‘parcels’ of data that can change form or format throughout the research. For example, if you’re sending out surveys for your research, the survey responses are considered a data asset. If, in addition to the surveys, you’re also holding focus groups, the data collected from the focus group are also considered a data asset, separate from the survey results. Most projects will have more than one data asset per data stage. It is common to provide data assets based on the data stage such as raw, processed, or analysed. Raw Data refers to original data collected, Processed Data is data that has undergone some level of transformation or organisation. Processing involves cleaning, formatting, and structuring raw data to make them more understandable and suitable for analysis. Analysed Data usually results from statistical methods, detailed examination or interpretation.\nHere are some examples of data assets in research data management:\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nType of data\nFormat\n\n\n\n\nRaw data\nInterviews\nAudio files\nMP3\n\n\n\nSpectographic analysis\nText files\nCSV\n\n\nProcessed data\nTranscription of interviews\nText files\nDocx\n\n\n\nData spreadsheet\nSPSS files\nSAV\n\n\nAnalysed data\nRegression graphic\nGraph\nPNG\n\n\n\nData table\nWord file\nDocx\n\n\nOther\nPoster presentation\nPowerpoint\nPPS\n\n\n\nProject Website\nHTML\n\n\n\n\nAnalysis code\nText files\nPython\n\n\n\nNote that these data assets also change in the different phases of the research! While the interview data are audio files in the raw stage, they are transcribed and become text files in the processed stage."
  },
  {
    "objectID": "topics/data-management-plan.html#dmp-elements",
    "href": "topics/data-management-plan.html#dmp-elements",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "The VU DMP template consists of seven sections with questions. In DMPonline, there is guidance available for all sections, as well as example answers. When you are writing your DMP, you can consult this information directly in DMPonline. Below we provide references to information and support available for various RDM-related aspects.\n\n\nIf you have questions about working with personal data in research, please get in touch with the Privacy Champion of your faculty. The overview of Privacy Champions can be found on the VU website. Make sure to contact your Privacy Champion in the following situations:\n\nIf you need to carry out a DPIA, or if you’re unsure if you need to do one\nIf you work with special category personal data, or otherwise very sensitive data\nIf you are collaborating with other parties\nIf you need software for which no licence is set up on behalf of the VU\nIf you wish to reuse existing data containing personal data\n\nIt is impossible to provide an overview of tasks to be carried out to ensure compliance with the GDPR that fits all research projects. For that reason, it is important to contact your Privacy Champion. They will be able to identify what needs to be arranged to adhere to the GDPR.\nEthical aspects of research should be addressed in the ethics procedure of your faculty. Each faculty has their own ethics committee. The webpages of all committees are listed below. Please go to the page of the ethics committee of your faculty to find instructions for ethical review procedures for your study.\n\nACTA: ACTA Institutional Review Board (IRB)\nBeta: Research ethics review committee Faculty of Science (BETHCIE)\nFGB: Scientific and Ethical Review Board (VCWE)\nFGW: Ethische Toetsingscommissie Onderzoek (EtCO)\nFSW: Research Ethics Review Committee (RERC)\nRCH (Faculty of Law): Ethics Committee\nSBE: Research Ethics and Integrity\nVUmc: METc (Medical Ethical Review Committee)\n\n\n\n\nAn overview of storage facilities at the VU is available in the Data Storage Finder. You can use this as a starting point to navigate storage solutions.\nIf you have questions about data storage and backup, send an email to rdm@vu.nl.\n\n\n\nIf your research data contains personal data and you’re unsure about which data may be published, please contact your Privacy Champion."
  },
  {
    "objectID": "topics/data-citation.html",
    "href": "topics/data-citation.html",
    "title": "{{< var title >}}",
    "section": "",
    "text": "Citing data is not different from citing a publication. Make sure to check the rules of the journal to know how you should cite when writing an article for a specific academic journal. For all of the journals, however, the minimum compulsory elements in a data citation include:\n\nAuthor(s): Name of the author (creator) of the dataset\nTitle: Name of the dataset\nDate of publication\nPublisher: Archive where dataset is stored\nPersistent Identifier: Unique identifier, most common is the DOI (see section Data Publication).\n\nOptional elements that may be included in the reference are:\n\nFile Type: Codebook, movie, software\nVersion: Version number of the edition\nCreation Date\nDate of Consultation (last)\n\nExample of a data citation:\nStephens, William, 2020, “Resiliences to Radicalisation - QSort Data”, https://doi.org/10.34894/35MTMN, DataverseNL, V1.\nFor more information, see the following guidelines:\n\nDataverse\nDataCite\nDCC UK\nData Citation Synthesis Group (2014). Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11"
  },
  {
    "objectID": "topics/data-citation.html#data-citation",
    "href": "topics/data-citation.html#data-citation",
    "title": "{{< var title >}}",
    "section": "",
    "text": "Citing data is not different from citing a publication. Make sure to check the rules of the journal to know how you should cite when writing an article for a specific academic journal. For all of the journals, however, the minimum compulsory elements in a data citation include:\n\nAuthor(s): Name of the author (creator) of the dataset\nTitle: Name of the dataset\nDate of publication\nPublisher: Archive where dataset is stored\nPersistent Identifier: Unique identifier, most common is the DOI (see section Data Publication).\n\nOptional elements that may be included in the reference are:\n\nFile Type: Codebook, movie, software\nVersion: Version number of the edition\nCreation Date\nDate of Consultation (last)\n\nExample of a data citation:\nStephens, William, 2020, “Resiliences to Radicalisation - QSort Data”, https://doi.org/10.34894/35MTMN, DataverseNL, V1.\nFor more information, see the following guidelines:\n\nDataverse\nDataCite\nDCC UK\nData Citation Synthesis Group (2014). Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11"
  },
  {
    "objectID": "pathways.html",
    "href": "pathways.html",
    "title": "Pathways",
    "section": "",
    "text": "Collect & Store Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData lifecycle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscover & Initiate Research\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocument & Preserve Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlan & Design\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcess & Analyse Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPublish & Share Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pathways/process-and-analyze.html",
    "href": "pathways/process-and-analyze.html",
    "title": "Process & Analyse Data",
    "section": "",
    "text": "Provenance describes the origin of an object. Data provenance refers to the knowledge of where data originate, where they were collected, by whom, for what reason, and similar aspects that help to understand how the data were originally gathered, processed and altered. In daily use, the term “data provenance” refers to a record trail that accounts for the origin of a piece of data (in a database, document or repository) together with an explanation of how and why it got to the present place (Encyclopedia of Database Systems, pp 608-608). You can also call it the process of keeping records of changes in the data. The need for Data Provenance increases as the reuse of datasets becomes more common in research. The term was originally mostly used in relation to works of art, but is now used in similar senses in a wide range of fields (Wikipedia).\n\nResearchers regularly use a lab notebook or a journal to document their hypotheses, experiments and initial analysis or interpretation of these experiments. If you manually change data in a dataset, this should also be documented. Sometimes records of changes in data can be kept by adding notes to programmes or scripts that are used.\n\nElectronic Lab Journals or Electronic Lab Notebooks are used to meticulously describe and document the process of analysis. Mostly used used in a laboratory environment,; biolab, chemical lab, etc.\nFor computational analyses, Computational Notebooks like Jupyter notebook are used, where you can describe the analysis steps alongside the computer code in different languages like Python, R, Spark, etc. It is important to document steps and changes in your code by writing comments. This way, others and future you can understand how your code works.\nThe Open Science Framework connects different storage types you already use (SURFdrive, Dataverse, etc) and logs automatically all changes of all the steps you make while you progress. With the fine grained history-log and version control system of OSF, you can see all steps you made. You can store and archive the whole provenance trail for citable reproducibility.\n\nFinally, when a dataset contains personal data, data provenance can help researchers to understand the specifics and the context in which the data were gathered, also to be able to assess whether or not the informed consent given for the first research, is applicable.\nFor every step of your data analysis, good data documentation is necessary."
  },
  {
    "objectID": "pathways/process-and-analyze.html#data-provenance",
    "href": "pathways/process-and-analyze.html#data-provenance",
    "title": "Process & Analyse Data",
    "section": "",
    "text": "Provenance describes the origin of an object. Data provenance refers to the knowledge of where data originate, where they were collected, by whom, for what reason, and similar aspects that help to understand how the data were originally gathered, processed and altered. In daily use, the term “data provenance” refers to a record trail that accounts for the origin of a piece of data (in a database, document or repository) together with an explanation of how and why it got to the present place (Encyclopedia of Database Systems, pp 608-608). You can also call it the process of keeping records of changes in the data. The need for Data Provenance increases as the reuse of datasets becomes more common in research. The term was originally mostly used in relation to works of art, but is now used in similar senses in a wide range of fields (Wikipedia).\n\nResearchers regularly use a lab notebook or a journal to document their hypotheses, experiments and initial analysis or interpretation of these experiments. If you manually change data in a dataset, this should also be documented. Sometimes records of changes in data can be kept by adding notes to programmes or scripts that are used.\n\nElectronic Lab Journals or Electronic Lab Notebooks are used to meticulously describe and document the process of analysis. Mostly used used in a laboratory environment,; biolab, chemical lab, etc.\nFor computational analyses, Computational Notebooks like Jupyter notebook are used, where you can describe the analysis steps alongside the computer code in different languages like Python, R, Spark, etc. It is important to document steps and changes in your code by writing comments. This way, others and future you can understand how your code works.\nThe Open Science Framework connects different storage types you already use (SURFdrive, Dataverse, etc) and logs automatically all changes of all the steps you make while you progress. With the fine grained history-log and version control system of OSF, you can see all steps you made. You can store and archive the whole provenance trail for citable reproducibility.\n\nFinally, when a dataset contains personal data, data provenance can help researchers to understand the specifics and the context in which the data were gathered, also to be able to assess whether or not the informed consent given for the first research, is applicable.\nFor every step of your data analysis, good data documentation is necessary."
  },
  {
    "objectID": "pathways/process-and-analyze.html#data-processing",
    "href": "pathways/process-and-analyze.html#data-processing",
    "title": "Process & Analyse Data",
    "section": "Data processing",
    "text": "Data processing\n\nData cleaning\nThe process of detecting and correcting (or removing) corrupt or inaccurate information or records, is called data cleaning. In essence, it refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting this data (Wikipedia). Depending on the type of analysis that is done, different pieces of software can be used to do this data cleaning. More often than not, the same software can also be used to perform the analysis. Licensed software may sometimes also be installed on personal computers or laptops.\n\nSoftware especially designed to clean re-used data is OpenRefine. It cleans starting and trailing blank spaces in cell field, clusters values based on similarities (e.g. in free text fields: Alphen a/d Rhijn, alfen ad rijn, etc. can be easily clustered), normalise data fields into one standard, etc. See below for several tutorials.\nIn some cases, researchers write their own scripts (in programming languages such as Python, R or SQL) to clean data, in which case the process must be documented. Researchers should include their scripts when they archive the datasets to allow for replication and verification.\nExtra background information:\n\nEMGO Quality Handbook on data cleaning\nMaking sense of data I: a practical guide to exploratory data analysis and data mining / Glenn J. Myatt, Wayne P. Johnson, 2014 (eBook)\nOpen Refine\n\nData Carpentry Open Refine website\nTutorial by the Programming Historian\nTutorial by Digitalnomad\nIntroduction to Digital Humanities with Open Refine\n\n\nFor every step of your data cleaning, good documentation and clarifying the data provenance is necessary.\n\n\nData transcription\nIt is common in many fields to hold interviews, focus group sessions, or make other observations that were recorded - video or audio. If indeed you have done so, and you need to have the text transcribed, there are several ways to do this. One option is to do this by hand, although this is very time-consuming.\nAnother option is to pay a transcription service to make the transcription or to use specialised software. The VU has drawn up processing agreements with one transcription service, Transcript Online, and one transcription software service, Amberscript.\nYou can find more information on the VU Library page on what these transcription options do, how they work, how much they cost, and how they can be used.\n\n\nAnonymisation/Pseudonymisation\nProcessing of personal data requires you as a researcher to make sure that any personal data collected from a human subject is according to the EU GDPR regulation. Anonymisation and Pseudonymisation are two ways to make personal data less easy to identify, in other words, it allows you to de-identify personal data.\nThere are various online tools that may help facilitate these processes. The VU has therefore recommended Amnesia as one of the tools to assist in the anonysmisation/pseudonymistaion of data.\nVU Amsterdam is preparing a decision guide on anonymisation and pseudonymisation.\nYou can find more information here on how Amnesia works, how it can be used and how you can make your data compliant with the GDPR regulations."
  },
  {
    "objectID": "pathways/process-and-analyze.html#data-analysis",
    "href": "pathways/process-and-analyze.html#data-analysis",
    "title": "Process & Analyse Data",
    "section": "Data analysis",
    "text": "Data analysis\n\nData Analysis\nAlthough data analysis is an ongoing process throughout the research project, this page focuses on the analysis of the data subsequent to its collection. To ensure that research is empirical and verifiable, it is crucial that researchers keep records (data documentation) of every step made during the data analysis.\nData analysis converts raw/processed data into information that is useful for understanding. Many steps may be required to gain useful information from raw data. The process of processing and analysing data may require computing power not readily available or specific storage and protection options. If multiple parties are involved in the analysis, data sharing may also be necessary.\nData analysis often requires the use of specialised software.The software offered and licensed by the university currently includes: Stata, SPSS, and Atlas.TI. Some of the software is available for download at: download.vu.nl. For open software, see below.\nIn some cases researchers write their own scripts to analyse the data. At the VU, most scripts are written in R, Python and SQL.\nIf you want to read up on data analysis you should check out what journal articles and books the VU library has available on the subject:\n\nAll sources: Data analysis\nQuantitative data analysis\nQualitative data analysis\nBig data\nData mining\n\n\n\nOpen Software\nUsing open software increases the Accessiblity, Interoperability and Reusability of your data. For that reason, we recommend that you use open software as much as possible for your data analysis. This could be software, code or scripts that you have written yourself - where possible, please make this software public, so your analysis is reproducible. Examples of open software are R and Python, which can be used instead of proprietary, commercial software such as SPSS and Matlab.\nResearchers often write their software themselves. There are also organisations that specialise in writing research software, such as the eScience Center. The eScience Center offers the software they built for free use online. Their software is tagged with a DOI and stored in Zenodo as well as GitHub.\nIf you use software for analysing personal or otherwise sensitive data, you need a processing agreement with the developer if the software does not run locally. You can contact your Privacy Champion if you are not sure if you need one, and for help to set up a processing agreement.\nThere are several ways in which to start using open software:\n\nFor Python: you should install Anaconda and launch the Jupyter Notebook from the Navigator.\nFor R: you should install Anaconda and launch R Studio from the Navigator.\nUse the Software Carpentries to learn the basics of programming in Python and R and version control with Git\nRead the recommendations for FAIR Software.\n\nThe VU has several research groups that offer their code online. You can find them here:\n\nThe Systems Bioinformatics research group, on GitHub\nThe Computational Lexicology & Terminology Lab, on GitHub\nThe course Python for Text Analysis, on GitHub\nVU RDM Tech IT group, on GitHub\nA list of RDM tools, on GitHub\n\n\n\nCompute services\n\nIf your pc or laptop takes too much time performing your analysis, it is time to scale up to a higher level. There are several options for employees and students who require more computing power than their own desktop or laptop can provide.\nSeveral options are detailed below. Contact IT for Research for advice on which solution could best fit your workflow\n\nHigh Performance Computing\n\nRoughly speaking, you should try to get access to the HPC when you need to stick a post-it on your laptop or PC that says: “do not touch, analysis ongoing”. Or when you want to run analyses parallel to each other, because they take too long. It is important to consider such a situation at the very beginning of your research or when writing your Data Management Plan: is it conceivable that your dataset will become so large or your analysis so complicated that you will need HPC? Please note that this can occur for any discipline and any sort of data, qualitative and quantitative. If you may need HPC, you also need to reconsider your analysis methods. Programmes like SPSS and Excel do not run well on a HPC, and you would need to (learn to) write scripts in R or Python . If you want to know if using HPC may be necessary or useful for your project, you can contact IT for Research to ask for more information (select the “Onderzoek service domain”).\n\n\nSURF Snellius Compute Cluster\nSnellius is the Dutch National supercomputer hosted at SURF. The system facilitates scientific research carried out in many Universities, independent research institutes, governmental organizations, and private companies in the Netherlands.\nIt’s a service comprising a wide range of resources, compilers and, such as R statistics and MATLAB, and libraries. SURF continually adjusts the service to the needs of the user community. For example, Snellius Compute Cluster includes accelerators (very fast processors),high memory nodes and GPU nodes. \nYou can find more information on the SURF Snellius Wiki.\n\n\nBAZIS Compute Cluster\nIT for Research (ITvO) offers access to your own Linux computational cluster at the VU. BAZIS is a managed service for high performance computing (HPC). Research groups can add their own compute server hardware to BAZIS, ITvO will take care of configuring and maintaining the software stack on your servers.\nBAZIS also has several “community” nodes for use by all VU researchers, sponsored by the VU HPC Council.\nBAZIS is connected to SciStor  providing easy access to your research data and analysis result.\n\n\nVU JupyterHub for education\nIf you are not yet ready to take the leap to cluster computing and work with Python consider JupyterHub. VU IT has built a Jupyter Notebook environment meant mainly for Education purposes, but accessible for researchers as well on https://hub.compute.vu.nl/\n\n\n(Virtual) servers\nThere are also several options to run applications in a server environment. This is useful if for example you use software that does not work on HPC, you want to run a web service, you want to create a research environment for your project. There are several options available for researchers.\n\nSciCloud\nIT for Research (ITvO) offers a virtual server environment where you can run your own server (Linux or Windows). ITvO installs the basic operating system and you are free to install needed software. Web services can be made accessible on the internet. You can find more information and a request form on the VU service portal\n\n\nSURF Research Cloud\nSURF also offers a virtual server environment. Several environments with pre-installed software can easily be installed from a catalog. Find more information on the (SURF wiki)[https://servicedesk.surf.nl/wiki/display/WIKI/SURF+Research+Cloud]\n\n\nDedicated hardware\nSometimes your workload needs dedicated hardware. ITvO offers the option to host your own server hardware in our on-campus data center. Please Contact IT for Research to discuss possibilities."
  },
  {
    "objectID": "pathways/document-and-preserve.html",
    "href": "pathways/document-and-preserve.html",
    "title": "Document & Preserve Data",
    "section": "",
    "text": "There is a difference between storing and archiving data. Storing refers to putting the data in a safe location while the research is ongoing. Because you are still working on the data, the data still change from time to time: they are cleaned, and analysed, and this analysis generates output. As the image below illustrates, storing could be like cooking a dish: you are cleaning and combining ingredients.\nArchiving, on the other hand, refers to putting the data in a safe place after the research is finished. The data are in a fixed state, they don’t change anymore. Archiving is done for verification purposes: so others can check that your research is sound. Or: it is done so that others can reuse the resulting dataset. There is also a difference between archiving and publishing, but in essence, archiving and publishing happen at a similar moment and for both, data do not change anymore.\n\nThis illustration is created by Scriberia with The Turing Way community. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807\n\n\n\nThere are various reasons to archive your data: replication, longitudinal research, data being unique or expensive to collect, re-usability and acceleration of research inside or outside your own discipline. It is VU policy to archive your data for (at least) 10 years after the last publication based on the dataset. Part of preparing your dataset for archiving is appraising and selecting your data.\n\n\nDuring your research you may accumulate a lot of data, some of which will be eligible for archiving. It is impossible to preserve all data infinitely. Archiving all digital data leads to high costs for storage itself and for maintaining and managing this ever-growing volume of data and their metadata; it may also lead to decline in discoverability (see the website of the Digital Curation Centre). For those reasons, it is crucial that you make a selection.\n\n\n\nSelecting data means making choices about what to keep for the long term, and what data to archive securely and what data to publish openly. This means that you have to decide whether your dataset contains data that need to be removed or separated. Reasons to exclude data from publishing include (but are not limited to):\n\ndata are redundant\ndata concern temporary byproducts which are irrelevant for future use\ndata contain material that is sensitive, for example personal data in the sense of the GDPR, like consent forms, voice recordings, DNA data; state secrets; data that are sensitive to competition in a commercial sense. These data need to be separated from other data and archived securely\npreserving data for the long term is in breach of contractual arrangements with your consortium partners or other parties involved\n\nIn preparing your dataset for archiving, the first step is to determine which parts of your data are sensitive, which can then be separated from the other data. Redundant data can be removed altogether.\n\n\n\nOnce you have separated the sensitive data from the rest of your dataset, you have to think about what to do with these sensitive materials. In some cases they may be destroyed, but you may also opt for archiving multiple datasets. For example, you may want to archive your dataset in more than one form depending on the purpose. For example:\n\nOne for reusability to share, and\nA second one that contains the sensitive data, and needs to be handled differently.\n\nFor the first, the non-sensitive data can be stored in an archive under restricted or open access conditions, so that you can share it and link it to publications. For the second, you need to make a separate selection, so the sensitive part can be stored safely in a secure archive (a so-called offline or dark archive). In the metadata of both archives you can create stable links between the two datasets using persistent identifiers.\n\n\n\nThere are several factors that determine what data to select for archiving. For example, whether data are unique, expensive to reproduce, or if your funder requires that you make your data publicly available. This might also help you or your department to think about a standard policy or procedures for what needs to be kept, what is vital for reproducing research or reuse in future research projects.\nMore information on selecting data:\n\nTjalsma, H. & Rombouts, J. (2011). Selection of research data: Guidelines for appraising and selecting research data. Data Archiving and Networked Services (DANS).\nDigital Curation Centre (DCC): Whyte, A. & Wilson, A. (2010). How to appraise and select research data for curation. DCC How-to Guides. Edinburgh: Digital Curation Centre.\nResearch Data Netherlands: Data selection.\n\n\n\n\n\nA dataset consists of the following documents:\n\nRaw or cleaned data (if the cleaned data has been archived, the provenance documentation is also required)\nProject documentation\nCodebook or protocol\nLogbook or lab journal (when available, dependent on the discipline)\nSoftware (& version) needed to open the files when no preferred formats for the data can be provided\n\nSee the section Metadata for more information about documenting your data.\nDepending on the research project it may be that more than one dataset is stored in more than one repository. Make sure that each consortium partner that collects data also stores all necessary data that is required for transparency and verification. A Consortium Agreement and Data Management Plan will include information on who is responsible for archiving the data."
  },
  {
    "objectID": "pathways/document-and-preserve.html#selecting-data",
    "href": "pathways/document-and-preserve.html#selecting-data",
    "title": "Document & Preserve Data",
    "section": "",
    "text": "There is a difference between storing and archiving data. Storing refers to putting the data in a safe location while the research is ongoing. Because you are still working on the data, the data still change from time to time: they are cleaned, and analysed, and this analysis generates output. As the image below illustrates, storing could be like cooking a dish: you are cleaning and combining ingredients.\nArchiving, on the other hand, refers to putting the data in a safe place after the research is finished. The data are in a fixed state, they don’t change anymore. Archiving is done for verification purposes: so others can check that your research is sound. Or: it is done so that others can reuse the resulting dataset. There is also a difference between archiving and publishing, but in essence, archiving and publishing happen at a similar moment and for both, data do not change anymore.\n\nThis illustration is created by Scriberia with The Turing Way community. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807\n\n\n\nThere are various reasons to archive your data: replication, longitudinal research, data being unique or expensive to collect, re-usability and acceleration of research inside or outside your own discipline. It is VU policy to archive your data for (at least) 10 years after the last publication based on the dataset. Part of preparing your dataset for archiving is appraising and selecting your data.\n\n\nDuring your research you may accumulate a lot of data, some of which will be eligible for archiving. It is impossible to preserve all data infinitely. Archiving all digital data leads to high costs for storage itself and for maintaining and managing this ever-growing volume of data and their metadata; it may also lead to decline in discoverability (see the website of the Digital Curation Centre). For those reasons, it is crucial that you make a selection.\n\n\n\nSelecting data means making choices about what to keep for the long term, and what data to archive securely and what data to publish openly. This means that you have to decide whether your dataset contains data that need to be removed or separated. Reasons to exclude data from publishing include (but are not limited to):\n\ndata are redundant\ndata concern temporary byproducts which are irrelevant for future use\ndata contain material that is sensitive, for example personal data in the sense of the GDPR, like consent forms, voice recordings, DNA data; state secrets; data that are sensitive to competition in a commercial sense. These data need to be separated from other data and archived securely\npreserving data for the long term is in breach of contractual arrangements with your consortium partners or other parties involved\n\nIn preparing your dataset for archiving, the first step is to determine which parts of your data are sensitive, which can then be separated from the other data. Redundant data can be removed altogether.\n\n\n\nOnce you have separated the sensitive data from the rest of your dataset, you have to think about what to do with these sensitive materials. In some cases they may be destroyed, but you may also opt for archiving multiple datasets. For example, you may want to archive your dataset in more than one form depending on the purpose. For example:\n\nOne for reusability to share, and\nA second one that contains the sensitive data, and needs to be handled differently.\n\nFor the first, the non-sensitive data can be stored in an archive under restricted or open access conditions, so that you can share it and link it to publications. For the second, you need to make a separate selection, so the sensitive part can be stored safely in a secure archive (a so-called offline or dark archive). In the metadata of both archives you can create stable links between the two datasets using persistent identifiers.\n\n\n\nThere are several factors that determine what data to select for archiving. For example, whether data are unique, expensive to reproduce, or if your funder requires that you make your data publicly available. This might also help you or your department to think about a standard policy or procedures for what needs to be kept, what is vital for reproducing research or reuse in future research projects.\nMore information on selecting data:\n\nTjalsma, H. & Rombouts, J. (2011). Selection of research data: Guidelines for appraising and selecting research data. Data Archiving and Networked Services (DANS).\nDigital Curation Centre (DCC): Whyte, A. & Wilson, A. (2010). How to appraise and select research data for curation. DCC How-to Guides. Edinburgh: Digital Curation Centre.\nResearch Data Netherlands: Data selection.\n\n\n\n\n\nA dataset consists of the following documents:\n\nRaw or cleaned data (if the cleaned data has been archived, the provenance documentation is also required)\nProject documentation\nCodebook or protocol\nLogbook or lab journal (when available, dependent on the discipline)\nSoftware (& version) needed to open the files when no preferred formats for the data can be provided\n\nSee the section Metadata for more information about documenting your data.\nDepending on the research project it may be that more than one dataset is stored in more than one repository. Make sure that each consortium partner that collects data also stores all necessary data that is required for transparency and verification. A Consortium Agreement and Data Management Plan will include information on who is responsible for archiving the data."
  },
  {
    "objectID": "pathways/document-and-preserve.html#data-documentation",
    "href": "pathways/document-and-preserve.html#data-documentation",
    "title": "Document & Preserve Data",
    "section": "Data Documentation",
    "text": "Data Documentation\n\nDocumenting your data\nData documentation aims to describe the collected data to make it easier to use, retrieve and manage. Data documentation takes various forms and describes the data on multiple levels. The description of the dataset and data object is also referred to as metadata, i.e. data about the data. One way to do add metadata is to attach a readme file to your data. ResearchData NL offers guidance for this. The CESSDA has made very detailed guidance available for creating documentation and metadata for your data.\nIn addition to describing their own datasets and objects, researchers can cross-refer to the project proposal where other researchers can find information about the research, e.g. aims and goals, methodology and data collection, the persons responsible for the project etc. The type of research and the nature of the data also influence what kind of documentation is necessary.\nDifferent types of data are governed by different standards (see also the image above), and these should be taken into account when documenting data. These requirements include, but are not limited to:\n\nFAIR data principles: the set of principles (Findable, Accessible, Interoperable, Reusable) for data exchange.\nDisciplinary metadata standards: guidelines for documenting data. This can refer to the dataset documentation, the object description, or both. Disciplinary metadata standards can document the dataset as a whole or as a data object (see number 5).\nProject documentation: the description of a project involving data collection. This documentation is often used for research verification and provenance.\nMetadata dataset: the description of a dataset, often used for discovering datasets within a repository.\nMetadata of a data object: name definition of a data object, often set up by the researcher to structure data or by the research group for collaboration during the project.\n\n\n\nCodebooks\nA codebook is a technical description of the data that were collected for a particular purpose in one or more datasets. It describes how the data are arranged in the computer file or files and what the parts or variables (numbers and letters) mean. A good description may also include specific instructions on how to use and interpret the data properly.\n\nLike any other kind of “book,” some codebooks are better than others. The best codebooks include the following elements:\n\nDescription of the study: who did it, why they did it, how they did it\nSampling information: what was the population studied, how was the sample drawn, what was the response rate\nTechnical information about the files themselves: number of observations, record length, number of records per observation, etc.\nStructure of the data within the file: hierarchical, multiple cards, etc.\nDetails about the data: the meaning of the variables, whether they are character or numeric, and if numeric, what format\nText of the questions and responses: some even include how many people responded a particular way.\n\nMore information about codebooks can be found on the website of the Kent State University Library (specifically useful if you want to create a codebook in SPSS) and on the website of the Data Documentation Initiative (specifically useful for researchers in the social sciences).\nExamples of codebooks are\n\nThe Guide to Social Science Data Preparation and Archiving. 5th ed. Inter-university Consortium for Political and Social Research (ICPSR) (2012).\nInstitute for Health and Care Research (EMGO) codebook (2015)"
  },
  {
    "objectID": "pathways/document-and-preserve.html#metadata",
    "href": "pathways/document-and-preserve.html#metadata",
    "title": "Document & Preserve Data",
    "section": "Metadata",
    "text": "Metadata\n\nControlled Vocabularies & Classifications\nWhen a metadata section in a Data Management Plan template includes a question on the used ontology (if any) what is usually meant is: is there a specific vocabulary or classification system used. Controlled vocabularies are created by domain experts to help translate ontological concepts as well as to organise knowledge for subsequent (information) retrieval. Controlled vocabularies (CESSDA: “structured controlled vocabularies”) are intended to reduce ambiguity that is inherent in normal human languages where the same concept can be given different names and to ensure consistency. Controlled vocabularies are used in subject indexing schemes, subject headings, thesauri, taxonomies and other knowledge organization systems. Some vocabularies are very internationally accepted and standardized and may even become an ISO standard or a regional standard/classification. Controlled vocabularies can be broad in scope or very limited to a specific field. When a Data Management Plan template includes a question on the used ontology (if any), what is usually meant is: is there a specific vocabulary or classification system used.\nExamples are:\n\nCDWA (Categories for the Description of Works of Art)\nGetty Thesaurus of Geographic names\nNUTS (Eurostat)\nMedical Subject HEadings (MeSH)\n\nMany examples of vocabularies and classification systems can be found at the FAIRsharing.org website. It has  a large list for multiple disciplines. If you are working on new concepts or new ideas and are using or creating your own ontology/terminology, be sure to include them as part of the metadata documentation in your dataset (for example as part of your codebook).\nControlled vocabularies help make searching for and re-using information or data much easier when they are part of a machine-readable metadata scheme or system.\n\n\nMetadata & Datasets\nMetadata is descriptive information about data / information. Metadata allow humans and programs to more easily understand and interpret information or data. Controlled vocabularies are often used to help make searching for and re-using information or data much easier when they are part of a machine-readable metadata scheme or system.\nThe CESSDA has created helpful guidance about creating metadata.\nThere are three main levels of metadata: Data assets, Dataset documentation and Dataset registration (more information):.\n\nData assets\nOn a micro-level there are four functional categories of metadata standards for datasets themselves that describe elements like structure, content, values (definitions, see also code book), and data formats (CSV, XML, etc.). Additionally, research groups often use a discipline’s standards to also describe data objects using naming conventions. There are, however, other guidelines for naming conventions and document versioning which can be useful for all documents, independent of whether they are research data or not. Often The table below gives an example of this.\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nType of data\nVersioning\n\n\n\n\nRaw data\nConsumer spending data\nText files\n2017-02-23_ConsumerSpending_1.2.txt\n\n\nProcessed data\nAnonymized Transcription of patient interviews\nWord files, Excel\n2014-11-17_RawTranscription_Checked1.docx\n\n\nAnalysed data\nPhoto Images with descriptions\nTIFF files, Word file\nC:\\Images\\Raw\\2016-07-01_Subject1-V2.tiff  C:\\Images\\Clean\\2016-07-01_Subject1-H1c.tiffC:\\Images\\Clean\\Descript\\2016-07-01_Subject1-H1c.Docx\n\n\n\n\n\nDataset documentation\nOn this general, descriptive, level the metadata concerns data packaging & metadata documentation on the dataset. It can include items like:\n\nReadme files that lists the period of research, collaborators, a short description of the research as well as the elements within the dataset\nCode Book: it provides descriptions, explanations or definitions of variables in a dataset\nPolicy documents describing the context of the research as well as referring to standard operating procedures used\nRe-use guidelines (or licences) describing if there are re-use restrictions or limitations, including contact details.\n\n\n\nDataset registration\nWhen you want to make sure that your dataset is findable it is recommended that the elements of the description of your dataset are made according to a certain metadata standard that allows for easier exchange of metadata and harvesting of the metadata by search engines. Many certified archives use a metadata standard for the descriptions. If you choose a data repository or registry, you should find out which metadata standard they use. At the VU the following standards are used:\n\nDataverseNL and DANS use the Dublin Core metadata standard\nThe VU Research Portal PURE uses the CERIF metadata standard\n\nMany archives implement or make use of specific metadata standards. The UK Digital Curation Centre (DCC) provides an overview of metadata standards for different disciplines. The list is a great and useful resource in establishing and carrying out your research methodology. Go to the overview of metadata standards. More important tips are available at Dataset & Publication.\n\n\n\nArchiving & FAIR Principles\nIf you want to archive your dataset in such a way that it is compatible with the FAIR-principles, you can use the information in this practical guide which describes how to implement the FAIR data policy and this table which matches metadata fields from different systems (these documents were written for the Faculty of Behavioural and Movement Sciences).\nThe Dutch Techcentre for Life Sciences has developed open source software code to enable you to make your dataset’s metadata FAIR. The software is being developed through GitHub and full details on the FAIR Data Point Software are available there. The Dutch eScience Center also developed Fair Data Point software, of which full details are, similarly, available on GitHub."
  },
  {
    "objectID": "pathways/data-lifecycle.html",
    "href": "pathways/data-lifecycle.html",
    "title": "Data lifecycle",
    "section": "",
    "text": "The data lifecycle helps you find, document, and preserve data."
  },
  {
    "objectID": "pathways/data-lifecycle.html#support",
    "href": "pathways/data-lifecycle.html#support",
    "title": "Data lifecycle",
    "section": "Support",
    "text": "Support\n\nResearch Data Services\n\n\n\nImage Scheme VU Research Data Support Offices\n\n\nResearch Data Management is supported by various departments at the VU. These departments will help all VU researchers. There are also faculty specific support departments for research data support; they support their own faculty members.\nHere you find references to other organisational units and departments that can help you with matters related to collecting and managing data.\nVU research data support (for all researchers)* \n\nGrant office\nLibrary\nLegal\nSecurity\nIT for Research\nIXA\n\nGeneral Faculty research support and management guidelines are available in the section Policies & Regulations.\n\n\nThis LibGuide\nQuestions regarding your data or the information on this website?\nThe RDS Support team can help you with all your question on data management plan, data archiving, data store or data privacy & security.\nThe RDS Helpdesk provides information about Research Data Management through the page Research Support on VUweb.\n\n\nVU IT for Research\nThe VU has an IT team specifically devoted to research: ITvO (from ‘IT voor Onderzoek’ in Dutch). They provide the following services:* Bazis HPC cluster computing: access to your own linux computational cluster at the VU\n\nSciCloud: a service with which you lease virtual server capacity for research purposes\nSciStor: inexpensively store large sets with research data\nAdvice/consultancy: ITvO will help you to find a suitable technical solution or support for your research group or project\nHousing: rack space in the server room for (remotely managed) equipment of research groups\n\nRead more about and get in touch with IT for Research\n\n\nExternal sources\n\n Research Data Netherlands\n\nResearch Data Netherlands is an alliance between 4TU.Centre for Research Data, Data Archiving and Networked Services (DANS) and SURFsara. With this coalition, which is also open to other parties, the three data archives join forces in the area of long-term data archiving.\n\n Landelijk Coördinatiepunt Research Data Management\n\nThe foundation of co-operating Dutch universities (VSNU) pointed out a need for a co-ordinated and decisive approach to Research Data Management through a dedicated Centre at SURF. The SURF Foundation is an organisation that facilitates education and research in the Netherlands.\n\nThe LCRDM aims to support the preparation, development and monitoring of Research Data Management policies for scientific research in the Netherlands. Important elements of this central approach are close co-operation with researchers in the field and the exchange of knowledge and experience.\n\n The Netherlands eScience Center\n\nThe eScience Center develops software for academic research specifically. Their software and tools enhance the use of digital methods in scientific research across all disciplines. Researchers - including those at the VU - can choose to collaborate on projects funded by the NWO or, if they already have funding, ask the eScience Center to collaborate with them.\n\n CESSDA\n\nThe Data Management Expert Guide of the Consortium of European Social Science Data Archives is a practical guide for researchers, addressing many issues they may have"
  },
  {
    "objectID": "pathways/data-lifecycle.html#rdm-training",
    "href": "pathways/data-lifecycle.html#rdm-training",
    "title": "Data lifecycle",
    "section": "RDM Training",
    "text": "RDM Training\nThe VU Library offers Workshops on writing a DMP, which are offered separately for each faculty. These workshops are also announced via the Library Calendar, where you can register. Questions about these workshops may be directed to the RDM Support Desk"
  },
  {
    "objectID": "pathways/data-lifecycle.html#surfsara",
    "href": "pathways/data-lifecycle.html#surfsara",
    "title": "Data lifecycle",
    "section": "SURFsara",
    "text": "SURFsara\n\nSURF is the collaborative organisation for ICT in Dutch education and research. SURF offers advanced ICT services specifically for researchers. You can start using some of these services right away with your VU credentials. For others you have to get in touch with SURF yourself. Please check SURF’s website and the pages about the specific services below for more information.\nSURF’s services are listed on this page.\n\nData services\nSURF offers a wide range of services for different phases in the life cycle of your research data. Everything for the secure storage, management, sharing and reuse of data.\n\nResearch Drive: securely and easily store and share research data.\n\nDoes your research team need large storage quotas, a secure environment to store personal and/or sensitive data, and work collaboratively with other educational and governmental institutions or external private parties? Research Drive is a cloud-based shared-storage environment specifically designed for these requirements.\n\nSURFfilesender: send large files securely and encrypted.\n\nWant to send and receive files quickly, securely and easily? With SURFfilesender, you can send large files, such as research data. The files are stored in the Netherlands. Encryption provides added security.\n\nSURFdrive* Store and share your files securely in the cloud with SURFdrive.\n\nStore, synchronise and share your documents easily with SURFdrive. SURFdrive is a personal cloudservice for the Dutch education and research. Your documents are kept safe and sound in our community cloud.\n\nData Archive* Secure, long-term storage with Data Archive.\n\nThe Data Archive is the centralised location for data archiving and (long-term) storage. You can securely store research data there, even in volumes running into the petabytes. The archive provides quick access to SURFsara’s computing facilities.\n\nData Ingest Service* Data Ingest Service: upload a large amount of data to SURF.\n\nThe Data Ingest Service is intended for researchers who want to store or analyse large amounts of data at SURFsara. The service is convenient for users who lack sufficient bandwidth or who have stored their data on a number of external hard disks.\n\nData Persistent Identifier* Data Persistent Identifier: data always findable by permanent references.\n\nPersistent identifiers (PIDs) ensure the findability of your data, now and always. PIDs are comparable to the ISBN numbers assigned to books. Even if the location or underlying infrastructure changes, the reference path remains intact. SURFsara offers the PID service in cooperation with the European Persistent Identifier Consortium (EPIC).\n\n\n\n\nData processing and analysis (see also ‘Computing’)\n\nJupyter Notebook* Jupyter Notebook: accessible and interactive data analysis for research and education.\n\nA Jupyter Notebook is an interactive web application that you can use to create documents, known as notebooks, that contain computer code, formatted text, comparisons and visualisations. The code can be executed in the environment and you can even create streaming applications and dashboards.\n\n\n\n\nComputing\nDo you encounter limitations with your own systems? SURF offers researchers a wide range of services in the field of high performance computing (HPC): thousands of times faster than your PC.\n\nCartesius: National Super Computer.\n\nIt is the most comprehensive system in the field of capability computing in the Netherlands. Cartesius is especially in high demand for its combination of fast processors and internal network, large storage capacity and the ability to process large datasets.\n\nLisa Compute Cluster: extra processing power for research.\n\nLisa Compute Cluster combines processing power with user friendliness. Are the limits of your own system inhibiting your research? This service lets you upscale to a higher level. Lisa Compute Cluster is preconfigured with a range of software packages, meaning you can start working right away.\n\nHPC Cloud: your flexible compute infrastructure.\n\nHPC Cloud gives you and your project team complete control over your computing infrastructure. The infrastructure ranges from a single work station to a complete cluster and can be expanded to suit your needs. You can use your own operating system and analysis software. HPC Cloud is housed in SURF’s own data centre.\n\nGrid: for processing and storing large datasets.\n\nDo you want to process and store large- amounts of data? The Grid may very well be suitable for your project. The grid infrastructure consists of a large number of clusters for computing and data storage, which are interconnected via a fast network.\n\nVisualisation: more insight into your data.\n\nDo you want to analyse, process or visualise complex research data or big data? These services give you insight into your research data. SURF’s Visualisation service allows you to visualise your own datasets on your desktop. This makes it easy to identify connections between data or gain other insight into your datasets. SURFsara offers a powerful remote visualisation service that combines high performance with ease of use.\n\nCollaboratorium: a visualization and presentation space for science and industry.\n\n\n\nExpertise, advice and training\nSURF offers advice and training on their services. Their training sessions are announced in the SURF Agenda.\n\nSURF Training courses for research.\n\nWant to get started with SURF systems but lack the necessary knowledge? SURF regularly organizes hands-on systems training courses at their offices in Utrecht and Amsterdam or at your education or research institution. You can also include the training courses in the educational programme of your institution.\n\nSURF Consultancy on ICT solutions for researchers.\n\nSURFsara possesses a wealth of experience in the field of ICT services for researchers. If you need help developing/improving your application or designing your infrastructure, then you’ve come to the right place. SURF experts will be happy to lend their expertise to support your research."
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "You can contribute to this book by making small edits or writing entirely new topics. From small to large, all contributions are welcome. If you are in need of specific information, you can skip ahead using the table of contents.\n\n\nWe offer a portal to reduce the barriers to contribute to the Research Support Handbook. You only need an internet connection and articulate what you want us to include. No accounts necessary 😊\n\n\n\n\n\n\nNote\n\n\n\nOpen the contribution portal by clicking here or copy-pasting: https://ez-github-contributor.netlify.app/\n\n\nYou can report issues you find with the Research Support Handbook using the “Report a problem” tab. This is a way for you to share your feedback with us.\nYou can propose new topics to the Research Support Handbook using the “Propose new page” tab. This will be considered for inclusion as a topic.\n\n\n\nScreenshot of the contributor portal\n\n\nIf you want to be credited with contributing, please share your name. If you’d like to hear back about what was done with your feedback or proposal, please also provide a direct way to contact you.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the next steps you need a GitHub account to contribute. You can create one directly on GitHub.\n\n\n\n\nThe easiest and quickest way to contribute to the book is make suggested edits. On each page you will find a button reading “Edit this page” (usually on the right).\n\n\n\nScreenshot of a handbook topic, with a red box on the right hand side of the page indicating where to find the “Edit this page” button\n\n\nWhen you click that, you will immediately be taken to GitHub to edit the text of that specific page. You may be prompted to create a fork (forking) in case these are your first edits.\n\n\n\nScreenshot of the GitHub file editor, with some changes made and the “Commit changes” button active\n\n\nOnce you made your edits, you are ready to commit (save) your changes and submit your pull request, requesting those changes to be included in the handbook.\n\n\n\nTo add a new topic, you need to create a new file ending in .qmd in the topics folder (e.g., topics/example.qmd). You can do this by visiting the handbook page on GitHub and clicking Add file -&gt; New file.\n\n\n\nScreenshot of GitHub highlighting where to find the “New file” button\n\n\nWhen you click this button you may be asked to fork the repository. This is not a problem so go ahead!\nThe topic itself needs to be written in Markdown. Every topic must contain a first level heading (e.g., # Heading), which will be the topic title. Section headings are second level headings (e.g., ## Section).\nAfter that, you are ready to submit your pull request! The reviewers will help you place the topic in the right place of the book.1\n\n\n\nTo add a new pathway, you need to create a new file ending in .qmd in the pathways folder (e.g., pathways/example.qmd). You can do this by visiting the handbook page on GitHub and clicking Add file -&gt; New file.\n\n\n\nScreenshot of GitHub highlighting where to find the “New file” button\n\n\nWhen you click this button you may be asked to fork the repository. This is not a problem so go ahead!\nEvery pathway must contain a first level heading (e.g., # Heading), which will be the pathway title. Section headings are second level headings (e.g., ## Section).\nThe pathway itself needs to be written in Markdown. For each topic you want to include, you can either mention so on a line surrounded by whitespaces:\nINSERT TOPIC: DATA MANAGEMENT PLAN\nThis will tell the editorial team to include that topic there. Please be specific in the naming. You can also directly include the topic yourself directly using the following code:\n{{&lt; include ../topics/filename.qmd &gt;}}\nYou can verify the filename directly, but it should correspond to each word separated by a minus sign (for example, data-management-plan.qmd).\nAfter that, you are ready to submit your pull request! The reviewers will help you place the topic in the right place of the book.2\n\n\n\nOnce you have made suggested changes, a pull request is the way for you to ask for your changes to be incorporated into the handbook. The people maintaining the handbook will review what you wrote, ask some questions, and accept or decline your contributions.\nWe recommend keeping your suggested changes small or limited in scope, and explaining why you are suggesting these changes. It is more likely your changes are included when you are fixing a typo or adding a paragraph, and less likely if you are revising the entire handbook. It is also more likely they are included if you explain why you are suggesting the changes, rather than dropping by and making edits without any context.\nIf you are adding a new topic, it is definitely recommended to open an issue first to see whether there is a need for it (and maybe you’ll find collaborators!).\nDuring the review process you may be asked to update your changes, or revisions may be added by the people maintaining the handbook. It is helpful if you keep an eye on your GitHub account to ensure timely responses to help the process along.\n\n\n\nThe book is created using Markdown - you can get familiarized with the basic syntax on the Markdown website. The getting started quick items are:\n# Heading level 1\n## Heading level 2\n### Heading level 3\n\nYou simply write text as you are used to. To make something *italic*, **bold**, or ***bold and italic***.\n\n&gt; this is how you add quotes\n\n- or lists\n- that can go on \n- and on\nIf you want to add code, use references, create links, or footnotes - it is all possible. We will expand examples here based on your needs, so if you need help, let us know by reporting an issue!\n\n\n\nWe use GitHub to create this website automatically, and to manage all the incoming updates. You do not need to know how it works entirely, but we want to help you understand some things so you are not confused.\n\n\nA repository on GitHub is like a folder on your computer. This can be many things, depending on what files it contains.\nWhen we mention a repository here, we mean that we want you to look at a specific folder. The repository for this website for example can be found on GitHub directly. You will always be contributing to a repository, in order to contribute to the handbook.\n\n\n\nA repository is owned by one or multiple people on GitHub. If you are not one of them, you can create a copy of the repository (folder) to make your edits in. This act of creating a copy is called “forking.”\nWhen you create a copy, you do not have to worry about accidentally removing or destroying the handbook. Your changes are not reflected in the website until you submit a pull request."
  },
  {
    "objectID": "contributing.html#direct-contributions",
    "href": "contributing.html#direct-contributions",
    "title": "Contributing",
    "section": "",
    "text": "We offer a portal to reduce the barriers to contribute to the Research Support Handbook. You only need an internet connection and articulate what you want us to include. No accounts necessary 😊\n\n\n\n\n\n\nNote\n\n\n\nOpen the contribution portal by clicking here or copy-pasting: https://ez-github-contributor.netlify.app/\n\n\nYou can report issues you find with the Research Support Handbook using the “Report a problem” tab. This is a way for you to share your feedback with us.\nYou can propose new topics to the Research Support Handbook using the “Propose new page” tab. This will be considered for inclusion as a topic.\n\n\n\nScreenshot of the contributor portal\n\n\nIf you want to be credited with contributing, please share your name. If you’d like to hear back about what was done with your feedback or proposal, please also provide a direct way to contact you."
  },
  {
    "objectID": "contributing.html#contributing-via-github",
    "href": "contributing.html#contributing-via-github",
    "title": "Contributing",
    "section": "",
    "text": "Note\n\n\n\nFor the next steps you need a GitHub account to contribute. You can create one directly on GitHub.\n\n\n\n\nThe easiest and quickest way to contribute to the book is make suggested edits. On each page you will find a button reading “Edit this page” (usually on the right).\n\n\n\nScreenshot of a handbook topic, with a red box on the right hand side of the page indicating where to find the “Edit this page” button\n\n\nWhen you click that, you will immediately be taken to GitHub to edit the text of that specific page. You may be prompted to create a fork (forking) in case these are your first edits.\n\n\n\nScreenshot of the GitHub file editor, with some changes made and the “Commit changes” button active\n\n\nOnce you made your edits, you are ready to commit (save) your changes and submit your pull request, requesting those changes to be included in the handbook.\n\n\n\nTo add a new topic, you need to create a new file ending in .qmd in the topics folder (e.g., topics/example.qmd). You can do this by visiting the handbook page on GitHub and clicking Add file -&gt; New file.\n\n\n\nScreenshot of GitHub highlighting where to find the “New file” button\n\n\nWhen you click this button you may be asked to fork the repository. This is not a problem so go ahead!\nThe topic itself needs to be written in Markdown. Every topic must contain a first level heading (e.g., # Heading), which will be the topic title. Section headings are second level headings (e.g., ## Section).\nAfter that, you are ready to submit your pull request! The reviewers will help you place the topic in the right place of the book.1\n\n\n\nTo add a new pathway, you need to create a new file ending in .qmd in the pathways folder (e.g., pathways/example.qmd). You can do this by visiting the handbook page on GitHub and clicking Add file -&gt; New file.\n\n\n\nScreenshot of GitHub highlighting where to find the “New file” button\n\n\nWhen you click this button you may be asked to fork the repository. This is not a problem so go ahead!\nEvery pathway must contain a first level heading (e.g., # Heading), which will be the pathway title. Section headings are second level headings (e.g., ## Section).\nThe pathway itself needs to be written in Markdown. For each topic you want to include, you can either mention so on a line surrounded by whitespaces:\nINSERT TOPIC: DATA MANAGEMENT PLAN\nThis will tell the editorial team to include that topic there. Please be specific in the naming. You can also directly include the topic yourself directly using the following code:\n{{&lt; include ../topics/filename.qmd &gt;}}\nYou can verify the filename directly, but it should correspond to each word separated by a minus sign (for example, data-management-plan.qmd).\nAfter that, you are ready to submit your pull request! The reviewers will help you place the topic in the right place of the book.2\n\n\n\nOnce you have made suggested changes, a pull request is the way for you to ask for your changes to be incorporated into the handbook. The people maintaining the handbook will review what you wrote, ask some questions, and accept or decline your contributions.\nWe recommend keeping your suggested changes small or limited in scope, and explaining why you are suggesting these changes. It is more likely your changes are included when you are fixing a typo or adding a paragraph, and less likely if you are revising the entire handbook. It is also more likely they are included if you explain why you are suggesting the changes, rather than dropping by and making edits without any context.\nIf you are adding a new topic, it is definitely recommended to open an issue first to see whether there is a need for it (and maybe you’ll find collaborators!).\nDuring the review process you may be asked to update your changes, or revisions may be added by the people maintaining the handbook. It is helpful if you keep an eye on your GitHub account to ensure timely responses to help the process along.\n\n\n\nThe book is created using Markdown - you can get familiarized with the basic syntax on the Markdown website. The getting started quick items are:\n# Heading level 1\n## Heading level 2\n### Heading level 3\n\nYou simply write text as you are used to. To make something *italic*, **bold**, or ***bold and italic***.\n\n&gt; this is how you add quotes\n\n- or lists\n- that can go on \n- and on\nIf you want to add code, use references, create links, or footnotes - it is all possible. We will expand examples here based on your needs, so if you need help, let us know by reporting an issue!\n\n\n\nWe use GitHub to create this website automatically, and to manage all the incoming updates. You do not need to know how it works entirely, but we want to help you understand some things so you are not confused.\n\n\nA repository on GitHub is like a folder on your computer. This can be many things, depending on what files it contains.\nWhen we mention a repository here, we mean that we want you to look at a specific folder. The repository for this website for example can be found on GitHub directly. You will always be contributing to a repository, in order to contribute to the handbook.\n\n\n\nA repository is owned by one or multiple people on GitHub. If you are not one of them, you can create a copy of the repository (folder) to make your edits in. This act of creating a copy is called “forking.”\nWhen you create a copy, you do not have to worry about accidentally removing or destroying the handbook. Your changes are not reflected in the website until you submit a pull request."
  },
  {
    "objectID": "contributing.html#footnotes",
    "href": "contributing.html#footnotes",
    "title": "Contributing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you are really enthusiastic and want to do it yourself: The topic needs to be added to the topics list in _quarto.yml. You can find the topics list around line 24 of that file.↩︎\nIf you are really enthusiastic and want to do it yourself: The topic needs to be added to the topics list in _quarto.yml. You can find the topics list around line 24 of that file.↩︎"
  },
  {
    "objectID": "blog/2024-06-27hackathon.html",
    "href": "blog/2024-06-27hackathon.html",
    "title": "First Handbook Hackathon",
    "section": "",
    "text": "On June 27th, 2024, the first hackathon for the Research Support Handbook took place with all the post authors. For this hackathon, we focused on non-GitHub based contributions, to make it as easy as possible to contribute. To make getting started with contributing easier, we created a choose your own adventure game. We document some lessons and clarifications below, in addition to the twelve reported problems and suggested changes.\nThe workshop helped articulate the dynamic relation between topics and pathways. Topics are contained pages around a specific subject; pathways are a collection of topics. This means that pathways include the topics directly and that this content should be up to date at any given time. When topics are changed, pathways are dynamically updated, making sure there are no discrepancies. The only situation where this may not be the case, is when a pathway is still a work in progress and the topics are not yet properly linked.\nPathways will become more efficient to create as we include more topics in the handbook. Given that pathways are primarily collections of topics, this means that there is barely any new content in there, if any at all. As we include more topics (eight at this time), pathways can focus more and more on the structuring of content, and focus less on creating the content itself.\nWith new contributions, contributors surfaced the need to preview the changes to the handbook. We documented two ways to render the handbook for such previews: (1) creating a Pull Request automatically deploys a preview website and (2) running quarto render locally on the code. Option 1 requires no additional software to be installed, but requires some knowledge of GitHub. Option 2 does not require much knowledge of GitHub, but requires the Quarto software to be installed. There was also the note that deploying the handbook using GitHub pages required a change to the URL, which may cause issues when merging the changes back into the main handbook. This highlights that ensuring reliable previews of contributed content is of importance to some contributors to the handbook.\nLastly, the hackathon surfaced many questions and discussions around the collaborative decisions that will need to be made. When does a topic become too long and should it be split up into multiple topics? Can a topic include subtopics? How is the GitHub environment maintained? How much technical expertise is necessary to ensure the content does not go offline? What contributor roles are there and who has which role? How do roles get distributed and can people volunteer for them? This highlights the engagement with the handbook, and we encourage everyone (including ourselves) to generously surface these discussions in issues or in a next hackathon.\nIn summary, the first hackathon is a success! This is the start of the next phase of the handbook journey, moving from design and scaffolding to nurturing and growing the contents. There will be more hackathons, and these will be announced on this blog and on other channels at VU Amsterdam. Until the next one!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The Open Handbook is a project started by Research Data Support in early 2024. After planning and design phases, we launched the initial version of the resource at the Research Support Days in May 2024.\nThe Open Handbook centralizes resources that VU researchers need to do their work. The Open Handbook also provides everyone with direct pathways to change resources in case anything has become outdated.\nPreviously such resources were spread out across many different pages at VU and were hard to update. The Open Handbook is curated by us all, and reviewed by specialists. This way we can help each other.\n\n\nThe Open Handbook was initiated by Lena Karvovskaya, Jessica Hrudey, Elisa, and Jolien Scholten. The initial infrastructure for the Open Handbook was built by Liberate Science.\nWe want to specifically call out the following folk who contributed outside of GitHub:\n\nJochem Lybaart\nRebecca Silva dos Santos\nJochem Nijs\n\n\n\n\n\n\nAll contributions to this project are gratefully acknowledged using the allcontributors package following the all-contributors specification. Contributions of any kind are welcome!\n\n\n\n   Alex-van-der-Jagt\n\n\n   chartgerink\n\n\n   Elisa-on-GitHub\n\n\n   jhrudey\n\n\n   Jolien-S\n\n\n   Karvovskaya\n\n\n   peer35"
  },
  {
    "objectID": "about.html#contributors",
    "href": "about.html#contributors",
    "title": "About",
    "section": "",
    "text": "The Open Handbook was initiated by Lena Karvovskaya, Jessica Hrudey, Elisa, and Jolien Scholten. The initial infrastructure for the Open Handbook was built by Liberate Science.\nWe want to specifically call out the following folk who contributed outside of GitHub:\n\nJochem Lybaart\nRebecca Silva dos Santos\nJochem Nijs\n\n\n\n\n\n\nAll contributions to this project are gratefully acknowledged using the allcontributors package following the all-contributors specification. Contributions of any kind are welcome!\n\n\n\n   Alex-van-der-Jagt\n\n\n   chartgerink\n\n\n   Elisa-on-GitHub\n\n\n   jhrudey\n\n\n   Jolien-S\n\n\n   Karvovskaya\n\n\n   peer35"
  },
  {
    "objectID": "blog/2024-05-23welcome.html",
    "href": "blog/2024-05-23welcome.html",
    "title": "Hello world!",
    "section": "",
    "text": "This is the first blog entry on the Research Support Handbook. We will be posting more at a later time, and are looking forward to your contributions as well.\nWe will follow up with more details later."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "First Handbook Hackathon\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2024\n\n\nAlex van der Jagt, Chris Hartgerink, Dimitri Unger, Elisa Rodenburg, Jessica Hrudey, Jolien Scholten, Lena Karvovskaya, Lucy O’ Shea, Mar Barrantes-Cepas, Meron Vermaas, Peter Vos, Stephanie van de Sandt\n\n\n\n\n\n\n\n\n\n\n\n\nHello world!\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "editors-guide.html",
    "href": "editors-guide.html",
    "title": "Editor’s guide",
    "section": "",
    "text": "Welcome to the Editor’s guide to the Handbook. This page contains resources around how the editors work. We make this open so you can see how it works behind the scenes."
  },
  {
    "objectID": "editors-guide.html#deleting-branches",
    "href": "editors-guide.html#deleting-branches",
    "title": "Editor’s guide",
    "section": "Deleting branches",
    "text": "Deleting branches\nAfter merging Pull Requests, it is expected to clean up the branch by deleting it. We enable auto deletion upon merging, but if that for some reason does not happen, please go ahead and “Delete branch”"
  },
  {
    "objectID": "editors-guide.html#netlify",
    "href": "editors-guide.html#netlify",
    "title": "Editor’s guide",
    "section": "Netlify",
    "text": "Netlify\nNetlify is a hosting service, where the files for the website live. This means that even though we create the website on GitHub, Netlify is where the website is hosted. When GitHub is down, the website should be unaffected."
  },
  {
    "objectID": "editors-guide.html#pull-request-handling",
    "href": "editors-guide.html#pull-request-handling",
    "title": "Editor’s guide",
    "section": "Pull Request handling",
    "text": "Pull Request handling\nWe maintain strict standards for the content included in the Research Support Handbook. Below we expand on the norms we maintain to merge and close pull requests.\n\nMerging pull requests\nWe maintain a set of rules to ensure content does not get included prematurely:\n\nAt least one editor needs to approve the pull request\nAll discussions in the pull request need to be marked as resolved\nAll automated quality checks need to be successful\n\nAll three criteria have to be met for successful inclusion in the Research Support Handbook. Note that any approvals are voided when someone adds additional changes. A new review will have to be provided at that time.\nA more implicit norm is that we provide at least 24 hours for people to review, even if the first editor reviews within the first hour. As such, everyone knows they have at least 24 hours to take a look at the content.\n\n\nClosing pull requests\nNot all pull requests make it into the handbook, and that is okay. We want to ensure that pull requests are not closed at random, and provide some norms around doing so.\nStale pull requests can be closed with a note for the contributor that they are welcome to request the pull request to be re-opened. We automatically mark pull requests as stale after 30 days.\nPull requests that provide too much discussion and do not have a pathway to resolution may be closed for being disputed. This does not mean the content goes to waste – here we recommend an issue to be opened to continue the discussion. Please note that disputed topics can only be carried on if done so in a mutually constructive manner."
  },
  {
    "objectID": "pathways/collect-and-store.html",
    "href": "pathways/collect-and-store.html",
    "title": "Collect & Store Data",
    "section": "",
    "text": "Data collection may consist of the re-use of existing data and/or the generation of new data. You can find more specific information on the re-use of existing data on the Finding Existing Data page in this LibGuide\nFor data to be considered valid and reliable, data collection should occur consistently and systematically throughout the course of the research project. Data collection guidelines and established methodologies should be used to gather data. Some disciplines make use of codebooks, whereas others use protocols for data gathering. These procedures help researchers collect data according to conventional methodological steps. If a research project involves multiple partners (in a consortium) it should be clear who is responsible for the collection of what (part of the) data. Important aspects of data collection include:\n\nStandardisation: codebooks & protocols\nStructure / organisation of the data\nData quality assurance methods\nDocumentation & metadata\nStorage & protection\n\nThis relates to the Reproducibility of your research according to the FAIR-data principles.\n\n\nThe tools being used in research to collect data are immensely diverse. For that reason, we will not provide an exhaustive overview here. What is important for data collection tools in relation to RDM is where such tools store the data that you collect and in which format. The storage location is particularly important when you are working with personal data. For example, the privacy legislation in the United States is very different from the European General Data Protection Regulation (GDPR). Hence, personal data collected in a Dutch research institute may not be stored on American servers. It is important to keep that in mind when you are contemplating which tool to use for your data collection.\nIf you are collecting personal data and you decide to use a tool for which no contract exists between VU Amsterdam and the provider of the software or tool, a service agreement and a processing agreement must be drawn up. Contact the privacy champion of your faculty for more information and a model processing agreement.\nQuestionnaire tools\nThe Faculty of Behavioural and Movement Sciences has developed a document with tips for safe use of the questionnaire tools Qualtrics and Survalyzer. The document was made for FGB researchers specifically but can also be helpful for others. Consult this document if you need a questionnaire tool to collect your data.\n\n\n\nSome research projects involve the participation of multiple organisations or institutes and may include even cross-border co-operation. When data is collected by several organisations, a Data Management Plan should provide information on who is responsible for which part of the data collection and storage. It should also provide information on how specific data collections are related to which part(s) of the research goal(s). Describing this precisely will help you to determine if a consortium agreement or joint controller agreement is necessary. You see a general example of such a specification in the table below:\n\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nResponsible organization for collection\nData origin\nData purpose\n\n\n\n\nRaw data\nCommunity level surveys\nVrije Universiteit Amsterdam\nAmsterdam, The Hague, Rotterdam\nIdentifying perceived problems, System responsiveness\n\n\nRaw data\nTrials & Focus Group Interviews\nLondon School of Hygiene and Tropical Medicine (LSHTM)\nGermany, Switzerland\nTrials to evaluate programs on . . ., Focus Group interviews to identify barriers to . . .\n\n\nRaw data\nPollution measurements using fish\nOceanographic Institute of Sweden\nCoastal waters, Northeast Spain\nEstablish pollution levels of plastic\n\n\n\n\n\n\nRegardless of the field of study or preference for defining data (quantitative, qualitative), accurate data collection is essential to maintaining the integrity (structure) of research. Both the selection of appropriate data collection instruments (existing, modified, or newly developed) and clearly delineated instructions for their correct use reduce the likelihood of errors.\nThere are two approaches for reducing and/or detecting errors in data which can help to preserve the integrity of your data and ensure scientific validity. These are:\n\nQuality assurance - activities that take place before data collection begins\nQuality control - activities that take place during and after data collection\n\nQuality assurance precedes data collection and its main focus is ‘prevention’ (i.e., forestalling problems with data collection). Prevention is the most cost-effective activity to ensure the integrity of data collection. This proactive measure is best demonstrated by the standardization of protocol developed in a comprehensive and detailed procedures manual for data collection.\nWhile quality control activities (detection/monitoring and action) occur during and after data collection, the details should be carefully documented in the procedures manual. A clearly defined communication structure is a necessary pre-condition for monitoring and tracking down errors. Quality control also identifies the required responses, or ‘actions’ necessary to correct faulty data collection practices and also minimise future occurrences.\nSome sources for protocols:\n\nHANDS Handbook for Adequate Natural Data Stewardship by the Federation of Dutch University Medical Centers (UMCs)\nProtocols.io - an open access repository of protocols\nProtocols Online - website with protocols available on the internet, sorted by discipline.\nSpringer Protocols - free and subscribed protocols collected by Springer."
  },
  {
    "objectID": "pathways/collect-and-store.html#data-collection",
    "href": "pathways/collect-and-store.html#data-collection",
    "title": "Collect & Store Data",
    "section": "",
    "text": "Data collection may consist of the re-use of existing data and/or the generation of new data. You can find more specific information on the re-use of existing data on the Finding Existing Data page in this LibGuide\nFor data to be considered valid and reliable, data collection should occur consistently and systematically throughout the course of the research project. Data collection guidelines and established methodologies should be used to gather data. Some disciplines make use of codebooks, whereas others use protocols for data gathering. These procedures help researchers collect data according to conventional methodological steps. If a research project involves multiple partners (in a consortium) it should be clear who is responsible for the collection of what (part of the) data. Important aspects of data collection include:\n\nStandardisation: codebooks & protocols\nStructure / organisation of the data\nData quality assurance methods\nDocumentation & metadata\nStorage & protection\n\nThis relates to the Reproducibility of your research according to the FAIR-data principles.\n\n\nThe tools being used in research to collect data are immensely diverse. For that reason, we will not provide an exhaustive overview here. What is important for data collection tools in relation to RDM is where such tools store the data that you collect and in which format. The storage location is particularly important when you are working with personal data. For example, the privacy legislation in the United States is very different from the European General Data Protection Regulation (GDPR). Hence, personal data collected in a Dutch research institute may not be stored on American servers. It is important to keep that in mind when you are contemplating which tool to use for your data collection.\nIf you are collecting personal data and you decide to use a tool for which no contract exists between VU Amsterdam and the provider of the software or tool, a service agreement and a processing agreement must be drawn up. Contact the privacy champion of your faculty for more information and a model processing agreement.\nQuestionnaire tools\nThe Faculty of Behavioural and Movement Sciences has developed a document with tips for safe use of the questionnaire tools Qualtrics and Survalyzer. The document was made for FGB researchers specifically but can also be helpful for others. Consult this document if you need a questionnaire tool to collect your data.\n\n\n\nSome research projects involve the participation of multiple organisations or institutes and may include even cross-border co-operation. When data is collected by several organisations, a Data Management Plan should provide information on who is responsible for which part of the data collection and storage. It should also provide information on how specific data collections are related to which part(s) of the research goal(s). Describing this precisely will help you to determine if a consortium agreement or joint controller agreement is necessary. You see a general example of such a specification in the table below:\n\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nResponsible organization for collection\nData origin\nData purpose\n\n\n\n\nRaw data\nCommunity level surveys\nVrije Universiteit Amsterdam\nAmsterdam, The Hague, Rotterdam\nIdentifying perceived problems, System responsiveness\n\n\nRaw data\nTrials & Focus Group Interviews\nLondon School of Hygiene and Tropical Medicine (LSHTM)\nGermany, Switzerland\nTrials to evaluate programs on . . ., Focus Group interviews to identify barriers to . . .\n\n\nRaw data\nPollution measurements using fish\nOceanographic Institute of Sweden\nCoastal waters, Northeast Spain\nEstablish pollution levels of plastic\n\n\n\n\n\n\nRegardless of the field of study or preference for defining data (quantitative, qualitative), accurate data collection is essential to maintaining the integrity (structure) of research. Both the selection of appropriate data collection instruments (existing, modified, or newly developed) and clearly delineated instructions for their correct use reduce the likelihood of errors.\nThere are two approaches for reducing and/or detecting errors in data which can help to preserve the integrity of your data and ensure scientific validity. These are:\n\nQuality assurance - activities that take place before data collection begins\nQuality control - activities that take place during and after data collection\n\nQuality assurance precedes data collection and its main focus is ‘prevention’ (i.e., forestalling problems with data collection). Prevention is the most cost-effective activity to ensure the integrity of data collection. This proactive measure is best demonstrated by the standardization of protocol developed in a comprehensive and detailed procedures manual for data collection.\nWhile quality control activities (detection/monitoring and action) occur during and after data collection, the details should be carefully documented in the procedures manual. A clearly defined communication structure is a necessary pre-condition for monitoring and tracking down errors. Quality control also identifies the required responses, or ‘actions’ necessary to correct faulty data collection practices and also minimise future occurrences.\nSome sources for protocols:\n\nHANDS Handbook for Adequate Natural Data Stewardship by the Federation of Dutch University Medical Centers (UMCs)\nProtocols.io - an open access repository of protocols\nProtocols Online - website with protocols available on the internet, sorted by discipline.\nSpringer Protocols - free and subscribed protocols collected by Springer."
  },
  {
    "objectID": "pathways/collect-and-store.html#data-storage",
    "href": "pathways/collect-and-store.html#data-storage",
    "title": "Collect & Store Data",
    "section": "Data Storage",
    "text": "Data Storage\n\nStorage During Research\nVU Amsterdam offers several options to store your research data. The choice for a specific option may depend on factors such as:\n\nDoes a project involve multiple organisations or departments?\nThe sensitivity of the data: does it involve personal data or copyrighted / commercial data?\nAre there any research partners with whom data need to be shared?\nAre any commercial parties involved?\nDoes the research project involve multiple locations (inside or maybe even outside the EU)?\nWill there be (lab) devices producing data that need to be stored as well?\nWhat will be the volume of the data?\nWill there be lots of interactions with the data (using software/tools)?\n\nStorage options may take several forms, for example:\n\nLocal storage on computers, networks or servers;\nCloud storage offered by the VU;\nLocations where physical data samples are stored (fridges, lockers, etc.).\n\nResearchers, including PhD candidates, have multiple options that can be used, some of which are listed below. More information about these storage options is available behind their respective links. The Storage finder is a tool that will give you a number of storage options suitable for your research. For more individual guidance, please get in touch with the Research Data Management Support Desk for advice, particularly when you are working with commercial, personal or otherwise sensitive data, or when you have a complex IT setup.\n\n\nStandard services offered by the VU\nVU IT offers several services for employees to store their files. Examples are:\n\nOneDrive: personal storage for all VU employees and part of the Microsoft 365 platform. OneDrive allows you to store files locally and in the Microsoft cloud, and share folders and documents with colleagues. Since this is personal storage, tied to someone’s personal VU account, we don’t usually recommend storing research data in OneDrive: if the account holder leaves the VU, the account and all the data on it, disappear.\nTeams. Faculties, divisions and departments have their own Team - part of the Microsoft 365 platform - where they store shared documents and where they can interact and chat. Projects may also request a project team. But note that Teams is not always the best location to store your research data and has several limitations, especially when it comes to working with non-Microsoft file formats, large volumes of data, interacting with data, and collaborating with partners outside of the VU. Contact the RDM Support Desk to find out more about the suitability of Teams for your project.\nSurfdrive: is a personal cloud storage service for the Dutch education and research community, offering staff, researchers and students an easy way to store, synchronise and share files in the secure and reliable SURF community cloud. All users receive storage space of up to 500 GB. Surfdrive is automatically offered to all VU employees. Since Surfdrive is personal storage, like OneDrive, we do not usually recommend it for research data\n\n\n\nResearch data-specific storage options\nThe options above are standard data storage options at the VU to which all employees have access. But the VU also offers storage specifically for research data. Some of them are hosted locally at the VU, while others are SURF cloud services. When selecting a cloud-based service it is important to remember to check where the data will be hosted. If the research project involves sensitive data it may be necessary to choose cloud-based options that guarantee that the data will stay in the EEA or on servers based in the EEA.\n\nSciStor (short for ‘Storage for Scientists’): This is storage hosted by IT for Research (ITvO) and allows for inexpensive storage of large volumes of data. There are various levels of security possible and various ways to get access to the files. SciStor is only intended for ongoing research, not for archiving.\nYoda (short for Your Data) is a cloud storage at SURF and is suitable for storing large-scale and sensitive datasets. Yoda also supports collaborating on projects in and outside the VU and adding contextual information (metadata) about your dataset as you go. Yoda is usually the best choice if your research data are very sensitive.\nResearch Drive is a cloud storage at Surf for research projects and is suitable for collaboration in and outside the VU, for storing sensitive data and large-scale research projects. You can also encrypt data in Research Drive using several tools. You are able to request storage space in Research Drive via a web form in the selfservice portal (VU employees only). Research Drive is the best choice if you need to manage access rights on a folder level. More general information about Research Drive can be found here, and its wiki pages, including tutorials, are here.\n\nThere are differences between Research Drive and Yoda and each one may support certain projects better than others. The storage finder can help you to get an idea of what would be the best choice for your project, but get in touch with the RDM Support Desk for more details.\n\n\nSending research data to partners\nSome projects may require data sharing with partners. Although Research Drive and Yoda support sharing data all through the project, it may also be the case that some data only need to be sent to a partner once. There are some secure options to send data to research partners:\n\nSurf Filesender: cloud service that allows you to send files of 1 Terabyte to other researchers and encrypted files of up to 250 GB.\n   \nZivver: All employees of Vrije Universiteit Amsterdam can use Zivver, the encryption programme that allows you to send email or data (sensitive or otherwise) securely by email. Attachments will also be encrypted and can be several Terabytes in size (max = 5 TB). Specific information on how to get and use Zivver are available on the selfservice portal. General explanations on how to use it are available at the Zivver website."
  },
  {
    "objectID": "pathways/collect-and-store.html#data-protection",
    "href": "pathways/collect-and-store.html#data-protection",
    "title": "Collect & Store Data",
    "section": "Data Protection",
    "text": "Data Protection\n\nWhat is Data Protection?\nProtection from what? From whom? When, and why? Before we talk about data protection, let us consider security first. More often than not, ‘security’ is regarded as a fixed state. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe question for you as a researcher is ‘when are the measures that you take secure enough?’. In order to answer this, please be aware that there are three entities that have an opinion about what is ‘secure enough’, namely: the law, the University, and you yourself as the data processor.\nThe University has a Security Baseline that sets a norm for levels of protection for every application it uses. The Baseline is based on international standards. For each of these applications, the University is considering for which means the security of these applications are adequate enough.\nThe legal requirements for the processing of personal data can be found in the section ‘GDPR and Privacy’ under Plan & Design There are additional laws and regulations as well. The assumption is that you are familiar with these, especially with laws regulating medical and criminal research.\nWhat you personally consider to be secure might be very different from what your colleagues, the Faculty or the University considers to be secure enough and the norms will vary with the variety of data that is being processed by different researchers and Faculties of the VU. Very generally speaking, there are three points of protection to consider:\n\nProtection against data loss, for which you need a back up periodically.\nProtection against data leakage, for which you need to consider all storage places and their access points.\nProtection of data integrity, for which you need version control and synchronisation management.\n\nThe security of your protection measures depends on the threat you face. We often think of threats as active, and motivated by bad intentions. But most common forms of data loss are accidental and most leakage is caused by trusting others. In reality, devices just get lost or break down, people download malware by accident, and each one of us forgets to save a document at times or gets confused about which version was last updated.\nIn all cases, protection starts with oversight on where your data is stored and processed. If you forget that you temporarily stored it in a certain place, you have then lost oversight of where that data is. The opposite is also true: if you know where you data is, you have insight in the level of security of the space in which you store it. As you can see, protection begins with organising your work in a reliable manner and thinking through your steps.\nFor example, if you data is on your laptop and synchronised with your phone, then it is stored in two places. Perhaps this is enough back up, perhaps not. If you put both you devices in the same bag and you lose your bag, you have no backup. A backup to an online storage might be a good solution, but might also mean your data leaks via the internet of via the storage provider who sells the data and your behavioural data for profit. Most importantly, there is no absolute security. It is best if you consider your personal behaviour and then think of scenarios that are more or less likely to happen and what would impact you most. If you frequently work in public places you should make it a habit to lock your device each time you leave it. If you eat and drink behind your desk often, better work with a remote keyboard to protect your laptop from the unavoidable coffee shower. Do you save your respondents’ contact details on your personal phone? Then protect it with a pin.\nHere are some basic protection guidelines:\n\nData are very difficult to erase. You have probably never done it.\nDecide how to back up data and test it before you rely on it.\nDo not give others your log-in credentials. If you have done so and your family members use your work device, then change it.\nDo not use passwords twice, do not use your birthday, initials, streetname, hobby.\nEncryption sounds secure, but it fails completely without good password management.\n\n\n\nData Protection\nThere can be many reasons why the data of a project needs to be kept protected:\n\nSensitivity of the data collected\nProtection of the research data from competition\nCommercial reasons / Intellectual property\nEtc.\n\n\nThere are also many levels of security that may be implemented, depending on the needs. Sometimes it will be enough to use a password-protected cloud-based server. In extreme cases encryption may be needed and also when data is transmitted between researchers or organisations. You should contact the RDM Support Desk to discuss available options, who may connect you to legal experts where sensitive data is concerned. Check the Data Storage section of this LibGuide with links to find out more on campus solutions and cloud-based options.\n\n\nSafe Transportation and Transfer\nIt is important to protect your data during the entire data life cycle. To find out whether your data are secure during all stages of your research, think about your data flow: where do your data originate and where do they go to? If data need to be transported from one physical place to the other, or need to be transferred from one device to another, these actions should happen in a secure way.\n\nTransferring digital data\n\nOnline connection on campus\nIf data collection takes place through a certain measurement device (e.g. MRI scanner, EEG scanner, eye tracker), the data need to be transferred from the measurement device to the storage location that you will use during your research project. Make sure that this transfer takes place in a secure way and also make a plan for the data on the measurement device; find out whether they need to be destroyed or can remain there.\n\n\nOnline connection outside campus (with and without VUnetID)\nIf you are doing fieldwork outside the campus and you have reliable and secure internet access, it is a good idea to upload the data to a storage location that is regularly backed up and secure, in order to prevent data loss. If you have a VUnetID, you can for example use:\n\nSURFdrive to store your data in a secure cloud service\nSURFfilesender to send you data to a colleague or consortium partner, who can store your data in an appropriate place\n\nYou can find more information about each of these storage options on the Data Storage page of this LibGuide.\nIf you need to receive data from colleagues in your project who don’t have access to these tools (e.g. because they are students, don’t work for a Dutch educational institution, or have no VUnetID), SURFdrive, SURFfilesender and Edugroepen can also be used:\n\nSURFdrive: you can set up a ‘File drop’ folder. By sharing the link of this folder to the researchers who need to upload documents, you enable them to do anonymous uploads to this folder. These users have solely upload rights, no view or download rights. The folder can be protected with a password, which you preferably share with the uploaders through another channel.\nSURFfilesender: as a SURFfilesender user, you can send a voucher to someone who doesn’t have access to this tool. This person can use this voucher to send documents to you. These files can be encrypted.\nZivver is an email plugin with which you can encrypt emails and attachments.\n\n\n\nOffline data outside campus\nIf you are doing fieldwork in an area with limited internet access, you might use a portable device to initially store your data during the phase of data collection, such as a USB drive or an external hard drive. These data can be transferred to a storage location that is connected to the internet (e.g. G-drive, SURFdrive) later. Please make sure that the data on such portable devices are secured, by using encryption (and by transporting them safely by using a lockable briefcase or backpack).\n\n\n\nTransporting physical data\nIf physical objects need to be transported, you should check with the data manager at your department (if available) what options are available. Special briefcases that can be locked or secure backpacks may need to be used to keep informed consent forms or other sensitive data objects (USB drives etc.) secure during transport.\n\n\nData transportation and transfer across borders\nSome countries have rules to control the movement of encryption technology that enter or exit their borders. If you need to travel with an encrypted laptop to secure your data, for example during fieldwork abroad, please keep this in mind. If you need to transfer data in and out of such countries, please get advice on encryption and secure transportation at the IT Service Desk.\nIf you have general questions about how to protect your data when transporting or transferring them, you can contact the IT Service Desk. In case of complex situations for which you need tailored support, you can consult the IT Relationship Manager representing the research domain, who can request capacity at IT for setting up an information security plan. Such a plan is usually based on documents which need to be completed beforehand, like a Data Protection Impact Assessment and a Data Classification. Please note that IT-capacity for tailored support is a paid service for which budget needs to be reserved."
  },
  {
    "objectID": "pathways/discover-and-initiate.html#finding-existing-data",
    "href": "pathways/discover-and-initiate.html#finding-existing-data",
    "title": "Discover & Initiate Research",
    "section": "Finding Existing Data",
    "text": "Finding Existing Data\n\nRe-using Existing Data\nAnything that can be used for analysis can be considered “data(sets)”. Many national and international organisations provide access to large datasets free of charge: this is called Open Data.\nDatasets may contain different kinds of data files, e.g. raw or edited/cleaned data, and macro or micro data. Raw data refers to the data as they are primarily collected, and includes all data, even the missed or mismatched pieces in the data file. Edited or cleaned data refers to data that have been tidied up for analysis and publication. Macro data and statistics are results based on micro data units and provide a general overview of the micro data. Although datasets can contain data of varying type or aggregation level, and there may be overlap between these definitions, each element can contain very important information.\nWhen re-using research data, scientists must be familiar with the rules and regulations governing data copyright, intellectual property rights, and laws governing sensitive or personal information. SURF has compiled a report on the legal status of raw data including information on the types of consent required for the re-use of data. Your Privacy Champion can answer questions about the use of personal data. IXA can provide legal help with the re-use of data.\nSee also the ZonMw explanation of different kinds of property rights in the Netherlands (text available in Dutch only).\n\n\nSources for Finding Existing Datasets\nThe number of datasets that are available grows rapidly. Datasets are made available in many formats, by many people or organizations. Some datasets are raw files and some are specifically organised and formatted as databases that require a licence or subscription to use them. The library of the Vrije Universiteit Amsterdam has collected links to some of the data repositories used and has licensed several databases.\n\nPopular Free and Licensed Databases: These can be found in the library e-resources list or with LibSearch Advanced.\n\nIf you need help finding & using free or licensed sources you can contact the Research Data Services Helpdesk. For students and personnel in the fields of economics, finance, or organisation science a separate LibGuide has been created to help them find and use/re-use data.\nYou can also start looking for data in these four places:\n\nThe literature. Research articles may point you to the data that they are based on. Sometimes, (part of) the data are added to the article as supplementary files, and sometimes the data are published separately in a data repository. In the latter case, the article usually provides a clear reference to the published dataset. Some datasets may even be specifically published in Data Journals.\nScientific data repositories. Data repositories are platforms used to access and archive research data. Universities often provide a repository for data archiving, but other platforms arranged by discipline or by country also exist. Some repositories are only accessible to consortium members, whereas others are free of charge. Many universities in the Netherlands use DataverseNL to archive datasets for the mid-term. Long-term archiving is provided by the national research data archives DANS and 4TU.Research Data. In Europe, B2SHARE and Zenodo are platforms used to access research data. Data repositories can be accessed by searching by topic or country using Re3data, a data repository registry. The VU has its own research portal, PURE, where researchers register their datasets. You can find instructions on how to register your own dataset in PURE on the Dataset Registration page of this LibGuide.\nData search engines. Search engines allow you to quickly browse data sets and supplementary data files published by researchers. They cover data sets from many sources. This makes them useful for quick orientation on a topic. Example of a search engines are: DataCite, Google DataSet Search, Elsevier Dataset Search.\nData portals of (governmental) organisations. Organisations that regularly collect (statistical) data sometimes offer these data through their own portal. An example is Eurostat, which collects and disseminates statistics at the European level, by country and by theme. Some of these websites have been linked in the Finding data LibGuide.\n\n\n\nData Sources for VU Researchers\nResearchers from the Vrije Universiteit Amsterdam have also developed some databases containing data collected during research. See here for some examples:\n\nNederlands Tweelingenregister (Netherlands Twin Register) The database contains data on twins and their families and was created to do research on the relationship between genetics and growth, development, personality, behaviour, diseases, mental health and all kinds of risks.\nGeoplaza VU - the portal for all matters related to GIS (Geographical Information Systems) and geodata at the VU University Amsterdam. It offers students and employers a platform to exchange, examine and download digital map material.\nDutch monasteries - database with information about Dutch monasteries of the Middle Ages.\nSlave owners in Amsterdam 1863 - the place of living of owners of slaves in Amsterdam in 1863, visualized in GeoPlaza.\nDeaths at the Borders Database - collection of official, state-produced evidence on people who died while attempting to reach southern EU countries from the Balkans, the Middle East, and North & West Africa, and whose bodies were found in or brought to Europe.\nDatasets published by VU Researchers can be found at the VU Research Portal."
  },
  {
    "objectID": "pathways/discover-and-initiate.html#data-citation",
    "href": "pathways/discover-and-initiate.html#data-citation",
    "title": "Discover & Initiate Research",
    "section": "Data Citation",
    "text": "Data Citation\n\nCitation Elements\nCiting data is not different from citing a publication. Make sure to check the rules of the journal to know how you should cite when writing an article for a specific academic journal. For all of the journals, however, the minimum compulsory elements in a data citation include:\n\nAuthor(s): Name of the author (creator) of the dataset\nTitle: Name of the dataset\nDate of publication\nPublisher: Archive where dataset is stored\nPersistent Identifier: Unique identifier, most common is the DOI (see section Data Publication).\n\nOptional elements that may be included in the reference are:\n\nFile Type: Codebook, movie, software\nVersion: Version number of the edition\nCreation Date\nDate of Consultation (last)\n\nExample of a data citation:\nStephens, William, 2020, “Resiliences to Radicalisation - QSort Data”, https://doi.org/10.34894/35MTMN, DataverseNL, V1.\nFor more information, see the following guidelines:\n\nDataverse\nDataCite\nDCC UK\nData Citation Synthesis Group (2014). Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11"
  },
  {
    "objectID": "pathways/plan-and-design.html",
    "href": "pathways/plan-and-design.html",
    "title": "Plan & Design",
    "section": "",
    "text": "Grant programmes from organisations like NWO, ZonMW and ERC increasingly require you to not only think about the journey of the data in your research project, but also the method of data collection and how to protect or share data during and after the research project. It is important to bear in mind the specific laws and regulations that apply to the kind of data that is collected. If a project involves data on persons and organisations this impacts the design of the necessary IT infrastructure. A more detailed description of this will later be captured in the data management plan.\nWhen writing your research proposal the following items are important:\n\nFill in the data management paragraph (see the four questions below)\nPlanning: one of the early deliverables will be a detailed data management plan\nBudget: take into account the costs (labour and material) for data storage during and data archiving after your project.\nWriting: Funders that distribute grants like to maximise the effectiveness of this investment. It is therefore highly recommended that the data will be made Findable, Accessible, Interoperable and Re-usable (FAIRdata). This does not mean that the data have to be open: laws, licenses and contracts regarding personal and sensitive data may limit the possibility to share the data publicly.\n\nResearch Data Services provides advice and help when writing a data paragraph as part of the Research proposal. The Library also regularly organises workshops to help you get started. Together with the VU Grants Office and project control we are part of the grant support team offering advice and practical aid for your grant. You will be directed to the specific unit during your support trajectory. Make sure to contact the team as early as possible.\n\n\n\nIn order to make data re-usable, funders require researchers to include a data section (= paragraph) in their project proposal, in which they explain whether research data will be collected or generated during the project, and how they plan to structure, archive and share their data. Depending on requirements of the funder, the paragraph can be short or more extensive.\nFunders may have different requirements for the Data Management Paragraph in the project proposal. Always check what your funder asks for. For example, in 2016, the NWO formulated four questions that need to be answered in the data paragraph of the research proposal:\n\nWill data be collected or generated that are suitable for re-use?\nWhere will the data be stored during the research?\nAfter the project has been completed, how will the data be stored for the long term and made available for the use by third parties? To whom will the data be accessible?\nWhich facilities (ICT, (secure) archive, refrigerators or legal expertise) do you expect will be needed for the storage of data during the research and after the research? Are these available?"
  },
  {
    "objectID": "pathways/plan-and-design.html#research-proposal",
    "href": "pathways/plan-and-design.html#research-proposal",
    "title": "Plan & Design",
    "section": "",
    "text": "Grant programmes from organisations like NWO, ZonMW and ERC increasingly require you to not only think about the journey of the data in your research project, but also the method of data collection and how to protect or share data during and after the research project. It is important to bear in mind the specific laws and regulations that apply to the kind of data that is collected. If a project involves data on persons and organisations this impacts the design of the necessary IT infrastructure. A more detailed description of this will later be captured in the data management plan.\nWhen writing your research proposal the following items are important:\n\nFill in the data management paragraph (see the four questions below)\nPlanning: one of the early deliverables will be a detailed data management plan\nBudget: take into account the costs (labour and material) for data storage during and data archiving after your project.\nWriting: Funders that distribute grants like to maximise the effectiveness of this investment. It is therefore highly recommended that the data will be made Findable, Accessible, Interoperable and Re-usable (FAIRdata). This does not mean that the data have to be open: laws, licenses and contracts regarding personal and sensitive data may limit the possibility to share the data publicly.\n\nResearch Data Services provides advice and help when writing a data paragraph as part of the Research proposal. The Library also regularly organises workshops to help you get started. Together with the VU Grants Office and project control we are part of the grant support team offering advice and practical aid for your grant. You will be directed to the specific unit during your support trajectory. Make sure to contact the team as early as possible.\n\n\n\nIn order to make data re-usable, funders require researchers to include a data section (= paragraph) in their project proposal, in which they explain whether research data will be collected or generated during the project, and how they plan to structure, archive and share their data. Depending on requirements of the funder, the paragraph can be short or more extensive.\nFunders may have different requirements for the Data Management Paragraph in the project proposal. Always check what your funder asks for. For example, in 2016, the NWO formulated four questions that need to be answered in the data paragraph of the research proposal:\n\nWill data be collected or generated that are suitable for re-use?\nWhere will the data be stored during the research?\nAfter the project has been completed, how will the data be stored for the long term and made available for the use by third parties? To whom will the data be accessible?\nWhich facilities (ICT, (secure) archive, refrigerators or legal expertise) do you expect will be needed for the storage of data during the research and after the research? Are these available?"
  },
  {
    "objectID": "pathways/plan-and-design.html#rdm-costs",
    "href": "pathways/plan-and-design.html#rdm-costs",
    "title": "Plan & Design",
    "section": "RDM Costs",
    "text": "RDM Costs\n\nCosts & Data Management\nMany research funders encourage applicants to include data management and sharing costs in research proposals. Some funders will provide advice on costs related to data management. Some remarks on costs are provided here:\n\nThe Data Management Plan should describe the activities that incur costs and provide justification for the allocation of resources (example: acquisition of a programmer who will write software needed to capture the data).\nNo expenditure can be ‘double funded’, i.e. a service that is centrally supported by indirect costs must not be included as a direct cost as well (example: computers that are already provided to employees and paid for by the university may not be included).\nThe budget and justification should broadly indicate where RDM costs will be incurred, where possible. E.g. data capture and cleaning, data curation and preservation, data sharing.\nInclude budget for long-term storage if data are expected to be deposited in a repository not funded by the university or external funders (VU repositories are: DataverseNL, Yoda). The VU has a breakdown of costs for storage and archiving for VU-managed storage and repositories. This is available here.\n\nA practical costing tool is available from the UK Data Archive. Based on this costing tool, Utrecht University has developed a guide to calculate the costs of data management. Similarly, in collaboration with RDM experts from several Dutch universities, the Landelijk Coördinatiepunt Research Data Management (LCRDM) has made an RDM costs table that includes budgetting for technical applications. You can use those guides as well to estimate the costs needed specifically for RDM.\nMost material costs of the storage solutions offered by the VU are covered centrally (up to 500 GB), but if you need to specify the costs for your project, look at the following sections in the LibGuide:\n\nStorage Solutions During Research\nArchival Solutions After Research (please note that most funders do not cover archival costs)\n\nExamples to put in a data management plan:\n\n\n\n\n\n\n\n\n\nData Stage\nDataset\nType of data\nCosts\n\n\n\n\nRaw data\nInterviews\nAudio files\nAudio equipment rentalLocation rental costsData storage & backup\n\n\nProcessed data\nTranscription of interviews\nWord files\nPersonnel costs: hiring research assistants for manual entryData storage & backup\n\n\n\nAnalysis software\nR script\nPersonnel costs: programmer to write a programme to mine the data\n\n\nAnalysed data\nRegression graphic\nPhotoshop files\nSoftware costs\n\n\n\nProject Website\nHTML, Java\nHosting feePersonnel to build initial website"
  },
  {
    "objectID": "pathways/plan-and-design.html#rdm-requirements",
    "href": "pathways/plan-and-design.html#rdm-requirements",
    "title": "Plan & Design",
    "section": "RDM Requirements",
    "text": "RDM Requirements\nIf you do research at the VU, you may be subject to the requirements for Research Data Management formulated by various parties. Please check which requirements apply to your research project.\nMany funders have specific requirements for RDM. The exact requirements vary by funder. They usually include a data management paragraph in the project proposal and a Data Management Plan (DMP) after funding has been granted. As funding agencies invest financially in your research project, they often have demands concerning research integrity, data quality, data publication and reusability. As research output, data are often compared to a kind of public good that should be made available to the community for re-use if possible. So always check what demands are set by a funder before you apply.\n\nFunding agencies\n\nData management paragraph in project proposal\nAt a grant application, some funders request a short data paragraph in your project proposal or an outline of a Data Management Plan. Without these your proposal will not be eligible for review.\n\nNWO: Data management section\nZonMw: Orientation of data management in project proposal\n\n\n\nData Management Plan\nIn a Data Management Plan (DMP; see also the section Data Management Plan) you explain how you will handle your research data. Check with your funder at what stage a DMP has to be submitted and how it should be composed. Many funders have their own templates.\nThe tool DMPonline can be used to access and fill in a DMP template. You can also write a DMP in collaboration and invite a third party to comment or give feedback on your DMP. Most funders’ templates are available in DMPonline. In order to write a DMP, you need to create your own account. We are working on better integration of this tool with the rest of VU infrastructure.\n\n\nOverview of funders’ RDM requirements and DMP templates\nThe Consortium of European Social Science Data Archives (CESSDA) presents a comprehensive overview of data management requirements and templates of the main Dutch and European funding bodies. This is helpful if you want to quickly find more information. However, make sure you always check the details that you receive in the documentation of your actual funding agency, so that you are aware of all up-to-date requirements.\n\nNational: NWO, ZonMw\nInternational: European Research Council (including H2020)\n\n\n\nPublishing your data and terms of use\nNormally a funder requires you to publish your data in a data repository at the end of the project (unless this is prohibited by legislation). Therefore, the funders’ DMP templates usually include the following questions:\n\nwhere your dataset can be found\nwhether your dataset has a persistent identifier\nhow your data are documented\nwhether your data may be reused freely or not and which terms and conditions apply\n\nPlease consider your funder’s data publishing requirements, so that you can take the necessary steps before and during your research project. For example, if you are working with personal data and you want to publish them in a data repository, this needs to be included in the informed consent forms that your participants have to sign.\n\n\n\nLocal requirements from your university and faculty\nThe VU is committed to support research that meets the highest requirements of replicability and transparency. The FAIR data principles (see also the section Overview), the purpose of which is to render research data Findable, Accessible, Interoperable and Reusable, the General Data Protection Regulation (GDPR) and the principles of Open Science are at the foundation of the Research Data Management (RDM) policy of the VU.\nIn addition to the central policy for RDM, faculties of the VU also have developed their own implementation of this policy.\nPlease check the relevant local policies and Standard Operating Procedures relevant for your faculty or department before you start your research project. An overview of all available policy documents can be found in the section VU policies and regulations.\n\n\nConsortium partners\nPartner institutions in a consortium may also have research data management requirements, for example with respect to data security. They may ask for:\n\ncertification in relation to data security of the VU’s infrastructure\nstatements from the IT department about the IT systems being used at the VU\n\nThe University Library’s Research Data Services team or your faculty’s research support office can help you with this."
  },
  {
    "objectID": "pathways/plan-and-design.html#collaboration",
    "href": "pathways/plan-and-design.html#collaboration",
    "title": "Plan & Design",
    "section": "Collaboration",
    "text": "Collaboration\nSome research projects involve more than one partner organisation. Be sure to indicate exactly who is responsible for collecting and managing the data in each case, where, and how. If more than one organisation is involved, it may also be necessary to create a Consortium Agreement. Depending on the area or sector of each project and of the degree of technical complexity that is involved, the Consortium Agreement usually contains the following information:\n\nprovisions on the governance structure of the consortium:\ntechnical provisions (e.g. the tasks of each party and the project schedule, description of the data collection responsibilities);\nfinancial provisions (e.g. the distribution of funds among participants, the financial plan, etc).\n\nThe agreement can include a section on who is ultimately responsible for the data and whether the data will be shared afterwards or whether certain restrictions on re-use apply. These restrictions can also be related to copyright issues or pending patent requests. IXA can help you to draw up a consortium agreement. The RDM Support Desk at the University Library can also help with questions about legal matters.\nIf you are working with personal data, GDPR requires that all parties working with the data sign a joint controller agreement. You can ask your Privacy Champion for advice about this. For multi-centre clinical research, a Clinical Trial Agreement is recommended.\nFor projects funded by the European Union, several sources are available:\n\nFor Horizon 2020 projects a document is available, called “Guidance How to draw up your consortium agreement”.\nThe European Union also has the European IPR Helpdesk which can provide guidance."
  },
  {
    "objectID": "pathways/plan-and-design.html#data-security",
    "href": "pathways/plan-and-design.html#data-security",
    "title": "Plan & Design",
    "section": "Data Security",
    "text": "Data Security\n\nData classification\n‘Security’ is often regarded as a fixed state. Therefore, people tend to think of security measures as fixed solutions in the form of technological measures. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe value of data or applications is established through classification in Confidentiality, Integrity and Availability (CIA) or in Dutch Beschikbaarheid, Integriteit en Vertrouwelijkheid (BIV).\nTraditionally, this classification assesses the value of an entity (data or application) to an organisation. For research data, however, the value to the University is in all cases the same. The value of each research project is the same. Does that mean that there is not need to classify Research Data? Referring back to the definition of security, it is the assessment of the level of protection against a certain threat and its accuracy depends on the value of (in this case) data. The reason to classify Research Data is that there is a huge variety in the risks that can have in case of data loss or theft.\nThe reason that the Vrije Universiteit and its Reseachers need to classify data is to understand the variety in risk that exists in order to assess if security measures are accurate.\nData classification is about the level of sensitivity (low, medium or high) of your data assets so you can judge the risks to your research (group). This will help you when deciding what security and protection measures you need to take for handling the data or parts of the data.\n\nData classification criteria\nIn order to classify your data collection or data processing (in categories from low, to medium, or high), the following properties are considered.\n\nAvailability: what risks are associated with accessibility to data (i.e. how readily do the data need to be available for use and how damaging would it be to your research if data are lost), what measures should you take to prevent data loss?\nIntegrity: what do you do to prevent measurement or data entry errors, corruption of stored data or unauthorised changes to the stored data?\nConfidentiality: how securely do data need to be managed to prevent sharing of data with unauthorised individuals? The necessity for confidentiality depends on the sensitivity of the information, either as sensitive personal information or confidential business information, as well as the vulnerability of the subjects from whom the data is collected and the laws that apply to the data being collected and analyzed. In some cases, confidentiality can be very high; when the confidentiality is high or very high, please contact the RDM Support Desk.\n\nFor all of these aspects, the damage impact should be considered, i.e. te risks to all parties involved (i.e. participants, but also the VU as an institute, the researchers, any collaborators etc. Untoward outcomes could be loss of privacy/secrecy, reputation damage, financial costs, fraud, mental, social or physical harm)\n\n\nExamples of Highly classified data\nYour data are classified as ‘high’ when you collect or process the following data:\n\npersonal data\nstate secrets\ncompetitive corporate information\nanimal-testing data\n\n\n\nPersonal data\nDo not confuse the risks of data loss with the need to comply to legal regulations. Data security is part of risk management and is aimed at balancing protection against productivity, investments against profit. The GDPR is European Law in the legal area of Human Rights and concerns the use of personal data. Personal data are a type of data that is commonly processed in many fields of scientific research. You collect or process personal data when the data can be linked to a unique individual, either directly through direct identifiers such as name, address, IP-address etc., or indirectly through a combination of information. Personal data need to be protected. More information about personal data, data protection and the European law on privacy, the General Data Protection Regulation (GDPR), can be found in the section GDPR & Privacy\n\n\nData Classification tool for researchers\nTo help you to determine the data classification for your research data assets, the VU has developed a tool that will help you to assess and classify the availability, integrity and confidentiality risks of these assets. Based on your results from using the tool, you may need to seek further advice from VU Security and Privacy Experts (see below). Some basic security tips were compiled by the data steward of the Faculty of Behavioural and Movement Sciences.\n\n\nVU Security and Privacy experts\nVU Security and Privacy experts can help you with the details on these aspects.\n\nGeneral questions about information security: RDM Support Desk. If you need advice when determining the data classification of your data assets, you can contact them.\nReporting a (potential) data breach: IT Servicedesk. A data breach is an incident in which the possibility exists that the confidentiality, integrity or availability of information or data processing systems has been potentially threatened, for example attempts to gain unauthorised access to information or systems (hacking), the loss of a USB stick with sensitive information, data theft of hardware.\nTailored advice or support: Relationship Managers at IT. Through the relationship managers researchers can request capacity at IT for setting up and/or assessing of information security plans or paragraphs. An information security plan is particularly important in projects with a complex infrastructure (e.g. international collaboration, use of various data sources and databases), tailored solutions and requirements from funding agencies or external partners. Please note: IT-capacity for tailored support is a paid service for which budget needs to be reserved.\n\nRead more practical information about this below in the section Data Protection & Security, or the Support section on the GDPR information page.\n\n\n\nData Protection & Security\nWhere sensitive information is collected, the researcher must consider the following:\n\nwho has access to the data during the study, and how the data will be made available after publication\nwhat security regimes apply to sensitive data, and how data are protected\nhow data access during and after the project will be managed\nhow to deal with sensitive information\nwhether informed consent is required and how the forms will be accessed and stored\n\nOn the VU Intranet information is available on Security, data loss and reporting incidents. Legal experts also can help you if you have questions about working with personal data and/or if you have to perform a Data Protection Impact Assessment. On the VU Intranet you can find more information about DPIAs at the VU. The data steward for the Faculty of Behavioural and Movement Sciences has also created a guide about data encryption."
  },
  {
    "objectID": "pathways/plan-and-design.html#gdpr-privacy",
    "href": "pathways/plan-and-design.html#gdpr-privacy",
    "title": "Plan & Design",
    "section": "GDPR & Privacy",
    "text": "GDPR & Privacy\n\nGDPR in Practice\n\nImportant definitions\n\nPersonal data refers to any information relating to an identified or identifiable natural person (‘data subject’). See also the definition of ’personal data’ according to the official text of the GDPR.\nData processing refers to any action performed on data, such as collecting, storing, modifying, distributing, deleting data. See also the definition of ‘processing’ according the official text of the GDPR.\nDirect and indirect identification: Some identifiers enable you to single out an indiviual directly, such as name, address, IP-address etc. Individuals can also be identifed indirectly through:\n\na combination of information that uniquely singles out an individual (e.g. a male with breast cancer in a breast cancer registry, a pregnant individual over 50 etc.), this includes information in one record and information across different data files or datasets\nunique information or patterns that are specific to an individual (e.g. genomic data, a very specific occupation, such as the president of a large company, repeated physical measurements or movement patterns that create a unique profile of an individual or measurements that are extreme and could be linked to subjects such as high-level athletes)\ndata that are linked to directly identifying information through a random identification code or number\n\nPseudonymous data: Data that are indirectly identifiable are generally considered to be pseudonymous; this means that they are NOT anonymous and still qualify as personal data. Therefore privacy laws, such as the GDPR, do in fact apply to these data. This is for example the case when direct identifiers are removed from the research data and put into a key file (or what is usually called a subject identification log in medical research) with which the direct identifiers can be mapped to the research data through unique codes, so that reidentification is possible. These data are therefore pseudonymous, and not anonymous. The LCRDM has made a reference card that illustrates the difference between pseudonymous and anonymous data.\n\n\n\nBackground information\nPrivacy in research - 10 key rules\nWhere research requires the collection of personal data, the researcher has to consider the main rules for personal data processing. These rules are summarised in the document Privacy in Research - 10 key rules.\nVSNU Code of Conduct for using personal data in research\nThe VSNU’s Code of Conduct for Research Integrity (Dutch, English, 2018) includes a reference to the GDPR and its Dutch implementation law UAVG. An updated Code of Conduct for Using Personal Data in Research which complies with GDPR is still work in progress.\n\n\nSupport within your faculty: Privacy Champions\nEach faculty has one or more Privacy Champions, who are the first point of contact for questions relating to privacy and the GDPR. The Privacy Champions can help you with completing a Data Protection Impact Assessment, registering your research in the record of processing activities, designing informed consent forms and other questions relating to the GDPR. The list of Privacy Champions can be found on VUnet. It is important that you make an overview of what data you are collecting. Your privacy champion can help you with this.\nThe Privacy Champion of the Faculty of Behavioural and Movement Sciences has prepared a checklist for what to consider when creating an informed consent form. An important issue in informed consent forms, is the possible future (re-)use of the data..You should always ask your Privacy Champion for advice when drawing up an informed consent form.\n\n\n\nComplete a Data Protection Impact Assessment (DPIA)\nWhen scientific research includes the processing of personal data, conducting a Data Protection Impact Assessment (DPIA) may be a legal requirement under the General Data Protection Regulation (GDPR). If it is not a legal requirement, conducting a DPIA is always a helpful exercise to make sure that you address all legal aspects that need to be addressed. It is the best way to GDPR-proof your research."
  },
  {
    "objectID": "pathways/plan-and-design.html#what-is-a-dpia",
    "href": "pathways/plan-and-design.html#what-is-a-dpia",
    "title": "Plan & Design",
    "section": "What is a DPIA?",
    "text": "What is a DPIA?\nA DPIA is an assessment to identify the risks of processing personal data. It consists of a number of questions on the basis of which you determine whether the processing of personal data in your research project is legitimate and which measures should be taken to make sure this processing takes place within the boundaries of the GDPR. A DPIA doesn’t deliver an automatic report at the end, but it rather makes you think about all relevant topics you need to address before starting the processing of personal data. The outcome of a DPIA should be used to determine appropriate measures to mitigate the identified risks, such as data minimisation (not collecting more data than necessary), pseudonymising data, selecting appropriate tools for data storage and data sharing."
  },
  {
    "objectID": "pathways/plan-and-design.html#when-is-a-dpia-required",
    "href": "pathways/plan-and-design.html#when-is-a-dpia-required",
    "title": "Plan & Design",
    "section": "When is a DPIA required?",
    "text": "When is a DPIA required?\nA DPIA is required when the processing of personal data is likely to result in a “high risk” for the participants of your research project. This is for example most likely the case when scientific research includes the processing of special categories of personal data, such as data concerning health, religious or philosophical beliefs, political opinions or criminal convictions and offences (see Privacy in Research - 10 key rules for more information about special categories of personal data).\nThere are two DPIA lists which describe situations in which a DPIA is required:\n\nThe Dutch data protection authority (Autoriteit Persoonsgegevens) has published a list of 17 “high risk” situations in which a DPIA is mandatory.\nThe European data protection authorities have together published a list of 9 criteria which can be used to determine whether there is a “high risk”.\n\nThe VU has developed a tool which combines these two DPIA lists. By completing this PreDPIA tool, you can determine whether a DPIA is required in your situation. You should consult your Privacy Champion when filling in the PreDPIA."
  },
  {
    "objectID": "pathways/plan-and-design.html#how-can-i-complete-a-dpia",
    "href": "pathways/plan-and-design.html#how-can-i-complete-a-dpia",
    "title": "Plan & Design",
    "section": "How can I complete a DPIA?",
    "text": "How can I complete a DPIA?\nThe VU has a DPIA template based on a form provided by the Dutch Government (see the original template if you wish to have more background information, only available in Dutch).\nWe advise you to use this template to complete a DPIA. Please complete a DPIA at least before you start collecting personal data. In some cases, it might be useful to have a look at the DPIA template at the stage of writing a research proposal.\nIf you are not sure whether it is required to conduct a DPIA or if you need help completing a DPIA, please contact your faculty’s Privacy Champion. If needed they can contact the legal specialists of Institutional and Legal Affairs."
  },
  {
    "objectID": "pathways/plan-and-design.html#policies-regulations",
    "href": "pathways/plan-and-design.html#policies-regulations",
    "title": "Plan & Design",
    "section": "Policies & Regulations",
    "text": "Policies & Regulations\n\nVU General Policies and Regulations\n\nResearch data management policy\nVrije Universiteit Amsterdam considers the careful handling of research data to be very important. The university has therefore formulated a Research Data Management policy which provides guidance for researchers and policy officers at VU Amsterdam.\n\nVU RDM policy (2020)\n\nSince the VU policy for RDM is formulated in general terms, faculties have worked out more detailed guidelines for their own faculty. These faculty-specific policies can be found below.\n\nACTA RDM policy, Academisch Centrum Tandheelkunde Amsterdam (2016, in Dutch)\nBeta RDM policy, Faculty of Science (2022)\nFGB RDM policy, Faculty of Behavioural and Movement Sciences (2023)\nFGW RDM policy , Faculty of Humanities (2023)\nFRT RDM policy, Faculty of Religion and Theology (2024)\nFSW RDM policy, Faculty of Social Sciences (2023)\nRCH RDM policy, Faculty of Law (2021)\nSBE RDM policy, School of Business and Economics (2023)\n\nFor RDM policies and guidelines at Amterdam UMC, location VUmc, please get in touch with Research Data Management Support at Amsterdam UMC.\n\n\nAcademic integrity complaints procedure\nBoth VU Amsterdam and Amsterdam UMC, location VUmc employ a joint policy for the handling academic integrity complaints. This policy outlines the steps to be taken in the event of a complaint, the officers who play a role in this procedure, and what should be expected once a complaint has been lodged.\n\n\nConfidential counselors\n​The VU has a list of confidential counsellors who handle academic integrity issues.\n\n\nData breach incident report\n​From 2016 onwards, any data security breaches (particularly those that have, or are likely to have, serious adverse consequences to the protection of personal data) should be reported immediately to the IT Servicedesk. Read the protocol reporting a data breach (available in Dutch only).\n\n\nRegulations and Guidelines\nSome faculties and departments have their own guidelines for RDM. You can find an overview of such guidelines below.\n\nACTA Research Code\nEMGO+ Research Quality Handbook\nFGB:\n\nCode of Ethics for Research in the Social and Behavioural Sciences Involving Human Participants\nCollection of guidelines and Standard Operating Procedures (SOPs)\n\nSBE Research Ethics Regulations for Researchers and route map for research data management\n\n\n\n\nEthics Committees\nIn cases where research involves human or animal participants, a research proposal may need to be reviewed by an ethics committee. VU and Amsterdam UMC, location VUmc, have several ethics committees, which are listed below. Please note that researchers at the VU also have to go to the METc at VUmc if their research is subject to the WMO, which is not restricted to research at VUmc.\n\nACTA: ACTA Institutional Review Board (IRB)\nBeta: Research ethics review committee Faculty of Science (BETHCIE)\nFGB: Scientific and Ethical Review Board (VCWE)\nFGW: Ethische Toetsingscommissie Onderzoek (EtCO)\nFSW: Research Ethics Review Committee (RERC)\nRCH (Faculty of Law): Ethics Committee\nSBE: Research Ethics and Integrity\nVUmc (Amsterdam UMC): Medical Ethical Review Committee (METc)\n\n\n\nNetherlands Code of Conduct for Scientific Integrity\nDutch scientists are required to comply with the Netherlands Code of Conduct for Research Integrity (VSNU, 2018). The principles of proper scientific and scholarly research, according to the Code of Conduct are:\n\nHonesty\nScrupulousness\nTransparency\nIndependence\nResponsibility\n\nThe principles of honesty and transparency state explicit guidelines on the way in which you treat your research data:\n\nHonesty: you should refrain from fabricating or falsifying data\nTransparency:\n\nYou should ensure that it is clear to others what data your research is based on, how the data were obtained, what the results are and how you got to these results\nAll steps in your research process must be verifiable (e.g. choice of research question, research design, methodology, sources used), so that it is clear to others how your research was conducted\n\n\nTo live up to these general principles, the Code of Conduct provides the following standards, which are addressed in a DMP, for good research practices related to data management:\n\nProvide a description of the way in which the collected research data are organised and classified, so that they can be verified and re-used (standard 3.2.10)\nMake research data public upon completion of your research project; if this is not possible, explain why (standards 3.2.11 and 3.4.45)\nDescribe the data you have collected and used in your research honestly, scrupulously and transparently (standard 3.3.23)\nManage your data carefully and store both the raw and processed versions for a period appropriate for your discipline (standard 3.3.24)\nContribute towards making data FAIR, where possible (standard 3.3.25)\nBe transparent about your methods and working procedures by using e.g. research protocols, logs, lab journals or reports to describe these processes (standard 3.4.35)\n\n\n\nStrategy Evalution Protocol\nThe Strategy Evaluation Protocol 2021-2027 (SEP) from the VSNU is used to assess the quality of research at Dutch universities, NWO and Academy institutes. It promotes the handling and storing of raw and processed data with care and integrity.\nThe SEP formulates questions on how a research institute deals with and stores raw and processed data. It also assesses the output of research institutes, including datasets, and the use of such output by peers and societal target groups.\nBy registering your datasets in the VU Research Portal, you contribute to an overview of datasets of your department, faculty and the VU as a whole.\n\n\nNWO Data Policy\nNWO aims to ensure that all the research it funds is openly accessible to everyone as part of it’s Open Science policy. Researchers are therefore expected to preserve the data resulting from their projects for at least ten years, unless legal provisions or discipline-specific guidelines dictate otherwise. As much as possible, research data should be made publicly available for re-use. As a minimum, NWO requires that the data underpinning research papers should be made available to other researchers at the time of the article’s publication, unless there are valid reasons not to do so.\nThe guiding principle here is ‘as open as possible, as closed as necessary.’ Due consideration is given to aspects such as privacy, public security, ethical limitations, property rights and commercial interests. In relation to research data, NWO recognizes that software (algorithms, scripts and code developed by researchers in the course of their work) may be necessary to access and interpret data. In such cases, the data management plan will be expected to address how information about such items will be made available alongside the data.\nMore information on Data Management is also available on the NWO website where a NWO Data Management Template is made available. The VU Data Management template in DMP Online is certified by both NWO and ZonMW and can also be used by VU researchers for projects funded by both organizations."
  },
  {
    "objectID": "pathways/publish-and-share.html",
    "href": "pathways/publish-and-share.html",
    "title": "Publish & Share Data",
    "section": "",
    "text": "In the Data Management Plan the researcher describes if the data will be stored for the mid or the long term.\n\n\n\nAccording to the VU RDM Policy, all publication-related data should be archived for at least ten years for verification and replication of research. For this purpose, Vrije Universiteit Amsterdam offers researchers two options to archive their data in one of the organisational repositories (DataverseNL and Yoda). Other archival options may be used depending on the discipline as described in faculty data management policy documents.\n\n\n\nData relevant for future research should be archived for the long term. A dataset is relevant for future research when at least one of the following general criteria applies:\n1. The data have a scientific or historical value\n2. The data are unique\n3. Others may want to reuse the data\n4. The data cannot be reproduced\nResearchers should bear in mind that repositories can charge for archiving data. These costs can vary according to the data volume and the archive used. It is important that you consider in advance how you will budget for these costs. Whatever archiving option is used, proper descriptions of the dataset(s) and adding metadata are important.\n\n\n\n\nVU Amsterdam requests that researchers archive the data used in a publication in a repository for at least ten years after the release of the publication (see also VU Policies & Regulations). There are a lot of digital archives and many more keep appearing.\nThe right archival option depends on the nature of the data and the field of science as described in faculty or departmental data management policy documents. The university offers 2 different general repositories for data archiving.\n\nThe RDM Support Desk and faculty data stewards can help researchers with the selection of a repository that meets all the relevant criteria of privacy (sensitivity), dataset size, etc.\n\nDataverseNL - an online platform for the publication of citable research data in a semi-open environment. DataverseNL allows users to link publications to datasets directly, and to share the data through online archives such as DANS.\nSpecifications:\n\nFor publishing research data on the internet\nThe researcher publishing the data decides whether access to the data is public or restricted\nNot suitable for privacy or otherwise sensitive information\nEnables researchers to publish open data according to grant providers’ regulations\nGenerates a link (persistent identifier), e.g. for data citations in publications\nRetention period is at least 10 years\n\n\nYoda - besides active storage, Yoda also has an archive function: the vault. You can use the vault in two ways:\n\nFor archiving data securely; data are only available for verification purposes and may be access only by special request. A special procedure will be followed if anyone requests access to the data in order to verify them.\nFor publishing data; data can be available for anyone, or on request. The data will get a persistent identifier as well.\n\nBefore sending data to the vault, you will need to add metadata. A data steward, metadata specialist or functional manager can help you with the metadata and the entire process of sending data to the vault. Please get in touch with the RDM Support Desk to find this help.\n\n\n\nThere is a difference between archiving and publishing data. When we talk about archiving data, we mean that data are deposited securely, in a fixed state, in a location that is not accessible to the public or even a colleague at the VU. Archiving often happens for data that are confidential - for privacy or other reasons - and that should not be accessible publicly. Archiving is usually done for verification purposes, or, in case of medical research, to comply with the preservation requirements within the WMO.\nPublishing refers to depositing data in a public repository that allows others to view, access and download your data. You can set certain restrictions, but as a rule of thumb, publishing should only happen for data that are not confidential at all. That includes data that have been anonymised, or were not personal to begin with, and data that were never otherwise confidential. If you cannot publish any data at all, we do usually recommend trying to publish some documentation, such as data collection protocols, scripts, codebooks, etc. In this way, others can see how the research was carried out, even if they cannot simply access the data.\nUse the image below to remind yourself of the difference between archiving and publishing, and read the data publication page to find out what aspects are important when you decide to publish your data.\n\nThis illustration is created by Scriberia with The Turing Way community. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807\n\n\n\nBesides the repositories offered by the VU, there are many others. Unless you are working with personal or otherwise confidential data and you need to archive them in Yoda, you are, in principle, free to choose a different repository from the ones hosted by the VU.\nThere can be various reasons to decide to use a different repository, including funder requirements, preferences of research partners, and a repository being a common choice in your field. For example, Dutch archaeologists mostly use DANS Data Stations to deposit and publish their data. Using a repository that is a common choice in your field will make your data more findable for your colleagues and increase the visibility of your work as a researcher. Some of the data repositories most commonly used in the Netherlands include:\n\nDANS Data Stations: a domain-agnostic research data repository hosted by the Data Archiving and Networked Services, an institute of NWO and KNAW. DANS also develops policies, services and new infrastructures for research data and provides researchers with advice on how to preserve their data. VU researchers are also welcome to deposit their data at DANS-EASY;\n4TU.ResearchData: a repository for science, engineering and design data hosted by the 4TU Federation. This is a consortium of the four Dutch technical universities: TU Delft, TU Eindhoven, University of Twente and Wageningen University and Research. VU researchers are also welcome to deposit their data at 4TU;\nZenodo: a domain-agnostic research data repository hosted by CERN in Switzerland and funded by the European Commission. Zenodo does not only host data, but also presentations, conference procedures and policy documents. It is also possible to archive GitHub repositories directly into Zenodo, by which you contribute to Open Science by making a snapshot of your code available in its current form and for the long term;\nOSF (Open Science Framework): a data management and research dissemination platform. The VU is an institutional member of the OSF, which means that you can sign up (and in) using your VU account by clicking on the Institution Button on the sign in/up pages. You can use the OSF to create registrations and preregistrations for your research, to publish preprints, and publish and share data and documentation. You can also link other repositories such as DataverseNL to your OSF project. The same goes for GitHub and storage options such as Research Drive and Surfdrive. Do be careful about what you connect! A full guide for VU OSF users, including instructions about connecting external storage can be found here.\n\nYou can also find repositories via the Registry of Research Data Repositories. When you are choosing a repository, it is important to check that it provides all the services you need. A good way to find out is to check if a repository as a Core Trust Seal, which is a form of certification for quality repositories. But if a repository does not have the Core Trust Seal, it does not necessarily mean it is not a good repository. As a minimum, you should check that:\n\nThe repository provides a persistent identifier, such as a DOI;\nThe repository enables you to add rich metadata to your dataset and ideally follows an internationally recognised metadata standard, such as Dublin Core or DataCite;\nThe repository offers functionality to publish data with an embargo or under restrictions, if you need that;\nThe repository allows you to add a licence to the dataset;\nThe repository is funded sustainably for at least the next 50 years;\nAnd, in some cases, that the repository’s servers are located in the EU.\n\nMore recommendations for choosing a data repository can be found on the websites of OpenAire or CESSDA.\nIf you would like advice about what would be a good place for you to archive your research data, you can always reach out to the RDM Support Desk."
  },
  {
    "objectID": "pathways/publish-and-share.html#selecting-an-archive",
    "href": "pathways/publish-and-share.html#selecting-an-archive",
    "title": "Publish & Share Data",
    "section": "",
    "text": "In the Data Management Plan the researcher describes if the data will be stored for the mid or the long term.\n\n\n\nAccording to the VU RDM Policy, all publication-related data should be archived for at least ten years for verification and replication of research. For this purpose, Vrije Universiteit Amsterdam offers researchers two options to archive their data in one of the organisational repositories (DataverseNL and Yoda). Other archival options may be used depending on the discipline as described in faculty data management policy documents.\n\n\n\nData relevant for future research should be archived for the long term. A dataset is relevant for future research when at least one of the following general criteria applies:\n1. The data have a scientific or historical value\n2. The data are unique\n3. Others may want to reuse the data\n4. The data cannot be reproduced\nResearchers should bear in mind that repositories can charge for archiving data. These costs can vary according to the data volume and the archive used. It is important that you consider in advance how you will budget for these costs. Whatever archiving option is used, proper descriptions of the dataset(s) and adding metadata are important.\n\n\n\n\nVU Amsterdam requests that researchers archive the data used in a publication in a repository for at least ten years after the release of the publication (see also VU Policies & Regulations). There are a lot of digital archives and many more keep appearing.\nThe right archival option depends on the nature of the data and the field of science as described in faculty or departmental data management policy documents. The university offers 2 different general repositories for data archiving.\n\nThe RDM Support Desk and faculty data stewards can help researchers with the selection of a repository that meets all the relevant criteria of privacy (sensitivity), dataset size, etc.\n\nDataverseNL - an online platform for the publication of citable research data in a semi-open environment. DataverseNL allows users to link publications to datasets directly, and to share the data through online archives such as DANS.\nSpecifications:\n\nFor publishing research data on the internet\nThe researcher publishing the data decides whether access to the data is public or restricted\nNot suitable for privacy or otherwise sensitive information\nEnables researchers to publish open data according to grant providers’ regulations\nGenerates a link (persistent identifier), e.g. for data citations in publications\nRetention period is at least 10 years\n\n\nYoda - besides active storage, Yoda also has an archive function: the vault. You can use the vault in two ways:\n\nFor archiving data securely; data are only available for verification purposes and may be access only by special request. A special procedure will be followed if anyone requests access to the data in order to verify them.\nFor publishing data; data can be available for anyone, or on request. The data will get a persistent identifier as well.\n\nBefore sending data to the vault, you will need to add metadata. A data steward, metadata specialist or functional manager can help you with the metadata and the entire process of sending data to the vault. Please get in touch with the RDM Support Desk to find this help.\n\n\n\nThere is a difference between archiving and publishing data. When we talk about archiving data, we mean that data are deposited securely, in a fixed state, in a location that is not accessible to the public or even a colleague at the VU. Archiving often happens for data that are confidential - for privacy or other reasons - and that should not be accessible publicly. Archiving is usually done for verification purposes, or, in case of medical research, to comply with the preservation requirements within the WMO.\nPublishing refers to depositing data in a public repository that allows others to view, access and download your data. You can set certain restrictions, but as a rule of thumb, publishing should only happen for data that are not confidential at all. That includes data that have been anonymised, or were not personal to begin with, and data that were never otherwise confidential. If you cannot publish any data at all, we do usually recommend trying to publish some documentation, such as data collection protocols, scripts, codebooks, etc. In this way, others can see how the research was carried out, even if they cannot simply access the data.\nUse the image below to remind yourself of the difference between archiving and publishing, and read the data publication page to find out what aspects are important when you decide to publish your data.\n\nThis illustration is created by Scriberia with The Turing Way community. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807\n\n\n\nBesides the repositories offered by the VU, there are many others. Unless you are working with personal or otherwise confidential data and you need to archive them in Yoda, you are, in principle, free to choose a different repository from the ones hosted by the VU.\nThere can be various reasons to decide to use a different repository, including funder requirements, preferences of research partners, and a repository being a common choice in your field. For example, Dutch archaeologists mostly use DANS Data Stations to deposit and publish their data. Using a repository that is a common choice in your field will make your data more findable for your colleagues and increase the visibility of your work as a researcher. Some of the data repositories most commonly used in the Netherlands include:\n\nDANS Data Stations: a domain-agnostic research data repository hosted by the Data Archiving and Networked Services, an institute of NWO and KNAW. DANS also develops policies, services and new infrastructures for research data and provides researchers with advice on how to preserve their data. VU researchers are also welcome to deposit their data at DANS-EASY;\n4TU.ResearchData: a repository for science, engineering and design data hosted by the 4TU Federation. This is a consortium of the four Dutch technical universities: TU Delft, TU Eindhoven, University of Twente and Wageningen University and Research. VU researchers are also welcome to deposit their data at 4TU;\nZenodo: a domain-agnostic research data repository hosted by CERN in Switzerland and funded by the European Commission. Zenodo does not only host data, but also presentations, conference procedures and policy documents. It is also possible to archive GitHub repositories directly into Zenodo, by which you contribute to Open Science by making a snapshot of your code available in its current form and for the long term;\nOSF (Open Science Framework): a data management and research dissemination platform. The VU is an institutional member of the OSF, which means that you can sign up (and in) using your VU account by clicking on the Institution Button on the sign in/up pages. You can use the OSF to create registrations and preregistrations for your research, to publish preprints, and publish and share data and documentation. You can also link other repositories such as DataverseNL to your OSF project. The same goes for GitHub and storage options such as Research Drive and Surfdrive. Do be careful about what you connect! A full guide for VU OSF users, including instructions about connecting external storage can be found here.\n\nYou can also find repositories via the Registry of Research Data Repositories. When you are choosing a repository, it is important to check that it provides all the services you need. A good way to find out is to check if a repository as a Core Trust Seal, which is a form of certification for quality repositories. But if a repository does not have the Core Trust Seal, it does not necessarily mean it is not a good repository. As a minimum, you should check that:\n\nThe repository provides a persistent identifier, such as a DOI;\nThe repository enables you to add rich metadata to your dataset and ideally follows an internationally recognised metadata standard, such as Dublin Core or DataCite;\nThe repository offers functionality to publish data with an embargo or under restrictions, if you need that;\nThe repository allows you to add a licence to the dataset;\nThe repository is funded sustainably for at least the next 50 years;\nAnd, in some cases, that the repository’s servers are located in the EU.\n\nMore recommendations for choosing a data repository can be found on the websites of OpenAire or CESSDA.\nIf you would like advice about what would be a good place for you to archive your research data, you can always reach out to the RDM Support Desk."
  },
  {
    "objectID": "pathways/publish-and-share.html#data-publication",
    "href": "pathways/publish-and-share.html#data-publication",
    "title": "Publish & Share Data",
    "section": "Data Publication",
    "text": "Data Publication\n\nOpen Access and Open Science\nOpen Access publishing means that you make your publication freely accessible online to everyone without restrictions. The Vrije Universiteit believes that government-funded research should be available free of charge to as many people as possible. To that end, the VU Library offers a guide on how to go about Open Access Publishing.\nOpen Access publishing is one component of Open Science. The European Commission has defined open science as follows: “Open Science represents a new approach to the scientific process based on cooperative work and new ways of diffusing knowledge by using digital technologies and new collaborative tools. The idea captures a systemic change to the way science and research have been carried out for the last fifty years: shifting from the standard practices of publishing research results in scientific publications towards sharing and using all available knowledge at an earlier stage in the research process”. (Definition taken from Nationaal Programma Open Science.) This includes making openly available research data, methods and documentation where possible. As such, RDM and the practices outlined in this LibGuide are a precondition of Open Science. You can read more about Open Science in the Netherlands on the website of the Nationaal Programma Open Science and join the Open Science Community Amsterdam, the community of VU employees interested in Open Science (joint with the University of Amsterdam).\n\n\nPublishing your data in a data journal\nInstead of archiving research data in a data repository, you may choose to publish an article about your data collection. This is not necessarily common for all disciplines. Some examples of data journals where you can publish your data and dataset, are:\n\nScientific Data - Nature\nGeoscience Data Journal\nGigascience\nDataset Papers in Science\nJournal of Physical and Chemical Reference Data\nEarth System Science Data\nJournal of Open Archaeology Data\nJournal of Open Health Data\nJournal of Open Psychology Data"
  },
  {
    "objectID": "pathways/publish-and-share.html#dataset-registration",
    "href": "pathways/publish-and-share.html#dataset-registration",
    "title": "Publish & Share Data",
    "section": "Dataset Registration",
    "text": "Dataset Registration\n\nRegistration & Findability\nWhen you have finished finalizing a dataset and are ready to archive it, there are many options available. Depending on the research and choices made earlier the archive provides the option to fill in descriptive fields for a dataset. The descriptions in the archives often are automatically created using metadata standards like DataCite or Dublin Core, or some other type of standard. See also the item Metadata in this LibGuide.\nWhen registering a dataset in an archive it is important to use unique identifiers to allow for increased findability and easy attribution & citation. Examples of this are:\n\nPersonal names: try to consistently use the same notation for all researchers and assistants that are included as authors\nORCID: using a unique identifier like this for all authors is recommended. More information is available here.\nInstitutonal names: avoid using different versions (or language versions) of participating Institutes/organizations and departments. In the case of the VU the official written name is: Vrije Universiteit Amsterdam. For each organization or Institute that is included: try to make sure that the official name is used each time.\n\nSome archives also allow you to preregister your project/dataset. Examples are:\n\nOpen Science Framework (OSF) Registration\nZenodo & registration\n\n\n\nRegister your Dataset in PURE\nJust like your publications, data that you have collected for your research constitute research output, too. Therefore you are required to record your datasets in PURE. Your datasets can be of interest to others, which can in turn lead to new collaboration opportunities. Datasets recorded in PURE also appear in reports that are used for research evaluations. Even if access to your dataset is closed, you are required to register your dataset in PURE. It is a record of the research, data collection and analysis that you have carried out.\n\nBenefits of recording your dataset in PURE\n\nIt increases the visibility and findability of your datasets\nIt contributes to re-use and transparency\nIt boosts your collaboration opportunities\nIt counts towards research evaluations and assessments\n\n\n\nHow to register your dataset in PURE?\n\n\n\nScreenshot: adding a dataset to your PURE profile\n\n\n\nLog into the VU or VUmc Research Portal (PURE) using your VU or VUmc credentials\nClick on the “+” (plus) icon next to selecting “Datasets” in the overview\nYou can fill in the form using this manual (NL)/manual (EN), and read more about the various metadata in use (generic and subject specific)\nClick on “Save” to store the registration"
  },
  {
    "objectID": "topics/bazis.html",
    "href": "topics/bazis.html",
    "title": "BAZIS HPC",
    "section": "",
    "text": "The BAZIS is a compute cluster for research at the Vrije Universiteit Amsterdam. It provides a service between the general facilties at the SURF HPC center and the desktop. It is a heterogenious system composed of clusters and servers from research departments and a general partition.\nIn this topic you will find information to get you started and best practices. Clustercomputing can be very powerfull and usefull skill to add to your toolbox and get more science done.\nAlso take a look at the SURF wiki about Snellius, it contains a lot of information which applies to Bazis as well.\nInformation and how to get an account can be found on VU Service Portal under IT &gt; Research &gt; HPC Cluster Computing\nBAZIS is maintained by IT for Research.\n\n\n\nVU BAZIS Cluster\n\n\n\n\n\n\nUse your favourite SSH client to login at (bazis.labs.vu.nl?). On windows we recommend MobaXterm or MS Terminal; Mac users can use iterm. Direct access is only possible from the Campus or from SURF. From other network locations first connect to the stepstone (ssh.data.vu.nl?), or use eduVPN Institutional Access (below).\n\n\n\nInstall the client. Start eduVPN and choose Institute Access to connect. You will need MFA to authenticate. Students can activate MFA at the servicedesk ( kb-item 11809 )\n\n\n\nMobaXterm has an integrated FTP file browser. Once you have logged in to the HPC system, you will see the file browser to the left of the terminal window, where it shows the contents of your home folder. You can browse through these folders, and drag-and-drop files and folders between this FTP file browser and the Windows File Explorer. Alternatively, you can use the download/upload buttons at the top of the FTP file browser window. A green refresh button is also located there to refresh the contents of the current folder. You can also open files in the FTP file browser to edit them directly. Upon saving, you’ll be asked if you want change these files on the HPC system.\nOther SFTP browsers There are a large number of free FTP browser out there. Some examples are\n\nFilezilla (Windows, MacOS, Linux)\nCyberduck (Windows, MacOS)\nWinSCP (Windows)\n\n\n\n\nSSH keys are an alternative method for authentication to obtain access to remote computing systems. They can also be used for authentication when transferring files or for accessing version control systems like github.\nThe cluster uses ssh keys to manage batch jobs.\nOn your workstation create ssh key pair ssh-keygen -t ed25519 -a 100\n\n-a (default is 16): number of rounds of passphrase derivation; increase to slow down brute force attacks.\n-t (default is rsa): specify the cryptographic algorithm. ed25519 is faster and shorter than RSA for comparable strength.\n-f (default is /home/user/.ssh/id_algorithm): filename to store your keys. If you already have SSH keys, make sure you specify a different name: ssh-keygen will overwrite the default key if you don’t specify!\n\nIf ed25519 is not available, use the older (but strong and trusted) RSA cryptography: ssh-keygen -a 100 -t rsa -b 4096\nWhen prompted, enter a strong password that you will remember.\nNote: on windows you can use MobaKeyGen from MobaXterm, but on Windows 11 Powershell or Command Prompt works as well.\nIn your ~/.ssh directory you will find a public and private key. Make sure to keep the private key safe as anyone with the private key has access.\nNow, when you add your public key to the ~/.ssh/authorized_keys file in a remote system, your key will be used to login.\nYou can either use copy-paste or the ssh-copy-id command:\n$ ssh-copy-id user@remote-host\nThe authenticity of host 'remote-host (192.168.111.135)' can't be established.\nECDSA key fingerprint is SHA256:hXGpY0ALjXvDUDF1cDs2N8WRO9SuJZ/lfq+9q99BPV0.\nAre you sure you want to continue connecting (yes/no)? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 2 key(s) remain to be installed -- if you are prompted now it is to install the new keys\nuser@remote-host's password:\n\nNumber of key(s) added: 2\nNow try logging into the machine, with: “ssh ‘user@remote-host’” and check to make sure that only the key(s) you wanted were added.\n##Advanced: Forwarding X\n##Fix Warning: Remote Host Identification Has Changed If you are sure that it is harmless and the remote host key has been changed in a legitimate way, you can skip the host key checking by sending the key to a null known_hosts file:\n$ ssh -o \"UserKnownHostsFile=/dev/null\" -o \"StrictHostKeyChecking=no\" username@bazis.labs.vu.nl\nTo make the change permanent remove the offending host key from your ~/.ssh/known_hosts file with:\nssh-keygen -R \"hostname\"\n\nReferences: 1. hpc carpentry 2. Comparing SSH keys 3. public key cryptography for non geeks 4. disable ssh host key checking\n\n\n\n\n\n\nSee jobs in the queue for a given user\n   squeue -u username\nShow available node features\n   sinfo -o \"%20N  %10c  %10m  %25f  %10G \"\nSubmit a job\n   sbatch script\nShow the status of a currently running job\n   sstat -j jobID\nShow the final status of a finished job\n   sacct -j jobID\nCancel a job\n   scancel jobid\n\n\n\nOrganise your input, output and temporary data. Make use of fast scratch directory ($TMPDIR).\nDon’t run large computation on the login nodes! It negatively impacts all cluster users. Grab a compute node with srun –pty bash option.\n\n\n\nThe SLURM constraint option allows for further control over which nodes your job can be scheduled on in a particular parition/queue. You may require a specific processor family or network interconnect. The features that can be used with the sbatch constraint option are defined by the system administrator and thus vary among HPC sites.\nConstraints available on BAZIS are cpu architecture and gpu. Example (singole constraint):\n#SBATCH --constraint=zen2\nExample combining constraints:\n#SBATCH --constraint=\"zen2|haswell\"\n\n\n\nThe parts of a modern computer we need to understand to apply to running jobs are listed here. (Note: This is way oversimplified and intended to give a basic overview for the purposes of understanding how to request resources from Slurm, there are a lot of resources out there to dig deeper into computer architecture.)\n\n\nA physical motherboard which contains one or more of each of Socket, Memory bus and PCI bus.\n\n\n\nA physical socket on a motherboard which accepts a physical CPU part.\n\n\n\nA physical part that is plugged into a socket.\n\n\n\nA physical CPU core, one of many possible cores, that are part of a CPU.\n\n\n\nA virtual CPU thread, associated with a specific Core. This can be enabled or disabled on a system. On BAZIS hyperthreading is typically enabled. Compute intensive workloads will benefit to disable hyperthreading.\n\n\n\nA communication bus between system memory and a Socket/CPU.\n\n\n\nA communication bus between a Socket/CPU and I/O controllers (disks, networking, graphics,…) in the server.\nSlurm complicates this, however, by using the terms core and cpu interchangeably depending on the context and Slurm command. –cpus-per-taks= for example is actually specifying the number of cores per task.\n\n\n\n\n\n\n#!/bin/bash -l\n#SBATCH -J MyTestJob\n#SBATCH -N 1\n#SBATCH -p defq\n\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\necho \"== Scratch dir. : ${TMPDIR}\"\n\ncd $TMPDIR\n# Your more useful application can be started below!\nhostname\n\n\n\n\n\n\n\nEach slurm job will have a fast scratch dir allocated on the nodes which is deleted after finishing the job. use the $TMPDIR virable to use this space for example to store intermediate results or work on many files.\n\n\n\nUse the workspace tools on the beegfs parallel filesystem to create a project space. The project space is a directory, with an associated expiration date, created on behalf of a user, to prevent disks from uncontrolled filling. The beegeefs parallel filesystem is faster than your usual NFS home space, but not backed up, so ideal for data which is easily recreated.\nYour project space lives in the filesystem under: /scratch-shared/ws\nThe project space is managed with the hpc-workspace tooling You can add them to your environment with: module load hpc-workspace\nExample: setup a workspace “MyData” in a batchjob for 10 days.\nSCR=$(ws_allocate MyData 10)\ncd $SCR\nCheck your workspaces\n$ ws_list \nid: MyData\n     workspace directory  : /scratch-shared/ws/username-MyData\n     remaining time       : 9 days 23 hours\n     creation time        : Wed Mar 13 23:51:57 2013\n     expiration date      : Sat Mar 23 23:51:57 2013\n     available extensions : 15\nRelease the project space with\nws_release MyData\nFor user guide see https://github.com/holgerBerger/hpc-workspace/blob/master/user-guide.md\n\n\n\n\nPython has many powerfull packages. In scientific computing many packages may be used in a single project. To manage many python packages often a package manager as conda is used.\nOn a HPC system we do not prefer conda as it does not use optimised binaries and the cache can take up a lot of space, but we understand it is usefull in some cases and try to help.\nWorking with virtual environments further makes the python environment better to manage\n\nA virtual environment is a named, isolated, working copy of Python that that maintains its own files,\ndirectories, and paths so that you can work with specific versions of libraries or Python itself without affecting other Python projects.\nVirtual environmets make it easy to cleanly separate different projects and avoid problems with different dependencies and version requirements across components.\n\nIn short: - use virtualenv (preferred) or conda - create an isolated environment - Install packages - Activate a virtual environment - Deactivate a virtual environment - Delete a virtual environment\n\n\nPython requirements files are a great way to keep track of the Python modules. It is a simple text file that saves a list of the modules and packages required by your project. By creating a Python requirements.txt file, you save yourself the hassle of having to track down and install all of the required modules manually.\nA reuirements file is a simple text file, which looks like this.\ntensorflow==2.3.1\nuvicorn==0.12.2\nfastapi==0.63.0\nInstalling modules from a requirements file is easy as.\npip install -r requirements.txt\nA requirements file can also be generated with:\npip freeze &gt; requirements.txt\nSee the article referenced below for more information.\n\n\n\nThe first line in a script usually starts the interpreter and is called the Shebang. It is recommended to use /usr/bin/env, which can interpret your $PATH. This makes scripts more portable than hard coded paths..\n#!/usr/local/bin/python\nWill only run your script if python is installed in /usr/local/bin.\n#!/usr/bin/env python\nWill interpret your $PATH, and find python in any directory in your $PATH.\nSo your script is more portable, and will work without modification on systems where python is installed as /usr/bin/python, or /usr/local/bin/python, or even custom directories (that have been added to $PATH), like /opt/local/bin/python.\nFurther Reading * python virtual environments primer * python requirements.txt file\n\n\n\n\nR has many powerfull scientific packages and a strong community. Installing and maintaining packages for R can be hard. On BAZIS the Bioconductor suite is installed and can be loaded with the appropiate module environment.\nWhen first running R on a Cluster some changes in the workflow are required making the transition from working interactively from a terminal to scripts in batchmode.\n\n\nPretty much all the time we get errors. Errors can be simple e.g.syntax error, R/python version error or more complex e.g. a problem in our data. In either case, please pay attention to what the error says carefully, because often the solution is in that message or at least it is the starting point of the solution while debugging your code. If it is an error you have not seen before, simply google it. Often you will find a solution in websites like stackoverflow.\n\n\n\n\n\nThe png() default device used the X11 driver, which is not avaialble in batch mode or remote operation. Adding the type=“cairo” option to your code solves this issue.\nExample:\npdf(file = \"testR.pdf\", width = 4, height = 4)\nplot(x = 1:10, y = 1:10)\nabline(v = 0 )\ntext(x=0, y=1, labels = \"random text\")\ndev.off()\npng(file = \"testR.png\", type=\"cairo\", width = 4, height = 4)\nplot(x = 1:10, y = 1:10)\nabline(v = 0 )\ntext(x=0, y=1, labels = \"random text\")\ndev.off()\nReferences * How do I produce PNG graphics in batch mode?\n\n\n\n\n\nMatlab has several features to work in batch mode on a HPC cluster. Assuming you know how to create matlab scripts we start simply by executing matlab interactively on a compute node\n\n\nRequest resources (1 node, 1 cpu) in a partition\nsrun -N 1 -p defq --pty /bin/bash\nmodule load matlab/R2023a\ncd your/data/\nHere is an example of a trivial MATLAB script (hello_world.m):\nfprintf('Hello world.\\n')\nRun with matlab using only one computational thread.\n$ matlab -nodisplay -singleCompThread -r hello_world\nHello world.\n&gt;&gt;\nMatlab waits at the end of the script if there is no exit. In an compute job this would keep the job running untill the wallclocklimit so we add an exit at the end. The convenient “-batch” option combines these options.\n-batch MATLAB_command   - Start MATLAB and execute the MATLAB command(s) with no desktop\n                              and certain interactive capabilities disabled. Terminates\n                              upon successful completion of the command and returns exit\n                              code 0. Upon failure, MATLAB terminates with a non-zero exit.\n                              Cannot be combined with -r.\n$ matlab -batch hello_world\n\n\n\nCombining this in a slurm script we can queue matlab workloads.\n    #!/bin/bash -l\n    #SBATCH -J MyMatlab\n    #SBATCH -N 1\n    #SBATCH --cpus-per-task=1\n    #SBATCH -p defq   \n    #SBATCH --output=%x_%j.out\n    #SBATCH --error=%x_%j.err\n    #SBATCH --mail-type=END,FAIL\n    #SBATCH --mail-user=&lt;YOUR EMAIL&gt;\n    \n    # Note: for parallel operations increase cpus-per-task above\n    # Note 2: output and error logs can be given absolute paths \n    \n    echo \"== Starting run at $(date)\"\n    echo \"== Job ID: ${SLURM_JOBID}\"\n    echo \"== Node list: ${SLURM_NODELIST}\"\n    echo \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\n    echo \"== Scratch dir. : ${TMPDIR}\"\n    \n    # cd $TMPDIR\n    # or change to a project folder with matlab file e.g. hello_World.m\n    # cd your/data\n\n    # Load matlab module\n    module load 2022 matlab/R2023a\n    \n    # execute\n    matlab -batch hello_world\n\n\n\n\n\n\n\nmathworks-parpool\n\n\n\n\n\niRODS/Yoda is a middleware and webinterface to store enriched data. On BAZIS the icommands are available to allow direct access.\nTo use the icommands a configuration file in your home folder is required. https://yoda.vu.nl/site/getting-started/icommands.html#configuration\nAfter configuration use iinit to connect, use a Data Access Password, similar to a WebDAV connection.\nFind an overview of the icommands here: https://docs.irods.org/master/icommands/user/ Check the ipwd, ils, icd, iput, iget en irsync commands for navigation and data transfer.\nYour project folder is located at /vu/home\nicommands Yoda-VU"
  },
  {
    "objectID": "topics/bazis.html#connect",
    "href": "topics/bazis.html#connect",
    "title": "BAZIS HPC",
    "section": "",
    "text": "Use your favourite SSH client to login at (bazis.labs.vu.nl?). On windows we recommend MobaXterm or MS Terminal; Mac users can use iterm. Direct access is only possible from the Campus or from SURF. From other network locations first connect to the stepstone (ssh.data.vu.nl?), or use eduVPN Institutional Access (below).\n\n\n\nInstall the client. Start eduVPN and choose Institute Access to connect. You will need MFA to authenticate. Students can activate MFA at the servicedesk ( kb-item 11809 )\n\n\n\nMobaXterm has an integrated FTP file browser. Once you have logged in to the HPC system, you will see the file browser to the left of the terminal window, where it shows the contents of your home folder. You can browse through these folders, and drag-and-drop files and folders between this FTP file browser and the Windows File Explorer. Alternatively, you can use the download/upload buttons at the top of the FTP file browser window. A green refresh button is also located there to refresh the contents of the current folder. You can also open files in the FTP file browser to edit them directly. Upon saving, you’ll be asked if you want change these files on the HPC system.\nOther SFTP browsers There are a large number of free FTP browser out there. Some examples are\n\nFilezilla (Windows, MacOS, Linux)\nCyberduck (Windows, MacOS)\nWinSCP (Windows)\n\n\n\n\nSSH keys are an alternative method for authentication to obtain access to remote computing systems. They can also be used for authentication when transferring files or for accessing version control systems like github.\nThe cluster uses ssh keys to manage batch jobs.\nOn your workstation create ssh key pair ssh-keygen -t ed25519 -a 100\n\n-a (default is 16): number of rounds of passphrase derivation; increase to slow down brute force attacks.\n-t (default is rsa): specify the cryptographic algorithm. ed25519 is faster and shorter than RSA for comparable strength.\n-f (default is /home/user/.ssh/id_algorithm): filename to store your keys. If you already have SSH keys, make sure you specify a different name: ssh-keygen will overwrite the default key if you don’t specify!\n\nIf ed25519 is not available, use the older (but strong and trusted) RSA cryptography: ssh-keygen -a 100 -t rsa -b 4096\nWhen prompted, enter a strong password that you will remember.\nNote: on windows you can use MobaKeyGen from MobaXterm, but on Windows 11 Powershell or Command Prompt works as well.\nIn your ~/.ssh directory you will find a public and private key. Make sure to keep the private key safe as anyone with the private key has access.\nNow, when you add your public key to the ~/.ssh/authorized_keys file in a remote system, your key will be used to login.\nYou can either use copy-paste or the ssh-copy-id command:\n$ ssh-copy-id user@remote-host\nThe authenticity of host 'remote-host (192.168.111.135)' can't be established.\nECDSA key fingerprint is SHA256:hXGpY0ALjXvDUDF1cDs2N8WRO9SuJZ/lfq+9q99BPV0.\nAre you sure you want to continue connecting (yes/no)? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 2 key(s) remain to be installed -- if you are prompted now it is to install the new keys\nuser@remote-host's password:\n\nNumber of key(s) added: 2\nNow try logging into the machine, with: “ssh ‘user@remote-host’” and check to make sure that only the key(s) you wanted were added.\n##Advanced: Forwarding X\n##Fix Warning: Remote Host Identification Has Changed If you are sure that it is harmless and the remote host key has been changed in a legitimate way, you can skip the host key checking by sending the key to a null known_hosts file:\n$ ssh -o \"UserKnownHostsFile=/dev/null\" -o \"StrictHostKeyChecking=no\" username@bazis.labs.vu.nl\nTo make the change permanent remove the offending host key from your ~/.ssh/known_hosts file with:\nssh-keygen -R \"hostname\"\n\nReferences: 1. hpc carpentry 2. Comparing SSH keys 3. public key cryptography for non geeks 4. disable ssh host key checking"
  },
  {
    "objectID": "topics/bazis.html#using-slurm",
    "href": "topics/bazis.html#using-slurm",
    "title": "BAZIS HPC",
    "section": "",
    "text": "See jobs in the queue for a given user\n   squeue -u username\nShow available node features\n   sinfo -o \"%20N  %10c  %10m  %25f  %10G \"\nSubmit a job\n   sbatch script\nShow the status of a currently running job\n   sstat -j jobID\nShow the final status of a finished job\n   sacct -j jobID\nCancel a job\n   scancel jobid\n\n\n\nOrganise your input, output and temporary data. Make use of fast scratch directory ($TMPDIR).\nDon’t run large computation on the login nodes! It negatively impacts all cluster users. Grab a compute node with srun –pty bash option.\n\n\n\nThe SLURM constraint option allows for further control over which nodes your job can be scheduled on in a particular parition/queue. You may require a specific processor family or network interconnect. The features that can be used with the sbatch constraint option are defined by the system administrator and thus vary among HPC sites.\nConstraints available on BAZIS are cpu architecture and gpu. Example (singole constraint):\n#SBATCH --constraint=zen2\nExample combining constraints:\n#SBATCH --constraint=\"zen2|haswell\"\n\n\n\nThe parts of a modern computer we need to understand to apply to running jobs are listed here. (Note: This is way oversimplified and intended to give a basic overview for the purposes of understanding how to request resources from Slurm, there are a lot of resources out there to dig deeper into computer architecture.)\n\n\nA physical motherboard which contains one or more of each of Socket, Memory bus and PCI bus.\n\n\n\nA physical socket on a motherboard which accepts a physical CPU part.\n\n\n\nA physical part that is plugged into a socket.\n\n\n\nA physical CPU core, one of many possible cores, that are part of a CPU.\n\n\n\nA virtual CPU thread, associated with a specific Core. This can be enabled or disabled on a system. On BAZIS hyperthreading is typically enabled. Compute intensive workloads will benefit to disable hyperthreading.\n\n\n\nA communication bus between system memory and a Socket/CPU.\n\n\n\nA communication bus between a Socket/CPU and I/O controllers (disks, networking, graphics,…) in the server.\nSlurm complicates this, however, by using the terms core and cpu interchangeably depending on the context and Slurm command. –cpus-per-taks= for example is actually specifying the number of cores per task.\n\n\n\n\n\n\n#!/bin/bash -l\n#SBATCH -J MyTestJob\n#SBATCH -N 1\n#SBATCH -p defq\n\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\necho \"== Scratch dir. : ${TMPDIR}\"\n\ncd $TMPDIR\n# Your more useful application can be started below!\nhostname"
  },
  {
    "objectID": "topics/bazis.html#workspace",
    "href": "topics/bazis.html#workspace",
    "title": "BAZIS HPC",
    "section": "",
    "text": "Each slurm job will have a fast scratch dir allocated on the nodes which is deleted after finishing the job. use the $TMPDIR virable to use this space for example to store intermediate results or work on many files.\n\n\n\nUse the workspace tools on the beegfs parallel filesystem to create a project space. The project space is a directory, with an associated expiration date, created on behalf of a user, to prevent disks from uncontrolled filling. The beegeefs parallel filesystem is faster than your usual NFS home space, but not backed up, so ideal for data which is easily recreated.\nYour project space lives in the filesystem under: /scratch-shared/ws\nThe project space is managed with the hpc-workspace tooling You can add them to your environment with: module load hpc-workspace\nExample: setup a workspace “MyData” in a batchjob for 10 days.\nSCR=$(ws_allocate MyData 10)\ncd $SCR\nCheck your workspaces\n$ ws_list \nid: MyData\n     workspace directory  : /scratch-shared/ws/username-MyData\n     remaining time       : 9 days 23 hours\n     creation time        : Wed Mar 13 23:51:57 2013\n     expiration date      : Sat Mar 23 23:51:57 2013\n     available extensions : 15\nRelease the project space with\nws_release MyData\nFor user guide see https://github.com/holgerBerger/hpc-workspace/blob/master/user-guide.md"
  },
  {
    "objectID": "topics/bazis.html#python-virtual-environments",
    "href": "topics/bazis.html#python-virtual-environments",
    "title": "BAZIS HPC",
    "section": "",
    "text": "Python has many powerfull packages. In scientific computing many packages may be used in a single project. To manage many python packages often a package manager as conda is used.\nOn a HPC system we do not prefer conda as it does not use optimised binaries and the cache can take up a lot of space, but we understand it is usefull in some cases and try to help.\nWorking with virtual environments further makes the python environment better to manage\n\nA virtual environment is a named, isolated, working copy of Python that that maintains its own files,\ndirectories, and paths so that you can work with specific versions of libraries or Python itself without affecting other Python projects.\nVirtual environmets make it easy to cleanly separate different projects and avoid problems with different dependencies and version requirements across components.\n\nIn short: - use virtualenv (preferred) or conda - create an isolated environment - Install packages - Activate a virtual environment - Deactivate a virtual environment - Delete a virtual environment\n\n\nPython requirements files are a great way to keep track of the Python modules. It is a simple text file that saves a list of the modules and packages required by your project. By creating a Python requirements.txt file, you save yourself the hassle of having to track down and install all of the required modules manually.\nA reuirements file is a simple text file, which looks like this.\ntensorflow==2.3.1\nuvicorn==0.12.2\nfastapi==0.63.0\nInstalling modules from a requirements file is easy as.\npip install -r requirements.txt\nA requirements file can also be generated with:\npip freeze &gt; requirements.txt\nSee the article referenced below for more information.\n\n\n\nThe first line in a script usually starts the interpreter and is called the Shebang. It is recommended to use /usr/bin/env, which can interpret your $PATH. This makes scripts more portable than hard coded paths..\n#!/usr/local/bin/python\nWill only run your script if python is installed in /usr/local/bin.\n#!/usr/bin/env python\nWill interpret your $PATH, and find python in any directory in your $PATH.\nSo your script is more portable, and will work without modification on systems where python is installed as /usr/bin/python, or /usr/local/bin/python, or even custom directories (that have been added to $PATH), like /opt/local/bin/python.\nFurther Reading * python virtual environments primer * python requirements.txt file"
  },
  {
    "objectID": "topics/bazis.html#r-environment",
    "href": "topics/bazis.html#r-environment",
    "title": "BAZIS HPC",
    "section": "",
    "text": "R has many powerfull scientific packages and a strong community. Installing and maintaining packages for R can be hard. On BAZIS the Bioconductor suite is installed and can be loaded with the appropiate module environment.\nWhen first running R on a Cluster some changes in the workflow are required making the transition from working interactively from a terminal to scripts in batchmode.\n\n\nPretty much all the time we get errors. Errors can be simple e.g.syntax error, R/python version error or more complex e.g. a problem in our data. In either case, please pay attention to what the error says carefully, because often the solution is in that message or at least it is the starting point of the solution while debugging your code. If it is an error you have not seen before, simply google it. Often you will find a solution in websites like stackoverflow.\n\n\n\n\n\nThe png() default device used the X11 driver, which is not avaialble in batch mode or remote operation. Adding the type=“cairo” option to your code solves this issue.\nExample:\npdf(file = \"testR.pdf\", width = 4, height = 4)\nplot(x = 1:10, y = 1:10)\nabline(v = 0 )\ntext(x=0, y=1, labels = \"random text\")\ndev.off()\npng(file = \"testR.png\", type=\"cairo\", width = 4, height = 4)\nplot(x = 1:10, y = 1:10)\nabline(v = 0 )\ntext(x=0, y=1, labels = \"random text\")\ndev.off()\nReferences * How do I produce PNG graphics in batch mode?"
  },
  {
    "objectID": "topics/bazis.html#matlab",
    "href": "topics/bazis.html#matlab",
    "title": "BAZIS HPC",
    "section": "",
    "text": "Matlab has several features to work in batch mode on a HPC cluster. Assuming you know how to create matlab scripts we start simply by executing matlab interactively on a compute node\n\n\nRequest resources (1 node, 1 cpu) in a partition\nsrun -N 1 -p defq --pty /bin/bash\nmodule load matlab/R2023a\ncd your/data/\nHere is an example of a trivial MATLAB script (hello_world.m):\nfprintf('Hello world.\\n')\nRun with matlab using only one computational thread.\n$ matlab -nodisplay -singleCompThread -r hello_world\nHello world.\n&gt;&gt;\nMatlab waits at the end of the script if there is no exit. In an compute job this would keep the job running untill the wallclocklimit so we add an exit at the end. The convenient “-batch” option combines these options.\n-batch MATLAB_command   - Start MATLAB and execute the MATLAB command(s) with no desktop\n                              and certain interactive capabilities disabled. Terminates\n                              upon successful completion of the command and returns exit\n                              code 0. Upon failure, MATLAB terminates with a non-zero exit.\n                              Cannot be combined with -r.\n$ matlab -batch hello_world\n\n\n\nCombining this in a slurm script we can queue matlab workloads.\n    #!/bin/bash -l\n    #SBATCH -J MyMatlab\n    #SBATCH -N 1\n    #SBATCH --cpus-per-task=1\n    #SBATCH -p defq   \n    #SBATCH --output=%x_%j.out\n    #SBATCH --error=%x_%j.err\n    #SBATCH --mail-type=END,FAIL\n    #SBATCH --mail-user=&lt;YOUR EMAIL&gt;\n    \n    # Note: for parallel operations increase cpus-per-task above\n    # Note 2: output and error logs can be given absolute paths \n    \n    echo \"== Starting run at $(date)\"\n    echo \"== Job ID: ${SLURM_JOBID}\"\n    echo \"== Node list: ${SLURM_NODELIST}\"\n    echo \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\n    echo \"== Scratch dir. : ${TMPDIR}\"\n    \n    # cd $TMPDIR\n    # or change to a project folder with matlab file e.g. hello_World.m\n    # cd your/data\n\n    # Load matlab module\n    module load 2022 matlab/R2023a\n    \n    # execute\n    matlab -batch hello_world\n\n\n\n\n\n\n\nmathworks-parpool"
  },
  {
    "objectID": "topics/bazis.html#connect-to-vu-yodairods-with-icommands",
    "href": "topics/bazis.html#connect-to-vu-yodairods-with-icommands",
    "title": "BAZIS HPC",
    "section": "",
    "text": "iRODS/Yoda is a middleware and webinterface to store enriched data. On BAZIS the icommands are available to allow direct access.\nTo use the icommands a configuration file in your home folder is required. https://yoda.vu.nl/site/getting-started/icommands.html#configuration\nAfter configuration use iinit to connect, use a Data Access Password, similar to a WebDAV connection.\nFind an overview of the icommands here: https://docs.irods.org/master/icommands/user/ Check the ipwd, ils, icd, iput, iget en irsync commands for navigation and data transfer.\nYour project folder is located at /vu/home\nicommands Yoda-VU"
  },
  {
    "objectID": "topics/data-collection.html",
    "href": "topics/data-collection.html",
    "title": "{{< var title >}}",
    "section": "",
    "text": "Data collection may consist of the re-use of existing data and/or the generation of new data. You can find more specific information on the re-use of existing data on the Finding Existing Data page in this LibGuide\nFor data to be considered valid and reliable, data collection should occur consistently and systematically throughout the course of the research project. Data collection guidelines and established methodologies should be used to gather data. Some disciplines make use of codebooks, whereas others use protocols for data gathering. These procedures help researchers collect data according to conventional methodological steps. If a research project involves multiple partners (in a consortium) it should be clear who is responsible for the collection of what (part of the) data. Important aspects of data collection include:\n\nStandardisation: codebooks & protocols\nStructure / organisation of the data\nData quality assurance methods\nDocumentation & metadata\nStorage & protection\n\nThis relates to the Reproducibility of your research according to the FAIR-data principles.\n\n\nThe tools being used in research to collect data are immensely diverse. For that reason, we will not provide an exhaustive overview here. What is important for data collection tools in relation to RDM is where such tools store the data that you collect and in which format. The storage location is particularly important when you are working with personal data. For example, the privacy legislation in the United States is very different from the European General Data Protection Regulation (GDPR). Hence, personal data collected in a Dutch research institute may not be stored on American servers. It is important to keep that in mind when you are contemplating which tool to use for your data collection.\nIf you are collecting personal data and you decide to use a tool for which no contract exists between VU Amsterdam and the provider of the software or tool, a service agreement and a processing agreement must be drawn up. Contact the privacy champion of your faculty for more information and a model processing agreement.\nQuestionnaire tools\nThe Faculty of Behavioural and Movement Sciences has developed a document with tips for safe use of the questionnaire tools Qualtrics and Survalyzer. The document was made for FGB researchers specifically but can also be helpful for others. Consult this document if you need a questionnaire tool to collect your data.\n\n\n\nSome research projects involve the participation of multiple organisations or institutes and may include even cross-border co-operation. When data is collected by several organisations, a Data Management Plan should provide information on who is responsible for which part of the data collection and storage. It should also provide information on how specific data collections are related to which part(s) of the research goal(s). Describing this precisely will help you to determine if a consortium agreement or joint controller agreement is necessary. You see a general example of such a specification in the table below:\n\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nResponsible organization for collection\nData origin\nData purpose\n\n\n\n\nRaw data\nCommunity level surveys\nVrije Universiteit Amsterdam\nAmsterdam, The Hague, Rotterdam\nIdentifying perceived problems, System responsiveness\n\n\nRaw data\nTrials & Focus Group Interviews\nLondon School of Hygiene and Tropical Medicine (LSHTM)\nGermany, Switzerland\nTrials to evaluate programs on . . ., Focus Group interviews to identify barriers to . . .\n\n\nRaw data\nPollution measurements using fish\nOceanographic Institute of Sweden\nCoastal waters, Northeast Spain\nEstablish pollution levels of plastic\n\n\n\n\n\n\nRegardless of the field of study or preference for defining data (quantitative, qualitative), accurate data collection is essential to maintaining the integrity (structure) of research. Both the selection of appropriate data collection instruments (existing, modified, or newly developed) and clearly delineated instructions for their correct use reduce the likelihood of errors.\nThere are two approaches for reducing and/or detecting errors in data which can help to preserve the integrity of your data and ensure scientific validity. These are:\n\nQuality assurance - activities that take place before data collection begins\nQuality control - activities that take place during and after data collection\n\nQuality assurance precedes data collection and its main focus is ‘prevention’ (i.e., forestalling problems with data collection). Prevention is the most cost-effective activity to ensure the integrity of data collection. This proactive measure is best demonstrated by the standardization of protocol developed in a comprehensive and detailed procedures manual for data collection.\nWhile quality control activities (detection/monitoring and action) occur during and after data collection, the details should be carefully documented in the procedures manual. A clearly defined communication structure is a necessary pre-condition for monitoring and tracking down errors. Quality control also identifies the required responses, or ‘actions’ necessary to correct faulty data collection practices and also minimise future occurrences.\nSome sources for protocols:\n\nHANDS Handbook for Adequate Natural Data Stewardship by the Federation of Dutch University Medical Centers (UMCs)\nProtocols.io - an open access repository of protocols\nProtocols Online - website with protocols available on the internet, sorted by discipline.\nSpringer Protocols - free and subscribed protocols collected by Springer."
  },
  {
    "objectID": "topics/data-collection.html#data-collection",
    "href": "topics/data-collection.html#data-collection",
    "title": "{{< var title >}}",
    "section": "",
    "text": "Data collection may consist of the re-use of existing data and/or the generation of new data. You can find more specific information on the re-use of existing data on the Finding Existing Data page in this LibGuide\nFor data to be considered valid and reliable, data collection should occur consistently and systematically throughout the course of the research project. Data collection guidelines and established methodologies should be used to gather data. Some disciplines make use of codebooks, whereas others use protocols for data gathering. These procedures help researchers collect data according to conventional methodological steps. If a research project involves multiple partners (in a consortium) it should be clear who is responsible for the collection of what (part of the) data. Important aspects of data collection include:\n\nStandardisation: codebooks & protocols\nStructure / organisation of the data\nData quality assurance methods\nDocumentation & metadata\nStorage & protection\n\nThis relates to the Reproducibility of your research according to the FAIR-data principles.\n\n\nThe tools being used in research to collect data are immensely diverse. For that reason, we will not provide an exhaustive overview here. What is important for data collection tools in relation to RDM is where such tools store the data that you collect and in which format. The storage location is particularly important when you are working with personal data. For example, the privacy legislation in the United States is very different from the European General Data Protection Regulation (GDPR). Hence, personal data collected in a Dutch research institute may not be stored on American servers. It is important to keep that in mind when you are contemplating which tool to use for your data collection.\nIf you are collecting personal data and you decide to use a tool for which no contract exists between VU Amsterdam and the provider of the software or tool, a service agreement and a processing agreement must be drawn up. Contact the privacy champion of your faculty for more information and a model processing agreement.\nQuestionnaire tools\nThe Faculty of Behavioural and Movement Sciences has developed a document with tips for safe use of the questionnaire tools Qualtrics and Survalyzer. The document was made for FGB researchers specifically but can also be helpful for others. Consult this document if you need a questionnaire tool to collect your data.\n\n\n\nSome research projects involve the participation of multiple organisations or institutes and may include even cross-border co-operation. When data is collected by several organisations, a Data Management Plan should provide information on who is responsible for which part of the data collection and storage. It should also provide information on how specific data collections are related to which part(s) of the research goal(s). Describing this precisely will help you to determine if a consortium agreement or joint controller agreement is necessary. You see a general example of such a specification in the table below:\n\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nResponsible organization for collection\nData origin\nData purpose\n\n\n\n\nRaw data\nCommunity level surveys\nVrije Universiteit Amsterdam\nAmsterdam, The Hague, Rotterdam\nIdentifying perceived problems, System responsiveness\n\n\nRaw data\nTrials & Focus Group Interviews\nLondon School of Hygiene and Tropical Medicine (LSHTM)\nGermany, Switzerland\nTrials to evaluate programs on . . ., Focus Group interviews to identify barriers to . . .\n\n\nRaw data\nPollution measurements using fish\nOceanographic Institute of Sweden\nCoastal waters, Northeast Spain\nEstablish pollution levels of plastic\n\n\n\n\n\n\nRegardless of the field of study or preference for defining data (quantitative, qualitative), accurate data collection is essential to maintaining the integrity (structure) of research. Both the selection of appropriate data collection instruments (existing, modified, or newly developed) and clearly delineated instructions for their correct use reduce the likelihood of errors.\nThere are two approaches for reducing and/or detecting errors in data which can help to preserve the integrity of your data and ensure scientific validity. These are:\n\nQuality assurance - activities that take place before data collection begins\nQuality control - activities that take place during and after data collection\n\nQuality assurance precedes data collection and its main focus is ‘prevention’ (i.e., forestalling problems with data collection). Prevention is the most cost-effective activity to ensure the integrity of data collection. This proactive measure is best demonstrated by the standardization of protocol developed in a comprehensive and detailed procedures manual for data collection.\nWhile quality control activities (detection/monitoring and action) occur during and after data collection, the details should be carefully documented in the procedures manual. A clearly defined communication structure is a necessary pre-condition for monitoring and tracking down errors. Quality control also identifies the required responses, or ‘actions’ necessary to correct faulty data collection practices and also minimise future occurrences.\nSome sources for protocols:\n\nHANDS Handbook for Adequate Natural Data Stewardship by the Federation of Dutch University Medical Centers (UMCs)\nProtocols.io - an open access repository of protocols\nProtocols Online - website with protocols available on the internet, sorted by discipline.\nSpringer Protocols - free and subscribed protocols collected by Springer."
  },
  {
    "objectID": "topics/data-protection.html",
    "href": "topics/data-protection.html",
    "title": "{{< var title >}}",
    "section": "",
    "text": "Protection from what? From whom? When, and why? Before we talk about data protection, let us consider security first. More often than not, ‘security’ is regarded as a fixed state. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe question for you as a researcher is ‘when are the measures that you take secure enough?’. In order to answer this, please be aware that there are three entities that have an opinion about what is ‘secure enough’, namely: the law, the University, and you yourself as the data processor.\nThe University has a Security Baseline that sets a norm for levels of protection for every application it uses. The Baseline is based on international standards. For each of these applications, the University is considering for which means the security of these applications are adequate enough.\nThe legal requirements for the processing of personal data can be found in the section ‘GDPR and Privacy’ under Plan & Design There are additional laws and regulations as well. The assumption is that you are familiar with these, especially with laws regulating medical and criminal research.\nWhat you personally consider to be secure might be very different from what your colleagues, the Faculty or the University considers to be secure enough and the norms will vary with the variety of data that is being processed by different researchers and Faculties of the VU. Very generally speaking, there are three points of protection to consider:\n\nProtection against data loss, for which you need a back up periodically.\nProtection against data leakage, for which you need to consider all storage places and their access points.\nProtection of data integrity, for which you need version control and synchronisation management.\n\nThe security of your protection measures depends on the threat you face. We often think of threats as active, and motivated by bad intentions. But most common forms of data loss are accidental and most leakage is caused by trusting others. In reality, devices just get lost or break down, people download malware by accident, and each one of us forgets to save a document at times or gets confused about which version was last updated.\nIn all cases, protection starts with oversight on where your data is stored and processed. If you forget that you temporarily stored it in a certain place, you have then lost oversight of where that data is. The opposite is also true: if you know where you data is, you have insight in the level of security of the space in which you store it. As you can see, protection begins with organising your work in a reliable manner and thinking through your steps.\nFor example, if you data is on your laptop and synchronised with your phone, then it is stored in two places. Perhaps this is enough back up, perhaps not. If you put both you devices in the same bag and you lose your bag, you have no backup. A backup to an online storage might be a good solution, but might also mean your data leaks via the internet of via the storage provider who sells the data and your behavioural data for profit. Most importantly, there is no absolute security. It is best if you consider your personal behaviour and then think of scenarios that are more or less likely to happen and what would impact you most. If you frequently work in public places you should make it a habit to lock your device each time you leave it. If you eat and drink behind your desk often, better work with a remote keyboard to protect your laptop from the unavoidable coffee shower. Do you save your respondents’ contact details on your personal phone? Then protect it with a pin.\nHere are some basic protection guidelines:\n\nData are very difficult to erase. You have probably never done it.\nDecide how to back up data and test it before you rely on it.\nDo not give others your log-in credentials. If you have done so and your family members use your work device, then change it.\nDo not use passwords twice, do not use your birthday, initials, streetname, hobby.\nEncryption sounds secure, but it fails completely without good password management.\n\n\n\n\nThere can be many reasons why the data of a project needs to be kept protected:\n\nSensitivity of the data collected\nProtection of the research data from competition\nCommercial reasons / Intellectual property\nEtc.\n\n\nThere are also many levels of security that may be implemented, depending on the needs. Sometimes it will be enough to use a password-protected cloud-based server. In extreme cases encryption may be needed and also when data is transmitted between researchers or organisations. You should contact the RDM Support Desk to discuss available options, who may connect you to legal experts where sensitive data is concerned. Check the Data Storage section of this LibGuide with links to find out more on campus solutions and cloud-based options.\n\n\n\nIt is important to protect your data during the entire data life cycle. To find out whether your data are secure during all stages of your research, think about your data flow: where do your data originate and where do they go to? If data need to be transported from one physical place to the other, or need to be transferred from one device to another, these actions should happen in a secure way.\n\n\n\n\nIf data collection takes place through a certain measurement device (e.g. MRI scanner, EEG scanner, eye tracker), the data need to be transferred from the measurement device to the storage location that you will use during your research project. Make sure that this transfer takes place in a secure way and also make a plan for the data on the measurement device; find out whether they need to be destroyed or can remain there.\n\n\n\nIf you are doing fieldwork outside the campus and you have reliable and secure internet access, it is a good idea to upload the data to a storage location that is regularly backed up and secure, in order to prevent data loss. If you have a VUnetID, you can for example use:\n\nSURFdrive to store your data in a secure cloud service\nSURFfilesender to send you data to a colleague or consortium partner, who can store your data in an appropriate place\n\nYou can find more information about each of these storage options on the Data Storage page of this LibGuide.\nIf you need to receive data from colleagues in your project who don’t have access to these tools (e.g. because they are students, don’t work for a Dutch educational institution, or have no VUnetID), SURFdrive, SURFfilesender and Edugroepen can also be used:\n\nSURFdrive: you can set up a ‘File drop’ folder. By sharing the link of this folder to the researchers who need to upload documents, you enable them to do anonymous uploads to this folder. These users have solely upload rights, no view or download rights. The folder can be protected with a password, which you preferably share with the uploaders through another channel.\nSURFfilesender: as a SURFfilesender user, you can send a voucher to someone who doesn’t have access to this tool. This person can use this voucher to send documents to you. These files can be encrypted.\nZivver is an email plugin with which you can encrypt emails and attachments.\n\n\n\n\nIf you are doing fieldwork in an area with limited internet access, you might use a portable device to initially store your data during the phase of data collection, such as a USB drive or an external hard drive. These data can be transferred to a storage location that is connected to the internet (e.g. G-drive, SURFdrive) later. Please make sure that the data on such portable devices are secured, by using encryption (and by transporting them safely by using a lockable briefcase or backpack).\n\n\n\n\nIf physical objects need to be transported, you should check with the data manager at your department (if available) what options are available. Special briefcases that can be locked or secure backpacks may need to be used to keep informed consent forms or other sensitive data objects (USB drives etc.) secure during transport.\n\n\n\nSome countries have rules to control the movement of encryption technology that enter or exit their borders. If you need to travel with an encrypted laptop to secure your data, for example during fieldwork abroad, please keep this in mind. If you need to transfer data in and out of such countries, please get advice on encryption and secure transportation at the IT Service Desk.\nIf you have general questions about how to protect your data when transporting or transferring them, you can contact the IT Service Desk. In case of complex situations for which you need tailored support, you can consult the IT Relationship Manager representing the research domain, who can request capacity at IT for setting up an information security plan. Such a plan is usually based on documents which need to be completed beforehand, like a Data Protection Impact Assessment and a Data Classification. Please note that IT-capacity for tailored support is a paid service for which budget needs to be reserved."
  },
  {
    "objectID": "topics/data-protection.html#data-protection",
    "href": "topics/data-protection.html#data-protection",
    "title": "{{< var title >}}",
    "section": "",
    "text": "Protection from what? From whom? When, and why? Before we talk about data protection, let us consider security first. More often than not, ‘security’ is regarded as a fixed state. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe question for you as a researcher is ‘when are the measures that you take secure enough?’. In order to answer this, please be aware that there are three entities that have an opinion about what is ‘secure enough’, namely: the law, the University, and you yourself as the data processor.\nThe University has a Security Baseline that sets a norm for levels of protection for every application it uses. The Baseline is based on international standards. For each of these applications, the University is considering for which means the security of these applications are adequate enough.\nThe legal requirements for the processing of personal data can be found in the section ‘GDPR and Privacy’ under Plan & Design There are additional laws and regulations as well. The assumption is that you are familiar with these, especially with laws regulating medical and criminal research.\nWhat you personally consider to be secure might be very different from what your colleagues, the Faculty or the University considers to be secure enough and the norms will vary with the variety of data that is being processed by different researchers and Faculties of the VU. Very generally speaking, there are three points of protection to consider:\n\nProtection against data loss, for which you need a back up periodically.\nProtection against data leakage, for which you need to consider all storage places and their access points.\nProtection of data integrity, for which you need version control and synchronisation management.\n\nThe security of your protection measures depends on the threat you face. We often think of threats as active, and motivated by bad intentions. But most common forms of data loss are accidental and most leakage is caused by trusting others. In reality, devices just get lost or break down, people download malware by accident, and each one of us forgets to save a document at times or gets confused about which version was last updated.\nIn all cases, protection starts with oversight on where your data is stored and processed. If you forget that you temporarily stored it in a certain place, you have then lost oversight of where that data is. The opposite is also true: if you know where you data is, you have insight in the level of security of the space in which you store it. As you can see, protection begins with organising your work in a reliable manner and thinking through your steps.\nFor example, if you data is on your laptop and synchronised with your phone, then it is stored in two places. Perhaps this is enough back up, perhaps not. If you put both you devices in the same bag and you lose your bag, you have no backup. A backup to an online storage might be a good solution, but might also mean your data leaks via the internet of via the storage provider who sells the data and your behavioural data for profit. Most importantly, there is no absolute security. It is best if you consider your personal behaviour and then think of scenarios that are more or less likely to happen and what would impact you most. If you frequently work in public places you should make it a habit to lock your device each time you leave it. If you eat and drink behind your desk often, better work with a remote keyboard to protect your laptop from the unavoidable coffee shower. Do you save your respondents’ contact details on your personal phone? Then protect it with a pin.\nHere are some basic protection guidelines:\n\nData are very difficult to erase. You have probably never done it.\nDecide how to back up data and test it before you rely on it.\nDo not give others your log-in credentials. If you have done so and your family members use your work device, then change it.\nDo not use passwords twice, do not use your birthday, initials, streetname, hobby.\nEncryption sounds secure, but it fails completely without good password management.\n\n\n\n\nThere can be many reasons why the data of a project needs to be kept protected:\n\nSensitivity of the data collected\nProtection of the research data from competition\nCommercial reasons / Intellectual property\nEtc.\n\n\nThere are also many levels of security that may be implemented, depending on the needs. Sometimes it will be enough to use a password-protected cloud-based server. In extreme cases encryption may be needed and also when data is transmitted between researchers or organisations. You should contact the RDM Support Desk to discuss available options, who may connect you to legal experts where sensitive data is concerned. Check the Data Storage section of this LibGuide with links to find out more on campus solutions and cloud-based options.\n\n\n\nIt is important to protect your data during the entire data life cycle. To find out whether your data are secure during all stages of your research, think about your data flow: where do your data originate and where do they go to? If data need to be transported from one physical place to the other, or need to be transferred from one device to another, these actions should happen in a secure way.\n\n\n\n\nIf data collection takes place through a certain measurement device (e.g. MRI scanner, EEG scanner, eye tracker), the data need to be transferred from the measurement device to the storage location that you will use during your research project. Make sure that this transfer takes place in a secure way and also make a plan for the data on the measurement device; find out whether they need to be destroyed or can remain there.\n\n\n\nIf you are doing fieldwork outside the campus and you have reliable and secure internet access, it is a good idea to upload the data to a storage location that is regularly backed up and secure, in order to prevent data loss. If you have a VUnetID, you can for example use:\n\nSURFdrive to store your data in a secure cloud service\nSURFfilesender to send you data to a colleague or consortium partner, who can store your data in an appropriate place\n\nYou can find more information about each of these storage options on the Data Storage page of this LibGuide.\nIf you need to receive data from colleagues in your project who don’t have access to these tools (e.g. because they are students, don’t work for a Dutch educational institution, or have no VUnetID), SURFdrive, SURFfilesender and Edugroepen can also be used:\n\nSURFdrive: you can set up a ‘File drop’ folder. By sharing the link of this folder to the researchers who need to upload documents, you enable them to do anonymous uploads to this folder. These users have solely upload rights, no view or download rights. The folder can be protected with a password, which you preferably share with the uploaders through another channel.\nSURFfilesender: as a SURFfilesender user, you can send a voucher to someone who doesn’t have access to this tool. This person can use this voucher to send documents to you. These files can be encrypted.\nZivver is an email plugin with which you can encrypt emails and attachments.\n\n\n\n\nIf you are doing fieldwork in an area with limited internet access, you might use a portable device to initially store your data during the phase of data collection, such as a USB drive or an external hard drive. These data can be transferred to a storage location that is connected to the internet (e.g. G-drive, SURFdrive) later. Please make sure that the data on such portable devices are secured, by using encryption (and by transporting them safely by using a lockable briefcase or backpack).\n\n\n\n\nIf physical objects need to be transported, you should check with the data manager at your department (if available) what options are available. Special briefcases that can be locked or secure backpacks may need to be used to keep informed consent forms or other sensitive data objects (USB drives etc.) secure during transport.\n\n\n\nSome countries have rules to control the movement of encryption technology that enter or exit their borders. If you need to travel with an encrypted laptop to secure your data, for example during fieldwork abroad, please keep this in mind. If you need to transfer data in and out of such countries, please get advice on encryption and secure transportation at the IT Service Desk.\nIf you have general questions about how to protect your data when transporting or transferring them, you can contact the IT Service Desk. In case of complex situations for which you need tailored support, you can consult the IT Relationship Manager representing the research domain, who can request capacity at IT for setting up an information security plan. Such a plan is usually based on documents which need to be completed beforehand, like a Data Protection Impact Assessment and a Data Classification. Please note that IT-capacity for tailored support is a paid service for which budget needs to be reserved."
  },
  {
    "objectID": "topics/data-storage.html",
    "href": "topics/data-storage.html",
    "title": "{{< var title >}}",
    "section": "",
    "text": "VU Amsterdam offers several options to store your research data. The choice for a specific option may depend on factors such as:\n\nDoes a project involve multiple organisations or departments?\nThe sensitivity of the data: does it involve personal data or copyrighted / commercial data?\nAre there any research partners with whom data need to be shared?\nAre any commercial parties involved?\nDoes the research project involve multiple locations (inside or maybe even outside the EU)?\nWill there be (lab) devices producing data that need to be stored as well?\nWhat will be the volume of the data?\nWill there be lots of interactions with the data (using software/tools)?\n\nStorage options may take several forms, for example:\n\nLocal storage on computers, networks or servers;\nCloud storage offered by the VU;\nLocations where physical data samples are stored (fridges, lockers, etc.).\n\nResearchers, including PhD candidates, have multiple options that can be used, some of which are listed below. More information about these storage options is available behind their respective links. The Storage finder is a tool that will give you a number of storage options suitable for your research. For more individual guidance, please get in touch with the Research Data Management Support Desk for advice, particularly when you are working with commercial, personal or otherwise sensitive data, or when you have a complex IT setup.\n\n\n\nVU IT offers several services for employees to store their files. Examples are:\n\nOneDrive: personal storage for all VU employees and part of the Microsoft 365 platform. OneDrive allows you to store files locally and in the Microsoft cloud, and share folders and documents with colleagues. Since this is personal storage, tied to someone’s personal VU account, we don’t usually recommend storing research data in OneDrive: if the account holder leaves the VU, the account and all the data on it, disappear.\nTeams. Faculties, divisions and departments have their own Team - part of the Microsoft 365 platform - where they store shared documents and where they can interact and chat. Projects may also request a project team. But note that Teams is not always the best location to store your research data and has several limitations, especially when it comes to working with non-Microsoft file formats, large volumes of data, interacting with data, and collaborating with partners outside of the VU. Contact the RDM Support Desk to find out more about the suitability of Teams for your project.\nSurfdrive: is a personal cloud storage service for the Dutch education and research community, offering staff, researchers and students an easy way to store, synchronise and share files in the secure and reliable SURF community cloud. All users receive storage space of up to 500 GB. Surfdrive is automatically offered to all VU employees. Since Surfdrive is personal storage, like OneDrive, we do not usually recommend it for research data\n\n\n\n\nThe options above are standard data storage options at the VU to which all employees have access. But the VU also offers storage specifically for research data. Some of them are hosted locally at the VU, while others are SURF cloud services. When selecting a cloud-based service it is important to remember to check where the data will be hosted. If the research project involves sensitive data it may be necessary to choose cloud-based options that guarantee that the data will stay in the EEA or on servers based in the EEA.\n\nSciStor (short for ‘Storage for Scientists’): This is storage hosted by IT for Research (ITvO) and allows for inexpensive storage of large volumes of data. There are various levels of security possible and various ways to get access to the files. SciStor is only intended for ongoing research, not for archiving.\nYoda (short for Your Data) is a cloud storage at SURF and is suitable for storing large-scale and sensitive datasets. Yoda also supports collaborating on projects in and outside the VU and adding contextual information (metadata) about your dataset as you go. Yoda is usually the best choice if your research data are very sensitive.\nResearch Drive is a cloud storage at Surf for research projects and is suitable for collaboration in and outside the VU, for storing sensitive data and large-scale research projects. You can also encrypt data in Research Drive using several tools. You are able to request storage space in Research Drive via a web form in the selfservice portal (VU employees only). Research Drive is the best choice if you need to manage access rights on a folder level. More general information about Research Drive can be found here, and its wiki pages, including tutorials, are here.\n\nThere are differences between Research Drive and Yoda and each one may support certain projects better than others. The storage finder can help you to get an idea of what would be the best choice for your project, but get in touch with the RDM Support Desk for more details.\n\n\n\nSome projects may require data sharing with partners. Although Research Drive and Yoda support sharing data all through the project, it may also be the case that some data only need to be sent to a partner once. There are some secure options to send data to research partners:\n\nSurf Filesender: cloud service that allows you to send files of 1 Terabyte to other researchers and encrypted files of up to 250 GB.\n   \nZivver: All employees of Vrije Universiteit Amsterdam can use Zivver, the encryption programme that allows you to send email or data (sensitive or otherwise) securely by email. Attachments will also be encrypted and can be several Terabytes in size (max = 5 TB). Specific information on how to get and use Zivver are available on the selfservice portal. General explanations on how to use it are available at the Zivver website."
  },
  {
    "objectID": "topics/data-storage.html#data-storage",
    "href": "topics/data-storage.html#data-storage",
    "title": "{{< var title >}}",
    "section": "",
    "text": "VU Amsterdam offers several options to store your research data. The choice for a specific option may depend on factors such as:\n\nDoes a project involve multiple organisations or departments?\nThe sensitivity of the data: does it involve personal data or copyrighted / commercial data?\nAre there any research partners with whom data need to be shared?\nAre any commercial parties involved?\nDoes the research project involve multiple locations (inside or maybe even outside the EU)?\nWill there be (lab) devices producing data that need to be stored as well?\nWhat will be the volume of the data?\nWill there be lots of interactions with the data (using software/tools)?\n\nStorage options may take several forms, for example:\n\nLocal storage on computers, networks or servers;\nCloud storage offered by the VU;\nLocations where physical data samples are stored (fridges, lockers, etc.).\n\nResearchers, including PhD candidates, have multiple options that can be used, some of which are listed below. More information about these storage options is available behind their respective links. The Storage finder is a tool that will give you a number of storage options suitable for your research. For more individual guidance, please get in touch with the Research Data Management Support Desk for advice, particularly when you are working with commercial, personal or otherwise sensitive data, or when you have a complex IT setup.\n\n\n\nVU IT offers several services for employees to store their files. Examples are:\n\nOneDrive: personal storage for all VU employees and part of the Microsoft 365 platform. OneDrive allows you to store files locally and in the Microsoft cloud, and share folders and documents with colleagues. Since this is personal storage, tied to someone’s personal VU account, we don’t usually recommend storing research data in OneDrive: if the account holder leaves the VU, the account and all the data on it, disappear.\nTeams. Faculties, divisions and departments have their own Team - part of the Microsoft 365 platform - where they store shared documents and where they can interact and chat. Projects may also request a project team. But note that Teams is not always the best location to store your research data and has several limitations, especially when it comes to working with non-Microsoft file formats, large volumes of data, interacting with data, and collaborating with partners outside of the VU. Contact the RDM Support Desk to find out more about the suitability of Teams for your project.\nSurfdrive: is a personal cloud storage service for the Dutch education and research community, offering staff, researchers and students an easy way to store, synchronise and share files in the secure and reliable SURF community cloud. All users receive storage space of up to 500 GB. Surfdrive is automatically offered to all VU employees. Since Surfdrive is personal storage, like OneDrive, we do not usually recommend it for research data\n\n\n\n\nThe options above are standard data storage options at the VU to which all employees have access. But the VU also offers storage specifically for research data. Some of them are hosted locally at the VU, while others are SURF cloud services. When selecting a cloud-based service it is important to remember to check where the data will be hosted. If the research project involves sensitive data it may be necessary to choose cloud-based options that guarantee that the data will stay in the EEA or on servers based in the EEA.\n\nSciStor (short for ‘Storage for Scientists’): This is storage hosted by IT for Research (ITvO) and allows for inexpensive storage of large volumes of data. There are various levels of security possible and various ways to get access to the files. SciStor is only intended for ongoing research, not for archiving.\nYoda (short for Your Data) is a cloud storage at SURF and is suitable for storing large-scale and sensitive datasets. Yoda also supports collaborating on projects in and outside the VU and adding contextual information (metadata) about your dataset as you go. Yoda is usually the best choice if your research data are very sensitive.\nResearch Drive is a cloud storage at Surf for research projects and is suitable for collaboration in and outside the VU, for storing sensitive data and large-scale research projects. You can also encrypt data in Research Drive using several tools. You are able to request storage space in Research Drive via a web form in the selfservice portal (VU employees only). Research Drive is the best choice if you need to manage access rights on a folder level. More general information about Research Drive can be found here, and its wiki pages, including tutorials, are here.\n\nThere are differences between Research Drive and Yoda and each one may support certain projects better than others. The storage finder can help you to get an idea of what would be the best choice for your project, but get in touch with the RDM Support Desk for more details.\n\n\n\nSome projects may require data sharing with partners. Although Research Drive and Yoda support sharing data all through the project, it may also be the case that some data only need to be sent to a partner once. There are some secure options to send data to research partners:\n\nSurf Filesender: cloud service that allows you to send files of 1 Terabyte to other researchers and encrypted files of up to 250 GB.\n   \nZivver: All employees of Vrije Universiteit Amsterdam can use Zivver, the encryption programme that allows you to send email or data (sensitive or otherwise) securely by email. Attachments will also be encrypted and can be several Terabytes in size (max = 5 TB). Specific information on how to get and use Zivver are available on the selfservice portal. General explanations on how to use it are available at the Zivver website."
  },
  {
    "objectID": "topics/finding-existing-data.html",
    "href": "topics/finding-existing-data.html",
    "title": "{{< var title >}}",
    "section": "",
    "text": "Anything that can be used for analysis can be considered “data(sets)”. Many national and international organisations provide access to large datasets free of charge: this is called Open Data.\nDatasets may contain different kinds of data files, e.g. raw or edited/cleaned data, and macro or micro data. Raw data refers to the data as they are primarily collected, and includes all data, even the missed or mismatched pieces in the data file. Edited or cleaned data refers to data that have been tidied up for analysis and publication. Macro data and statistics are results based on micro data units and provide a general overview of the micro data. Although datasets can contain data of varying type or aggregation level, and there may be overlap between these definitions, each element can contain very important information.\nWhen re-using research data, scientists must be familiar with the rules and regulations governing data copyright, intellectual property rights, and laws governing sensitive or personal information. SURF has compiled a report on the legal status of raw data including information on the types of consent required for the re-use of data. Your Privacy Champion can answer questions about the use of personal data. IXA can provide legal help with the re-use of data.\nSee also the ZonMw explanation of different kinds of property rights in the Netherlands (text available in Dutch only).\n\n\n\nThe number of datasets that are available grows rapidly. Datasets are made available in many formats, by many people or organizations. Some datasets are raw files and some are specifically organised and formatted as databases that require a licence or subscription to use them. The library of the Vrije Universiteit Amsterdam has collected links to some of the data repositories used and has licensed several databases.\n\nPopular Free and Licensed Databases: These can be found in the library e-resources list or with LibSearch Advanced.\n\nIf you need help finding & using free or licensed sources you can contact the Research Data Services Helpdesk. For students and personnel in the fields of economics, finance, or organisation science a separate LibGuide has been created to help them find and use/re-use data.\nYou can also start looking for data in these four places:\n\nThe literature. Research articles may point you to the data that they are based on. Sometimes, (part of) the data are added to the article as supplementary files, and sometimes the data are published separately in a data repository. In the latter case, the article usually provides a clear reference to the published dataset. Some datasets may even be specifically published in Data Journals.\nScientific data repositories. Data repositories are platforms used to access and archive research data. Universities often provide a repository for data archiving, but other platforms arranged by discipline or by country also exist. Some repositories are only accessible to consortium members, whereas others are free of charge. Many universities in the Netherlands use DataverseNL to archive datasets for the mid-term. Long-term archiving is provided by the national research data archives DANS and 4TU.Research Data. In Europe, B2SHARE and Zenodo are platforms used to access research data. Data repositories can be accessed by searching by topic or country using Re3data, a data repository registry. The VU has its own research portal, PURE, where researchers register their datasets. You can find instructions on how to register your own dataset in PURE on the Dataset Registration page of this LibGuide.\nData search engines. Search engines allow you to quickly browse data sets and supplementary data files published by researchers. They cover data sets from many sources. This makes them useful for quick orientation on a topic. Example of a search engines are: DataCite, Google DataSet Search, Elsevier Dataset Search.\nData portals of (governmental) organisations. Organisations that regularly collect (statistical) data sometimes offer these data through their own portal. An example is Eurostat, which collects and disseminates statistics at the European level, by country and by theme. Some of these websites have been linked in the Finding data LibGuide.\n\n\n\n\nResearchers from the Vrije Universiteit Amsterdam have also developed some databases containing data collected during research. See here for some examples:\n\nNederlands Tweelingenregister (Netherlands Twin Register) The database contains data on twins and their families and was created to do research on the relationship between genetics and growth, development, personality, behaviour, diseases, mental health and all kinds of risks.\nGeoplaza VU - the portal for all matters related to GIS (Geographical Information Systems) and geodata at the VU University Amsterdam. It offers students and employers a platform to exchange, examine and download digital map material.\nDutch monasteries - database with information about Dutch monasteries of the Middle Ages.\nSlave owners in Amsterdam 1863 - the place of living of owners of slaves in Amsterdam in 1863, visualized in GeoPlaza.\nDeaths at the Borders Database - collection of official, state-produced evidence on people who died while attempting to reach southern EU countries from the Balkans, the Middle East, and North & West Africa, and whose bodies were found in or brought to Europe.\nDatasets published by VU Researchers can be found at the VU Research Portal."
  },
  {
    "objectID": "topics/finding-existing-data.html#finding-existing-data",
    "href": "topics/finding-existing-data.html#finding-existing-data",
    "title": "{{< var title >}}",
    "section": "",
    "text": "Anything that can be used for analysis can be considered “data(sets)”. Many national and international organisations provide access to large datasets free of charge: this is called Open Data.\nDatasets may contain different kinds of data files, e.g. raw or edited/cleaned data, and macro or micro data. Raw data refers to the data as they are primarily collected, and includes all data, even the missed or mismatched pieces in the data file. Edited or cleaned data refers to data that have been tidied up for analysis and publication. Macro data and statistics are results based on micro data units and provide a general overview of the micro data. Although datasets can contain data of varying type or aggregation level, and there may be overlap between these definitions, each element can contain very important information.\nWhen re-using research data, scientists must be familiar with the rules and regulations governing data copyright, intellectual property rights, and laws governing sensitive or personal information. SURF has compiled a report on the legal status of raw data including information on the types of consent required for the re-use of data. Your Privacy Champion can answer questions about the use of personal data. IXA can provide legal help with the re-use of data.\nSee also the ZonMw explanation of different kinds of property rights in the Netherlands (text available in Dutch only).\n\n\n\nThe number of datasets that are available grows rapidly. Datasets are made available in many formats, by many people or organizations. Some datasets are raw files and some are specifically organised and formatted as databases that require a licence or subscription to use them. The library of the Vrije Universiteit Amsterdam has collected links to some of the data repositories used and has licensed several databases.\n\nPopular Free and Licensed Databases: These can be found in the library e-resources list or with LibSearch Advanced.\n\nIf you need help finding & using free or licensed sources you can contact the Research Data Services Helpdesk. For students and personnel in the fields of economics, finance, or organisation science a separate LibGuide has been created to help them find and use/re-use data.\nYou can also start looking for data in these four places:\n\nThe literature. Research articles may point you to the data that they are based on. Sometimes, (part of) the data are added to the article as supplementary files, and sometimes the data are published separately in a data repository. In the latter case, the article usually provides a clear reference to the published dataset. Some datasets may even be specifically published in Data Journals.\nScientific data repositories. Data repositories are platforms used to access and archive research data. Universities often provide a repository for data archiving, but other platforms arranged by discipline or by country also exist. Some repositories are only accessible to consortium members, whereas others are free of charge. Many universities in the Netherlands use DataverseNL to archive datasets for the mid-term. Long-term archiving is provided by the national research data archives DANS and 4TU.Research Data. In Europe, B2SHARE and Zenodo are platforms used to access research data. Data repositories can be accessed by searching by topic or country using Re3data, a data repository registry. The VU has its own research portal, PURE, where researchers register their datasets. You can find instructions on how to register your own dataset in PURE on the Dataset Registration page of this LibGuide.\nData search engines. Search engines allow you to quickly browse data sets and supplementary data files published by researchers. They cover data sets from many sources. This makes them useful for quick orientation on a topic. Example of a search engines are: DataCite, Google DataSet Search, Elsevier Dataset Search.\nData portals of (governmental) organisations. Organisations that regularly collect (statistical) data sometimes offer these data through their own portal. An example is Eurostat, which collects and disseminates statistics at the European level, by country and by theme. Some of these websites have been linked in the Finding data LibGuide.\n\n\n\n\nResearchers from the Vrije Universiteit Amsterdam have also developed some databases containing data collected during research. See here for some examples:\n\nNederlands Tweelingenregister (Netherlands Twin Register) The database contains data on twins and their families and was created to do research on the relationship between genetics and growth, development, personality, behaviour, diseases, mental health and all kinds of risks.\nGeoplaza VU - the portal for all matters related to GIS (Geographical Information Systems) and geodata at the VU University Amsterdam. It offers students and employers a platform to exchange, examine and download digital map material.\nDutch monasteries - database with information about Dutch monasteries of the Middle Ages.\nSlave owners in Amsterdam 1863 - the place of living of owners of slaves in Amsterdam in 1863, visualized in GeoPlaza.\nDeaths at the Borders Database - collection of official, state-produced evidence on people who died while attempting to reach southern EU countries from the Balkans, the Middle East, and North & West Africa, and whose bodies were found in or brought to Europe.\nDatasets published by VU Researchers can be found at the VU Research Portal."
  },
  {
    "objectID": "topics/research-data-management.html",
    "href": "topics/research-data-management.html",
    "title": "Research Data Management (RDM)",
    "section": "",
    "text": "Research Data Management (RDM)\nRDM concerns the organisation, documentation, storage, archiving and sharing of digital and analogue data. Data management applies throughout the entire research data life cycle, which is visualised in the circle above. RDM aims to ensure reliable verification of results, and permits new and innovative research built on existing information. RDM is also part of the research process and is intended to make the research process as efficient as possible. This LibGuide provides guidance on research data management planning, data storage and protection, data archiving, and other resources. The Data Management Plan provides information on how these activities will be carried out during the research project.\nGood data management will heighten the quality of your own research (data) as well as your institution’s scientific output, and it contributes significantly to your field as a whole.\nGood data management:\n\nPromotes the integrity of your research,\nIncreases the impact of your research,\nImproves the quality of your data,\nSupports future use of your research data, and\nComplies with internal and external regulations."
  }
]