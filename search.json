[
  {
    "objectID": "topics.html",
    "href": "topics.html",
    "title": "Topics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nBAZIS HPC\n\n\n \n\n\n\n\nCompute environments\n\n\n \n\n\n\n\nData Management Plan\n\n\n \n\n\n\n\nData Publication\n\n\n \n\n\n\n\nFAIR Principles\n\n\n \n\n\n\n\nResearch Data Management\n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "topics/research-data-management.html",
    "href": "topics/research-data-management.html",
    "title": "Research Data Management",
    "section": "",
    "text": "Research Data Management\nResearch data management (RDM) concerns the organisation, documentation, storage, archiving and sharing of digital and analogue data. Data management applies throughout the entire research data life cycle, which is visualised in the circle above. RDM aims to ensure reliable verification of results, and permits new and innovative research built on existing information. RDM is also part of the research process and is intended to make the research process as efficient as possible. This LibGuide provides guidance on research data management planning, data storage and protection, data archiving, and other resources. The Data Management Plan provides information on how these activities will be carried out during the research project.\nGood data management will heighten the quality of your own research (data) as well as your institution’s scientific output, and it contributes significantly to your field as a whole.\nGood data management:\n\nPromotes the integrity of your research,\nIncreases the impact of your research,\nImproves the quality of your data,\nSupports future use of your research data, and\nComplies with internal and external regulations."
  },
  {
    "objectID": "topics/data-publication.html",
    "href": "topics/data-publication.html",
    "title": "Data Publication",
    "section": "",
    "text": "Open Access publishing means that you make your publication freely accessible online to everyone without restrictions. The Vrije Universiteit believes that government-funded research should be available free of charge to as many people as possible. To that end, the VU Library offers a guide on how to go about Open Access Publishing.\nOpen Access publishing is one component of Open Science. UNESCO defines Open Science as\n\na set of principles and practices that aim to make scientific research from all fields accessible to everyone for the benefits of scientists and society as a whole. Open science is about making sure not only that scientific knowledge is accessible but also that the production of that knowledge itself is inclusive, equitable and sustainable\n\nThis includes making openly available research data, methods, and documentation where possible. As such, RDM and the practices outlined in this LibGuide are a precondition of Open Science.\nYou can read more about Open Science in the Netherlands on the website of the Nationaal Programma Open Science and join the Open Science Community Amsterdam, the community of VU employees interested in Open Science (joint with the University of Amsterdam and Amsterdam University of Applied Sciences).\n\n\n\nInstead of archiving research data in a data repository, you may choose to publish an article about your data collection. This is not necessarily common for all disciplines. Some examples of data journals where you can publish your data and dataset, are:\n\nScientific Data - Nature\nGeoscience Data Journal\nGigascience\nDataset Papers in Science\nJournal of Physical and Chemical Reference Data\nEarth System Science Data\nJournal of Open Archaeology Data\nJournal of Open Health Data\nJournal of Open Psychology Data\n\n\n\n\nA persistent identifier (PID) is a durable reference to a digital dataset document, website or other object. It is a kind of ISBN for digital files. By using a persistent identifier, you make sure that your dataset will be findable well into the future. A DOI or Handle are the commonly used PIDs. The data archiving options at the VU commonly offer DOIs.\nMost data archives or repositories offer a persistent identifier and generate this automatically when research data are archived. For example, this is the case for DataverseNL at the VU. In Yoda at the VU, assigning a PID is possible, but does not happen automatically. Please get in touch with the RDM Support Desk if you have questions about assigning a PID when you archive data in Yoda.\n\n\n\nA data licence agreement is a legal instrument that lets others know what they can and cannot do with your research data (and any documentation. scripts and metadata that are published with the data). It is important to consider what kind of limitations are relevant. An important component can be a guideline on how people should cite the dataset. Other components could be:\n\nCan people make copies or even distribute copies\nWho should be contacted if you need access to re-use data\nEtc.\n\nIn principle, Dataverse allows you to choose your terms of use. Some data repositories require you to use a certain licence if you want to deposit your data with them. At Dryad, for example, all datasets are published under the terms of Creative Commons Zero to minimise legal barriers and to maximise the impact for research and education. Some funders may also require that you publish the data as open data. Open data are data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and share alike (Open Knowledge International definition). If you need help with drawing up license agreements, you can contact the IXA office.\nAdditional websites and tools:\n\nExplanation about copyrights and licences by a professor from Leiden University (English subtitles available)\nThe Guide to Creative Commons for Scholarly Publishing and Educational Resources by NWO, VSNU and the University and Royal Libraries\nDCC how-to guide on licensing research data, a guide that links to the Creative Commons website, where many terms are explained\nOpen Data Commons Public Domain Dedication and License (PDDL)\nEUDAT B2SHARE license selection wizard, which Pawel Kamocki (et al) released under an open source license.\n\n\n\n\nAs with data, it is important to license any software that you write for your research. Some basic information about software licensing can be found here, but it is also important to keep the following in mind:\n\nFor software, choosing a licence usually happens right when you start developing the software or when you put it in a public repository, rather than when the software is finished and fully baked. This makes sense: immediately when the software appears on a public repository, the software can be found and seen by others. Giving it a licence helps others to figure out what they can and can’t do with it.\nIf you are reusing software or libraries written by someone else, you must stick to the clauses of the licence given to the original software/library;\nIf you used proprietary software to write your code (e.g. SPSS syntax), your options in choosing a licence will probably be limited;\nWhen choosing a licence, do not just think about what others may do with the software, but also what you might want to do with the software in the future.\n\nThere are several tools that can help you to choose a suitable sotware licence. If you have questions or need guidance in choosing a licence for your software, get in touch with the RDM Support Desk."
  },
  {
    "objectID": "topics/data-publication.html#open-access-and-open-science",
    "href": "topics/data-publication.html#open-access-and-open-science",
    "title": "Data Publication",
    "section": "",
    "text": "Open Access publishing means that you make your publication freely accessible online to everyone without restrictions. The Vrije Universiteit believes that government-funded research should be available free of charge to as many people as possible. To that end, the VU Library offers a guide on how to go about Open Access Publishing.\nOpen Access publishing is one component of Open Science. UNESCO defines Open Science as\n\na set of principles and practices that aim to make scientific research from all fields accessible to everyone for the benefits of scientists and society as a whole. Open science is about making sure not only that scientific knowledge is accessible but also that the production of that knowledge itself is inclusive, equitable and sustainable\n\nThis includes making openly available research data, methods, and documentation where possible. As such, RDM and the practices outlined in this LibGuide are a precondition of Open Science.\nYou can read more about Open Science in the Netherlands on the website of the Nationaal Programma Open Science and join the Open Science Community Amsterdam, the community of VU employees interested in Open Science (joint with the University of Amsterdam and Amsterdam University of Applied Sciences)."
  },
  {
    "objectID": "topics/data-publication.html#publishing-your-data-in-a-data-journal",
    "href": "topics/data-publication.html#publishing-your-data-in-a-data-journal",
    "title": "Data Publication",
    "section": "",
    "text": "Instead of archiving research data in a data repository, you may choose to publish an article about your data collection. This is not necessarily common for all disciplines. Some examples of data journals where you can publish your data and dataset, are:\n\nScientific Data - Nature\nGeoscience Data Journal\nGigascience\nDataset Papers in Science\nJournal of Physical and Chemical Reference Data\nEarth System Science Data\nJournal of Open Archaeology Data\nJournal of Open Health Data\nJournal of Open Psychology Data"
  },
  {
    "objectID": "topics/data-publication.html#what-is-a-persistent-identifier",
    "href": "topics/data-publication.html#what-is-a-persistent-identifier",
    "title": "Data Publication",
    "section": "",
    "text": "A persistent identifier (PID) is a durable reference to a digital dataset document, website or other object. It is a kind of ISBN for digital files. By using a persistent identifier, you make sure that your dataset will be findable well into the future. A DOI or Handle are the commonly used PIDs. The data archiving options at the VU commonly offer DOIs.\nMost data archives or repositories offer a persistent identifier and generate this automatically when research data are archived. For example, this is the case for DataverseNL at the VU. In Yoda at the VU, assigning a PID is possible, but does not happen automatically. Please get in touch with the RDM Support Desk if you have questions about assigning a PID when you archive data in Yoda."
  },
  {
    "objectID": "topics/data-publication.html#licensing-the-data",
    "href": "topics/data-publication.html#licensing-the-data",
    "title": "Data Publication",
    "section": "",
    "text": "A data licence agreement is a legal instrument that lets others know what they can and cannot do with your research data (and any documentation. scripts and metadata that are published with the data). It is important to consider what kind of limitations are relevant. An important component can be a guideline on how people should cite the dataset. Other components could be:\n\nCan people make copies or even distribute copies\nWho should be contacted if you need access to re-use data\nEtc.\n\nIn principle, Dataverse allows you to choose your terms of use. Some data repositories require you to use a certain licence if you want to deposit your data with them. At Dryad, for example, all datasets are published under the terms of Creative Commons Zero to minimise legal barriers and to maximise the impact for research and education. Some funders may also require that you publish the data as open data. Open data are data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and share alike (Open Knowledge International definition). If you need help with drawing up license agreements, you can contact the IXA office.\nAdditional websites and tools:\n\nExplanation about copyrights and licences by a professor from Leiden University (English subtitles available)\nThe Guide to Creative Commons for Scholarly Publishing and Educational Resources by NWO, VSNU and the University and Royal Libraries\nDCC how-to guide on licensing research data, a guide that links to the Creative Commons website, where many terms are explained\nOpen Data Commons Public Domain Dedication and License (PDDL)\nEUDAT B2SHARE license selection wizard, which Pawel Kamocki (et al) released under an open source license."
  },
  {
    "objectID": "topics/data-publication.html#licensing-software",
    "href": "topics/data-publication.html#licensing-software",
    "title": "Data Publication",
    "section": "",
    "text": "As with data, it is important to license any software that you write for your research. Some basic information about software licensing can be found here, but it is also important to keep the following in mind:\n\nFor software, choosing a licence usually happens right when you start developing the software or when you put it in a public repository, rather than when the software is finished and fully baked. This makes sense: immediately when the software appears on a public repository, the software can be found and seen by others. Giving it a licence helps others to figure out what they can and can’t do with it.\nIf you are reusing software or libraries written by someone else, you must stick to the clauses of the licence given to the original software/library;\nIf you used proprietary software to write your code (e.g. SPSS syntax), your options in choosing a licence will probably be limited;\nWhen choosing a licence, do not just think about what others may do with the software, but also what you might want to do with the software in the future.\n\nThere are several tools that can help you to choose a suitable sotware licence. If you have questions or need guidance in choosing a licence for your software, get in touch with the RDM Support Desk."
  },
  {
    "objectID": "topics/compute.html",
    "href": "topics/compute.html",
    "title": "Compute environments",
    "section": "",
    "text": "For different applications, - Data gathering - Data analysis - Hosting web applications - Running (licensed) server applications - Visualization\n\n\nPros: - Always available, install software and go - Can use graphical tools Cons: - Not all software is available on all OS’s - Limited performance - Laptops: can not run 24/7 - Can not host web applications for 3rd party\n\n\n\n\n\nPros: - Fixed price per month - Windows or Linux - Can host web applications for 3rd parties - Mount SciStor shares - Possible to use GUI tools Cons: - No GPU - Some server maintenance necessary (application & OS updates) - Not easy to start multiple identical servers - Knowledge needed on how to setup your environment*\n\nIT for Research will only do the basic OS updates. We can help you get started but all other setup and maintenance is your own responsibility.\n\n\n\n\nPros: - Windows or Linux - Easier to spin up multiple identical servers - GPU available - Possible to use GUI tools - Can host web applications for 3rd parties - Make use of existing setups created by others Cons: - Less suitable for hosting web applicaions because of cost model - Some extra work needed to make server rebuildable\n\n\n\nPros: - Free for researchers - Possible to use GUI tools Cons: - Windows only - Can not leave running 24/7 - Extra software only available on request\n\n\n\n\nIt is possible to spin up Docker containers on SciCloud and SURF Research Cloud. Pros: - Easier to spin up multiple indentical applications - Platform independent, Easy to develop on your laptop and then deploy somewhere else - More reproducible, your Docker- and compose files document your setup - Things don’t break as easily on OS updates Cons: - Some knowledge about Docker needed - Some CLI knowledge needed\n\n\nPros: - CI/CD pipelines, e.g. push code on Github/Gitlab and automatically rebuild the container - No need to worry about updating a (virtual) server Cons: - It takes extra work to make your containers suitable for OpenShift.\n\n\n\n\nIn unusual cases you need a server to run specific applications, but the performance of a Virtual Server on SciCloud is not sufficient or some other requitement demands physical hardware. In that case you could opt to buy your own hardware and host it in IT for research’s Data Centre. IT will take care of the hardware and network connectivity, when absolutely necessary you can have physical access to the machine.\nPlease contact ITvO to discuss your requirements.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnly use data which can be considered public.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn order to minimize the chances of data leaks IT Privacy & Security could require you to work in a secure environment where it is impossible to copy data. You will work on a remote desktop with the analysis tools you need. The data will only be accessible within the virtual environment. Only and IT admin or Data Steward will be able to install software and import or export data. ### SANE ### Custom solutions on SciCloud"
  },
  {
    "objectID": "topics/compute.html#workstations-and-laptops",
    "href": "topics/compute.html#workstations-and-laptops",
    "title": "Compute environments",
    "section": "",
    "text": "Pros: - Always available, install software and go - Can use graphical tools Cons: - Not all software is available on all OS’s - Limited performance - Laptops: can not run 24/7 - Can not host web applications for 3rd party"
  },
  {
    "objectID": "topics/compute.html#virtual-servers",
    "href": "topics/compute.html#virtual-servers",
    "title": "Compute environments",
    "section": "",
    "text": "Pros: - Fixed price per month - Windows or Linux - Can host web applications for 3rd parties - Mount SciStor shares - Possible to use GUI tools Cons: - No GPU - Some server maintenance necessary (application & OS updates) - Not easy to start multiple identical servers - Knowledge needed on how to setup your environment*\n\nIT for Research will only do the basic OS updates. We can help you get started but all other setup and maintenance is your own responsibility.\n\n\n\n\nPros: - Windows or Linux - Easier to spin up multiple identical servers - GPU available - Possible to use GUI tools - Can host web applications for 3rd parties - Make use of existing setups created by others Cons: - Less suitable for hosting web applicaions because of cost model - Some extra work needed to make server rebuildable\n\n\n\nPros: - Free for researchers - Possible to use GUI tools Cons: - Windows only - Can not leave running 24/7 - Extra software only available on request"
  },
  {
    "objectID": "topics/compute.html#containers",
    "href": "topics/compute.html#containers",
    "title": "Compute environments",
    "section": "",
    "text": "It is possible to spin up Docker containers on SciCloud and SURF Research Cloud. Pros: - Easier to spin up multiple indentical applications - Platform independent, Easy to develop on your laptop and then deploy somewhere else - More reproducible, your Docker- and compose files document your setup - Things don’t break as easily on OS updates Cons: - Some knowledge about Docker needed - Some CLI knowledge needed\n\n\nPros: - CI/CD pipelines, e.g. push code on Github/Gitlab and automatically rebuild the container - No need to worry about updating a (virtual) server Cons: - It takes extra work to make your containers suitable for OpenShift."
  },
  {
    "objectID": "topics/compute.html#physical-servers",
    "href": "topics/compute.html#physical-servers",
    "title": "Compute environments",
    "section": "",
    "text": "In unusual cases you need a server to run specific applications, but the performance of a Virtual Server on SciCloud is not sufficient or some other requitement demands physical hardware. In that case you could opt to buy your own hardware and host it in IT for research’s Data Centre. IT will take care of the hardware and network connectivity, when absolutely necessary you can have physical access to the machine.\nPlease contact ITvO to discuss your requirements."
  },
  {
    "objectID": "topics/compute.html#jupyter-notebooks",
    "href": "topics/compute.html#jupyter-notebooks",
    "title": "Compute environments",
    "section": "",
    "text": "Only use data which can be considered public."
  },
  {
    "objectID": "topics/compute.html#highly-secure-environments",
    "href": "topics/compute.html#highly-secure-environments",
    "title": "Compute environments",
    "section": "",
    "text": "In order to minimize the chances of data leaks IT Privacy & Security could require you to work in a secure environment where it is impossible to copy data. You will work on a remote desktop with the analysis tools you need. The data will only be accessible within the virtual environment. Only and IT admin or Data Steward will be able to install software and import or export data. ### SANE ### Custom solutions on SciCloud"
  },
  {
    "objectID": "pathways.html",
    "href": "pathways.html",
    "title": "Pathways",
    "section": "",
    "text": "Data lifecycle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Hello world!\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The Open Handbook is a project started by Research Data Support in early 2024. After planning and design phases, we launched the initial version of the resource at the Research Support Days in May 2024.\nThe Open Handbook centralizes resources that VU researchers need to do their work. The Open Handbook also provides everyone with direct pathways to change resources in case anything has become outdated.\nPreviously such resources were spread out across many different pages at VU and were hard to update. The Open Handbook is curated by us all, and reviewed by specialists. This way we can help each other.\n\n\nThe Open Handbook was initiated by Lena Karvovskaya, Jessica Hrudey, Elisa, and Jolien Scholten. The initial infrastructure for the Open Handbook was built by Liberate Science.\nWe want to specifically call out the following folk who contributed outside of GitHub:\n\nJochem Lybaart\n\n\n\n\n\n\nAll contributions to this project are gratefully acknowledged using the allcontributors package following the all-contributors specification. Contributions of any kind are welcome!\n\n\n\n   chartgerink\n\n\n   Jolien-S\n\n\n   Elisa-on-GitHub\n\n\n   Karvovskaya\n\n\n   jhrudey"
  },
  {
    "objectID": "about.html#contributors",
    "href": "about.html#contributors",
    "title": "About",
    "section": "",
    "text": "The Open Handbook was initiated by Lena Karvovskaya, Jessica Hrudey, Elisa, and Jolien Scholten. The initial infrastructure for the Open Handbook was built by Liberate Science.\nWe want to specifically call out the following folk who contributed outside of GitHub:\n\nJochem Lybaart\n\n\n\n\n\n\nAll contributions to this project are gratefully acknowledged using the allcontributors package following the all-contributors specification. Contributions of any kind are welcome!\n\n\n\n   chartgerink\n\n\n   Jolien-S\n\n\n   Elisa-on-GitHub\n\n\n   Karvovskaya\n\n\n   jhrudey"
  },
  {
    "objectID": "blog/2024-05-23welcome.html",
    "href": "blog/2024-05-23welcome.html",
    "title": "Hello world!",
    "section": "",
    "text": "This is the first blog entry on the Research Support Handbook. We will be posting more at a later time, and are looking forward to your contributions as well.\nWe will follow up with more details later."
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "You can contribute to this book by making small edits or writing entirely new topics. From small to large, all contributions are welcome. If you are in need of specific information, you can skip ahead using the table of contents.\n\n\nWe offer a portal to reduce the barriers to contribute to the Research Support Handbook. You only need an internet connection and articulate what you want us to include. No accounts necessary 😊\n\n\n\n\n\n\nNote\n\n\n\nOpen the contribution portal by clicking here or copy-pasting: https://ez-github-contributor.netlify.app/\n\n\nYou can report issues you find with the Research Support Handbook using the “Report a problem” tab. This is a way for you to share your feedback with us.\nYou can propose new topics to the Research Support Handbook using the “Propose new page” tab. This will be considered for inclusion as a topic.\n\n\n\nScreenshot of the contributor portal\n\n\nIf you want to be credited with contributing, please share your name. If you’d like to hear back about what was done with your feedback or proposal, please also provide a direct way to contact you.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the next steps you need a GitHub account to contribute. You can create one directly on GitHub.\n\n\n\n\nThe easiest and quickest way to contribute to the book is make suggested edits. On each page you will find a button reading “Edit this page” (usually on the right).\n\n\n\nScreenshot of a handbook topic, with a red box on the right hand side of the page indicating where to find the “Edit this page” button\n\n\nWhen you click that, you will immediately be taken to GitHub to edit the text of that specific page. You may be prompted to create a fork (forking) in case these are your first edits.\n\n\n\nScreenshot of the GitHub file editor, with some changes made and the “Commit changes” button active\n\n\nOnce you made your edits, you are ready to commit (save) your changes and submit your pull request, requesting those changes to be included in the handbook.\n\n\n\nTo add a new topic, you need to create a new file ending in .qmd (e.g., topics/example.qmd). You can do this by visiting the handbook page on GitHub and clicking Add file -&gt; New file.\n\n\n\nScreenshot of GitHub highlighting where to find the “New file” button\n\n\nWhen you click this button you may be asked to fork the repository. This is not a problem so go ahead!\nThe topic itself needs to be written in Markdown. Every topic must contain a first level heading (e.g., # Heading), which will be the topic title. Section headings are second level headings (e.g., ## Section).\nAfter that, you are ready to submit your pull request! The reviewers will help you place the topic in the right place of the book.1\n\n\n\nOnce you have made suggested changes, a pull request is the way for you to ask for your changes to be incorporated into the handbook. The people maintaining the handbook will review what you wrote, ask some questions, and accept or decline your contributions.\nWe recommend keeping your suggested changes small or limited in scope, and explaining why you are suggesting these changes. It is more likely your changes are included when you are fixing a typo or adding a paragraph, and less likely if you are revising the entire handbook. It is also more likely they are included if you explain why you are suggesting the changes, rather than dropping by and making edits without any context.\nIf you are adding a new topic, it is definitely recommended to open an issue first to see whether there is a need for it (and maybe you’ll find collaborators!).\nDuring the review process you may be asked to update your changes, or revisions may be added by the people maintaining the handbook. It is helpful if you keep an eye on your GitHub account to ensure timely responses to help the process along.\n\n\n\nThe book is created using Markdown - you can get familiarized with the basic syntax on the Markdown website. The getting started quick items are:\n# Heading level 1\n## Heading level 2\n### Heading level 3\n\nYou simply write text as you are used to. To make something *italic*, **bold**, or ***bold and italic***.\n\n&gt; this is how you add quotes\n\n- or lists\n- that can go on \n- and on\nIf you want to add code, use references, create links, or footnotes - it is all possible. We will expand examples here based on your needs, so if you need help, let us know by reporting an issue!\n\n\n\nWe use GitHub to create this website automatically, and to manage all the incoming updates. You do not need to know how it works entirely, but we want to help you understand some things so you are not confused.\n\n\nA repository on GitHub is like a folder on your computer. This can be many things, depending on what files it contains.\nWhen we mention a repository here, we mean that we want you to look at a specific folder. The repository for this website for example can be found on GitHub directly. You will always be contributing to a repository, in order to contribute to the handbook.\n\n\n\nA repository is owned by one or multiple people on GitHub. If you are not one of them, you can create a copy of the repository (folder) to make your edits in. This act of creating a copy is called “forking.”\nWhen you create a copy, you do not have to worry about accidentally removing or destroying the handbook. Your changes are not reflected in the website until you submit a pull request."
  },
  {
    "objectID": "contributing.html#direct-contributions",
    "href": "contributing.html#direct-contributions",
    "title": "Contributing",
    "section": "",
    "text": "We offer a portal to reduce the barriers to contribute to the Research Support Handbook. You only need an internet connection and articulate what you want us to include. No accounts necessary 😊\n\n\n\n\n\n\nNote\n\n\n\nOpen the contribution portal by clicking here or copy-pasting: https://ez-github-contributor.netlify.app/\n\n\nYou can report issues you find with the Research Support Handbook using the “Report a problem” tab. This is a way for you to share your feedback with us.\nYou can propose new topics to the Research Support Handbook using the “Propose new page” tab. This will be considered for inclusion as a topic.\n\n\n\nScreenshot of the contributor portal\n\n\nIf you want to be credited with contributing, please share your name. If you’d like to hear back about what was done with your feedback or proposal, please also provide a direct way to contact you."
  },
  {
    "objectID": "contributing.html#contributing-via-github",
    "href": "contributing.html#contributing-via-github",
    "title": "Contributing",
    "section": "",
    "text": "Note\n\n\n\nFor the next steps you need a GitHub account to contribute. You can create one directly on GitHub.\n\n\n\n\nThe easiest and quickest way to contribute to the book is make suggested edits. On each page you will find a button reading “Edit this page” (usually on the right).\n\n\n\nScreenshot of a handbook topic, with a red box on the right hand side of the page indicating where to find the “Edit this page” button\n\n\nWhen you click that, you will immediately be taken to GitHub to edit the text of that specific page. You may be prompted to create a fork (forking) in case these are your first edits.\n\n\n\nScreenshot of the GitHub file editor, with some changes made and the “Commit changes” button active\n\n\nOnce you made your edits, you are ready to commit (save) your changes and submit your pull request, requesting those changes to be included in the handbook.\n\n\n\nTo add a new topic, you need to create a new file ending in .qmd (e.g., topics/example.qmd). You can do this by visiting the handbook page on GitHub and clicking Add file -&gt; New file.\n\n\n\nScreenshot of GitHub highlighting where to find the “New file” button\n\n\nWhen you click this button you may be asked to fork the repository. This is not a problem so go ahead!\nThe topic itself needs to be written in Markdown. Every topic must contain a first level heading (e.g., # Heading), which will be the topic title. Section headings are second level headings (e.g., ## Section).\nAfter that, you are ready to submit your pull request! The reviewers will help you place the topic in the right place of the book.1\n\n\n\nOnce you have made suggested changes, a pull request is the way for you to ask for your changes to be incorporated into the handbook. The people maintaining the handbook will review what you wrote, ask some questions, and accept or decline your contributions.\nWe recommend keeping your suggested changes small or limited in scope, and explaining why you are suggesting these changes. It is more likely your changes are included when you are fixing a typo or adding a paragraph, and less likely if you are revising the entire handbook. It is also more likely they are included if you explain why you are suggesting the changes, rather than dropping by and making edits without any context.\nIf you are adding a new topic, it is definitely recommended to open an issue first to see whether there is a need for it (and maybe you’ll find collaborators!).\nDuring the review process you may be asked to update your changes, or revisions may be added by the people maintaining the handbook. It is helpful if you keep an eye on your GitHub account to ensure timely responses to help the process along.\n\n\n\nThe book is created using Markdown - you can get familiarized with the basic syntax on the Markdown website. The getting started quick items are:\n# Heading level 1\n## Heading level 2\n### Heading level 3\n\nYou simply write text as you are used to. To make something *italic*, **bold**, or ***bold and italic***.\n\n&gt; this is how you add quotes\n\n- or lists\n- that can go on \n- and on\nIf you want to add code, use references, create links, or footnotes - it is all possible. We will expand examples here based on your needs, so if you need help, let us know by reporting an issue!\n\n\n\nWe use GitHub to create this website automatically, and to manage all the incoming updates. You do not need to know how it works entirely, but we want to help you understand some things so you are not confused.\n\n\nA repository on GitHub is like a folder on your computer. This can be many things, depending on what files it contains.\nWhen we mention a repository here, we mean that we want you to look at a specific folder. The repository for this website for example can be found on GitHub directly. You will always be contributing to a repository, in order to contribute to the handbook.\n\n\n\nA repository is owned by one or multiple people on GitHub. If you are not one of them, you can create a copy of the repository (folder) to make your edits in. This act of creating a copy is called “forking.”\nWhen you create a copy, you do not have to worry about accidentally removing or destroying the handbook. Your changes are not reflected in the website until you submit a pull request."
  },
  {
    "objectID": "contributing.html#footnotes",
    "href": "contributing.html#footnotes",
    "title": "Contributing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you are really enthusiastic and want to do it yourself: The topic needs to be added to the topics list in _quarto.yml. You can find the topics list around line 24 of that file.↩︎"
  },
  {
    "objectID": "pathways/data-lifecycle.html",
    "href": "pathways/data-lifecycle.html",
    "title": "Data lifecycle",
    "section": "",
    "text": "The data lifecycle helps you find, document, and preserve data."
  },
  {
    "objectID": "pathways/data-lifecycle.html#overview",
    "href": "pathways/data-lifecycle.html#overview",
    "title": "Data lifecycle",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "pathways/data-lifecycle.html#discover-initiate",
    "href": "pathways/data-lifecycle.html#discover-initiate",
    "title": "Data lifecycle",
    "section": "Discover & Initiate",
    "text": "Discover & Initiate\nThere are two pages in this section:\nFinding Existing Data: how to find and reuse data that already exist\nData Citation: how to cite data that you use in your research\n\nFinding Existing Data\n\nRe-using Existing Data\nAnything that can be used for analysis can be considered “data(sets)”. Many national and international organisations provide access to large datasets free of charge: this is called Open Data.\nDatasets may contain different kinds of data files, e.g. raw or edited/cleaned data, and macro or micro data. Raw data refers to the data as they are primarily collected, and includes all data, even the missed or mismatched pieces in the data file. Edited or cleaned data refers to data that have been tidied up for analysis and publication. Macro data and statistics are results based on micro data units and provide a general overview of the micro data. Although datasets can contain data of varying type or aggregation level, and there may be overlap between these definitions, each element can contain very important information.\nWhen re-using research data, scientists must be familiar with the rules and regulations governing data copyright, intellectual property rights, and laws governing sensitive or personal information. SURF has compiled a report on the legal status of raw data including information on the types of consent required for the re-use of data. Your Privacy Champion can answer questions about the use of personal data. IXA can provide legal help with the re-use of data.\nSee also the ZonMw explanation of different kinds of property rights in the Netherlands (text available in Dutch only).\n\n\nSources for Finding Existing Datasets\nThe number of datasets that are available grows rapidly. Datasets are made available in many formats, by many people or organizations. Some datasets are raw files and some are specifically organised and formatted as databases that require a licence or subscription to use them. The library of the Vrije Universiteit Amsterdam has collected links to some of the data repositories used and has licensed several databases.\n\nPopular Free and Licensed Databases: These can be found in the library e-resources list or with LibSearch Advanced.\n\nIf you need help finding & using free or licensed sources you can contact the Research Data Services Helpdesk. For students and personnel in the fields of economics, finance, or organisation science a separate LibGuide has been created to help them find and use/re-use data.\nYou can also start looking for data in these four places:\n\nThe literature. Research articles may point you to the data that they are based on. Sometimes, (part of) the data are added to the article as supplementary files, and sometimes the data are published separately in a data repository. In the latter case, the article usually provides a clear reference to the published dataset. Some datasets may even be specifically published in Data Journals.\nScientific data repositories. Data repositories are platforms used to access and archive research data. Universities often provide a repository for data archiving, but other platforms arranged by discipline or by country also exist. Some repositories are only accessible to consortium members, whereas others are free of charge. Many universities in the Netherlands use DataverseNL to archive datasets for the mid-term. Long-term archiving is provided by the national research data archives DANS and 4TU.Research Data. In Europe, B2SHARE and Zenodo are platforms used to access research data. Data repositories can be accessed by searching by topic or country using Re3data, a data repository registry. The VU has its own research portal, PURE, where researchers register their datasets. You can find instructions on how to register your own dataset in PURE on the Dataset Registration page of this LibGuide.\nData search engines. Search engines allow you to quickly browse data sets and supplementary data files published by researchers. They cover data sets from many sources. This makes them useful for quick orientation on a topic. Example of a search engines are: DataCite, Google DataSet Search, Elsevier Dataset Search.\nData portals of (governmental) organisations. Organisations that regularly collect (statistical) data sometimes offer these data through their own portal. An example is Eurostat, which collects and disseminates statistics at the European level, by country and by theme. Some of these websites have been linked in the Finding data LibGuide.\n\n\n\nData Sources for VU Researchers\nResearchers from the Vrije Universiteit Amsterdam have also developed some databases containing data collected during research. See here for some examples:\n\nNederlands Tweelingenregister (Netherlands Twin Register) The database contains data on twins and their families and was created to do research on the relationship between genetics and growth, development, personality, behaviour, diseases, mental health and all kinds of risks.\nGeoplaza VU - the portal for all matters related to GIS (Geographical Information Systems) and geodata at the VU University Amsterdam. It offers students and employers a platform to exchange, examine and download digital map material.\nDutch monasteries - database with information about Dutch monasteries of the Middle Ages.\nSlave owners in Amsterdam 1863 - the place of living of owners of slaves in Amsterdam in 1863, visualized in GeoPlaza.\nDeaths at the Borders Database - collection of official, state-produced evidence on people who died while attempting to reach southern EU countries from the Balkans, the Middle East, and North & West Africa, and whose bodies were found in or brought to Europe.\nDatasets published by VU Researchers can be found at the VU Research Portal.\n\n\n\n\nData Citation\n\nCitation Elements\nCiting data is not different from citing a publication. Make sure to check the rules of the journal to know how you should cite when writing an article for a specific academic journal. For all of the journals, however, the minimum compulsory elements in a data citation include:\n\nAuthor(s): Name of the author (creator) of the dataset\nTitle: Name of the dataset\nDate of publication\nPublisher: Archive where dataset is stored\nPersistent Identifier: Unique identifier, most common is the DOI (see section Data Publication).\n\nOptional elements that may be included in the reference are:\n\nFile Type: Codebook, movie, software\nVersion: Version number of the edition\nCreation Date\nDate of Consultation (last) Example of a data citation:\n\nStephens, William, 2020, “Resiliences to Radicalisation - QSort Data”, https://doi.org/10.34894/35MTMN, DataverseNL, V1.\nFor more information, see the following guidelines:\n\nDataverse\nDataCite\nDCC UK\nData Citation Synthesis Group (2014). Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11"
  },
  {
    "objectID": "pathways/data-lifecycle.html#plan-design",
    "href": "pathways/data-lifecycle.html#plan-design",
    "title": "Data lifecycle",
    "section": "Plan & Design",
    "text": "Plan & Design"
  },
  {
    "objectID": "pathways/data-lifecycle.html#collect-store",
    "href": "pathways/data-lifecycle.html#collect-store",
    "title": "Data lifecycle",
    "section": "Collect & Store",
    "text": "Collect & Store"
  },
  {
    "objectID": "pathways/data-lifecycle.html#process-analyse",
    "href": "pathways/data-lifecycle.html#process-analyse",
    "title": "Data lifecycle",
    "section": "Process & Analyse",
    "text": "Process & Analyse"
  },
  {
    "objectID": "pathways/data-lifecycle.html#document-preserve",
    "href": "pathways/data-lifecycle.html#document-preserve",
    "title": "Data lifecycle",
    "section": "Document & Preserve",
    "text": "Document & Preserve"
  },
  {
    "objectID": "pathways/data-lifecycle.html#publish-share",
    "href": "pathways/data-lifecycle.html#publish-share",
    "title": "Data lifecycle",
    "section": "Publish & Share",
    "text": "Publish & Share"
  },
  {
    "objectID": "topics/bazis.html",
    "href": "topics/bazis.html",
    "title": "BAZIS HPC",
    "section": "",
    "text": "The BAZIS is a compute cluster for research at the Vrije Universiteit Amsterdam. It provides a service between the general facilties at the SURF HPC center and the desktop. It is a heterogenious system composed of clusters and servers from research departments and a general partition.\nIn this topic you will find information to get you started and best practices. Clustercomputing can be very powerfull and usefull skill to add to your toolbox and get more science done.\nAlso take a look at the SURF wiki about Snellius, it contains a lot of information which applies to Bazis as well. https://servicedesk.surf.nl/wiki/display/WIKI/Snellius\nInformation and how to get an account can be found on VU Service Portal under IT &gt; Research &gt; HPC Cluster Computing\nBAZIS is maintained by IT for Research.\n\n\n\nVU BAZIS Cluster\n\n\n\n\n\n\nUse your favourite SSH client to login at (bazis.labs.vu.nl?). On windows we recommend MobaXterm or MS Terminal; Mac users can use iterm. Direct access is only possible from the Campus or from SURF. From other network locations first connect to the stepstone (ssh.data.vu.nl?), or use eduVPN Institutional Access (below).\n\n\n\nInstall the client. Start eduVPN and choose Institute Access to connect. You will need MFA to authenticate. Students can activate MFA at the servicedesk ( kb-item 11809 )\n\n\n\nMobaXterm has an integrated FTP file browser. Once you have logged in to the HPC system, you will see the file browser to the left of the terminal window, where it shows the contents of your home folder. You can browse through these folders, and drag-and-drop files and folders between this FTP file browser and the Windows File Explorer. Alternatively, you can use the download/upload buttons at the top of the FTP file browser window. A green refresh button is also located there to refresh the contents of the current folder. You can also open files in the FTP file browser to edit them directly. Upon saving, you’ll be asked if you want change these files on the HPC system.\nOther SFTP browsers There are a large number of free FTP browser out there. Some examples are\n\nFilezilla (Windows, MacOS, Linux)\nCyberduck (Windows, MacOS)\nWinSCP (Windows)\n\n\n\n\nSSH keys are an alternative method for authentication to obtain access to remote computing systems. They can also be used for authentication when transferring files or for accessing version control systems like github.\nThe cluster uses ssh keys to manage batch jobs.\nOn your workstation create ssh key pair ssh-keygen -t ed25519 -a 100\n\n-a (default is 16): number of rounds of passphrase derivation; increase to slow down brute force attacks.\n-t (default is rsa): specify the cryptographic algorithm. ed25519 is faster and shorter than RSA for comparable strength.\n-f (default is /home/user/.ssh/id_algorithm): filename to store your keys. If you already have SSH keys, make sure you specify a different name: ssh-keygen will overwrite the default key if you don’t specify!\n\nIf ed25519 is not available, use the older (but strong and trusted) RSA cryptography: ssh-keygen -a 100 -t rsa -b 4096\nWhen prompted, enter a strong password that you will remember.\nNote: on windows you can use MobaKeyGen from MobaXterm, but on Windows 11 Powershell or Command Prompt works as well.\nIn your ~/.ssh directory you will find a public and private key. Make sure to keep the private key safe as anyone with the private key has access.\nNow, when you add your public key to the ~/.ssh/authorized_keys file in a remote system, your key will be used to login.\nYou can either use copy-paste or the ssh-copy-id command:\n$ ssh-copy-id user@remote-host\nThe authenticity of host 'remote-host (192.168.111.135)' can't be established.\nECDSA key fingerprint is SHA256:hXGpY0ALjXvDUDF1cDs2N8WRO9SuJZ/lfq+9q99BPV0.\nAre you sure you want to continue connecting (yes/no)? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 2 key(s) remain to be installed -- if you are prompted now it is to install the new keys\nuser@remote-host's password:\n\nNumber of key(s) added: 2\nNow try logging into the machine, with: “ssh ‘user@remote-host’” and check to make sure that only the key(s) you wanted were added.\n##Advanced: Forwarding X\n##Fix Warning: Remote Host Identification Has Changed If you are sure that it is harmless and the remote host key has been changed in a legitimate way, you can skip the host key checking by sending the key to a null known_hosts file:\n$ ssh -o \"UserKnownHostsFile=/dev/null\" -o \"StrictHostKeyChecking=no\" username@bazis.labs.vu.nl\nTo make the change permanent remove the offending host key from your ~/.ssh/known_hosts file with:\nssh-keygen -R \"hostname\"\n\nReferences: 1. hpc carpentry 2. Comparing SSH keys 3. public key cryptography for non geeks 4. disable ssh host key checking\n\n\n\n\n\n\nSee jobs in the queue for a given user\n   squeue -u username\nShow available node features\n   sinfo -o \"%20N  %10c  %10m  %25f  %10G \"\nSubmit a job\n   sbatch script\nShow the status of a currently running job\n   sstat -j jobID\nShow the final status of a finished job\n   sacct -j jobID\nCancel a job\n   scancel jobid\n\n\n\nOrganise your input, output and temporary data. Make use of fast scratch directory ($TMPDIR).\nDon’t run large computation on the login nodes! It negatively impacts all cluster users. Grab a compute node with srun –pty bash option.\n\n\n\nThe SLURM constraint option allows for further control over which nodes your job can be scheduled on in a particular parition/queue. You may require a specific processor family or network interconnect. The features that can be used with the sbatch constraint option are defined by the system administrator and thus vary among HPC sites.\nConstraints available on BAZIS are cpu architecture and gpu. Example (singole constraint):\n#SBATCH --constraint=zen2\nExample combining constraints:\n#SBATCH --constraint=\"zen2|haswell\"\n\n\n\nThe parts of a modern computer we need to understand to apply to running jobs are listed here. (Note: This is way oversimplified and intended to give a basic overview for the purposes of understanding how to request resources from Slurm, there are a lot of resources out there to dig deeper into computer architecture.)\n\n\nA physical motherboard which contains one or more of each of Socket, Memory bus and PCI bus.\n\n\n\nA physical socket on a motherboard which accepts a physical CPU part.\n\n\n\nA physical part that is plugged into a socket.\n\n\n\nA physical CPU core, one of many possible cores, that are part of a CPU.\n\n\n\nA virtual CPU thread, associated with a specific Core. This can be enabled or disabled on a system. On BAZIS hyperthreading is typically enabled. Compute intensive workloads will benefit to disable hyperthreading.\n\n\n\nA communication bus between system memory and a Socket/CPU.\n\n\n\nA communication bus between a Socket/CPU and I/O controllers (disks, networking, graphics,…) in the server.\nSlurm complicates this, however, by using the terms core and cpu interchangeably depending on the context and Slurm command. –cpus-per-taks= for example is actually specifying the number of cores per task.\n\n\n\n\n\n\n#!/bin/bash -l\n#SBATCH -J MyTestJob\n#SBATCH -N 1\n#SBATCH -p defq\n\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\necho \"== Scratch dir. : ${TMPDIR}\"\n\ncd $TMPDIR\n# Your more useful application can be started below!\nhostname\n\n\n\n\n\n\n\nEach slurm job will have a fast scratch dir allocated on the nodes which is deleted after finishing the job. use the $TMPDIR virable to use this space for example to store intermediate results or work on many files.\n\n\n\nUse the workspace tools on the beegfs parallel filesystem to create a project space. The project space is a directory, with an associated expiration date, created on behalf of a user, to prevent disks from uncontrolled filling. The beegeefs parallel filesystem is faster than your usual NFS home space, but not backed up, so ideal for data which is easily recreated.\nYour project space lives in the filesystem under: /scratch-shared/ws\nThe project space is managed with the hpc-workspace tooling You can add them to your environment with: module load hpc-workspace\nExample: setup a workspace “MyData” in a batchjob for 10 days.\nSCR=$(ws_allocate MyData 10)\ncd $SCR\nCheck your workspaces\n$ ws_list \nid: MyData\n     workspace directory  : /scratch-shared/ws/username-MyData\n     remaining time       : 9 days 23 hours\n     creation time        : Wed Mar 13 23:51:57 2013\n     expiration date      : Sat Mar 23 23:51:57 2013\n     available extensions : 15\nRelease the project space with\nws_release MyData\nFor user guide see https://github.com/holgerBerger/hpc-workspace/blob/master/user-guide.md\n\n\n\n\nPython has many powerfull powerfull packages. In scientific computing many packages may be used in a single project. To manage many python packages often a package manager as conda is used.\nOn a HPC system we do not prefer conda as it does not use optimised binaries and the cache can take up a lot of space, but we understand it is usefull in some cases and try to help.\nWorking with virtual environments further makes the python environment better to manage\n\nA virtual environment is a named, isolated, working copy of Python that that maintains its own files,\ndirectories, and paths so that you can work with specific versions of libraries or Python itself without affecting other Python projects.\nVirtual environmets make it easy to cleanly separate different projects and avoid problems with different dependencies and version requirements across components.\n\nIn short: - use virtualenv (preferred) or conda - create an isolated environment - Install packages - Activate a virtual environment - Deactivate a virtual environment - Delete a virtual environment\n\n\nPython requirements files are a great way to keep track of the Python modules. It is a simple text file that saves a list of the modules and packages required by your project. By creating a Python requirements.txt file, you save yourself the hassle of having to track down and install all of the required modules manually.\nA reuirements file is a simple text file, which looks like this.\ntensorflow==2.3.1\nuvicorn==0.12.2\nfastapi==0.63.0\nInstalling modules from a requirements file is easy as.\npip install -r requirements.txt\nA requirements file can also be generated with:\npip freeze &gt; requirements.txt\nSee the article referenced below for more information.\n\n\n\nThe first line in a script usually starts the interpreter and is called the Shebang. It is recommended to use /usr/bin/env, which can interpret your $PATH. This makes scripts more portable than hard coded paths..\n#!/usr/local/bin/python\nWill only run your script if python is installed in /usr/local/bin.\n#!/usr/bin/env python\nWill interpret your $PATH, and find python in any directory in your $PATH.\nSo your script is more portable, and will work without modification on systems where python is installed as /usr/bin/python, or /usr/local/bin/python, or even custom directories (that have been added to $PATH), like /opt/local/bin/python.\nFurther Reading * python virtual environments primer * python requirements.txt file\n\n\n\n\nR has many powerfull scientific packages and a strong community. Installing and maintaining packages for R can be hard. On BAZIS the Bioconductor suite is installed and can be loaded with the appropiate module environment.\nWhen first running R on a Cluster some changes in the workflow are required making the transition from working interactively from a terminal to scripts in batchmode.\n\n\nPretty much all the time we get errors. Errors can be simple e.g.syntax error, R/python version error or more complex e.g. a problem in our data. In either case, please pay attention to what the error says carefully, because often the solution is in that message or at least it is the starting point of the solution while debugging your code. If it is an error you have not seen before, simply google it. Often you will find a solution in websites like stackoverflow.\n\n\n\n\n\nThe png() default device used the X11 driver, which is not avaialble in batch mode or remote operation. Adding the type=“cairo” option to your code solves this issue.\nExample:\npdf(file = \"testR.pdf\", width = 4, height = 4)\nplot(x = 1:10, y = 1:10)\nabline(v = 0 )\ntext(x=0, y=1, labels = \"random text\")\ndev.off()\npng(file = \"testR.png\", type=\"cairo\", width = 4, height = 4)\nplot(x = 1:10, y = 1:10)\nabline(v = 0 )\ntext(x=0, y=1, labels = \"random text\")\ndev.off()\nReferences * How do I produce PNG graphics in batch mode?\n\n\n\n\n\nMatlab has several features to work in batch mode on a HPC cluster. Assuming you know how to create matlab scripts we start simply by executing matlab interactively on a compute node\n\n\nRequest resources (1 node, 1 cpu) in a partition\nsrun -N 1 -p defq --pty /bin/bash\nmodule load matlab/R2023a\ncd your/data/\nHere is an example of a trivial MATLAB script (hello_world.m):\nfprintf('Hello world.\\n')\nRun with matlab using only one computational thread.\n$ matlab -nodisplay -singleCompThread -r hello_world\nHello world.\n&gt;&gt;\nMatlab waits at the end of the script if there is no exit. In an compute job this would keep the job running untill the wallclocklimit so we add an exit at the end. The convenient “-batch” option combines these options.\n-batch MATLAB_command   - Start MATLAB and execute the MATLAB command(s) with no desktop\n                              and certain interactive capabilities disabled. Terminates\n                              upon successful completion of the command and returns exit\n                              code 0. Upon failure, MATLAB terminates with a non-zero exit.\n                              Cannot be combined with -r.\n$ matlab -batch hello_world\n\n\n\nCombining this in a slurm script we can queue matlab workloads.\n    #!/bin/bash -l\n    #SBATCH -J MyMatlab\n    #SBATCH -N 1\n    #SBATCH --cpus-per-task=1\n    #SBATCH -p defq   \n    #SBATCH --output=%x_%j.out\n    #SBATCH --error=%x_%j.err\n    #SBATCH --mail-type=END,FAIL\n    #SBATCH --mail-user=&lt;YOUR EMAIL&gt;\n    \n    # Note: for parallel operations increase cpus-per-task above\n    # Note 2: output and error logs can be given absolute paths \n    \n    echo \"== Starting run at $(date)\"\n    echo \"== Job ID: ${SLURM_JOBID}\"\n    echo \"== Node list: ${SLURM_NODELIST}\"\n    echo \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\n    echo \"== Scratch dir. : ${TMPDIR}\"\n    \n    # cd $TMPDIR\n    # or change to a project folder with matlab file e.g. hello_World.m\n    # cd your/data\n\n    # Load matlab module\n    module load 2022 matlab/R2023a\n    \n    # execute\n    matlab -batch hello_world\n\n\n\n\n\n\n\nmathworks-parpool\n\n\n\n\n\niRODS/Yoda is a middleware and webinterface to store enriched data. On BAZIS the icommands are available to allow direct access.\nTo use the icommands a configuration file in your home folder is required. https://yoda.vu.nl/site/getting-started/icommands.html#configuration\nAfter configuration use iinit to connect, use a Data Access Password, similar to a WebDAV connection.\nFind an overview of the icommands here: https://docs.irods.org/master/icommands/user/ Check the ipwd, ils, icd, iput, iget en irsync commands for navigation and data transfer.\nYour project folder is located at /vu/home\nicommands Yoda-VU"
  },
  {
    "objectID": "topics/bazis.html#connect",
    "href": "topics/bazis.html#connect",
    "title": "BAZIS HPC",
    "section": "",
    "text": "Use your favourite SSH client to login at (bazis.labs.vu.nl?). On windows we recommend MobaXterm or MS Terminal; Mac users can use iterm. Direct access is only possible from the Campus or from SURF. From other network locations first connect to the stepstone (ssh.data.vu.nl?), or use eduVPN Institutional Access (below).\n\n\n\nInstall the client. Start eduVPN and choose Institute Access to connect. You will need MFA to authenticate. Students can activate MFA at the servicedesk ( kb-item 11809 )\n\n\n\nMobaXterm has an integrated FTP file browser. Once you have logged in to the HPC system, you will see the file browser to the left of the terminal window, where it shows the contents of your home folder. You can browse through these folders, and drag-and-drop files and folders between this FTP file browser and the Windows File Explorer. Alternatively, you can use the download/upload buttons at the top of the FTP file browser window. A green refresh button is also located there to refresh the contents of the current folder. You can also open files in the FTP file browser to edit them directly. Upon saving, you’ll be asked if you want change these files on the HPC system.\nOther SFTP browsers There are a large number of free FTP browser out there. Some examples are\n\nFilezilla (Windows, MacOS, Linux)\nCyberduck (Windows, MacOS)\nWinSCP (Windows)\n\n\n\n\nSSH keys are an alternative method for authentication to obtain access to remote computing systems. They can also be used for authentication when transferring files or for accessing version control systems like github.\nThe cluster uses ssh keys to manage batch jobs.\nOn your workstation create ssh key pair ssh-keygen -t ed25519 -a 100\n\n-a (default is 16): number of rounds of passphrase derivation; increase to slow down brute force attacks.\n-t (default is rsa): specify the cryptographic algorithm. ed25519 is faster and shorter than RSA for comparable strength.\n-f (default is /home/user/.ssh/id_algorithm): filename to store your keys. If you already have SSH keys, make sure you specify a different name: ssh-keygen will overwrite the default key if you don’t specify!\n\nIf ed25519 is not available, use the older (but strong and trusted) RSA cryptography: ssh-keygen -a 100 -t rsa -b 4096\nWhen prompted, enter a strong password that you will remember.\nNote: on windows you can use MobaKeyGen from MobaXterm, but on Windows 11 Powershell or Command Prompt works as well.\nIn your ~/.ssh directory you will find a public and private key. Make sure to keep the private key safe as anyone with the private key has access.\nNow, when you add your public key to the ~/.ssh/authorized_keys file in a remote system, your key will be used to login.\nYou can either use copy-paste or the ssh-copy-id command:\n$ ssh-copy-id user@remote-host\nThe authenticity of host 'remote-host (192.168.111.135)' can't be established.\nECDSA key fingerprint is SHA256:hXGpY0ALjXvDUDF1cDs2N8WRO9SuJZ/lfq+9q99BPV0.\nAre you sure you want to continue connecting (yes/no)? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 2 key(s) remain to be installed -- if you are prompted now it is to install the new keys\nuser@remote-host's password:\n\nNumber of key(s) added: 2\nNow try logging into the machine, with: “ssh ‘user@remote-host’” and check to make sure that only the key(s) you wanted were added.\n##Advanced: Forwarding X\n##Fix Warning: Remote Host Identification Has Changed If you are sure that it is harmless and the remote host key has been changed in a legitimate way, you can skip the host key checking by sending the key to a null known_hosts file:\n$ ssh -o \"UserKnownHostsFile=/dev/null\" -o \"StrictHostKeyChecking=no\" username@bazis.labs.vu.nl\nTo make the change permanent remove the offending host key from your ~/.ssh/known_hosts file with:\nssh-keygen -R \"hostname\"\n\nReferences: 1. hpc carpentry 2. Comparing SSH keys 3. public key cryptography for non geeks 4. disable ssh host key checking"
  },
  {
    "objectID": "topics/bazis.html#using-slurm",
    "href": "topics/bazis.html#using-slurm",
    "title": "BAZIS HPC",
    "section": "",
    "text": "See jobs in the queue for a given user\n   squeue -u username\nShow available node features\n   sinfo -o \"%20N  %10c  %10m  %25f  %10G \"\nSubmit a job\n   sbatch script\nShow the status of a currently running job\n   sstat -j jobID\nShow the final status of a finished job\n   sacct -j jobID\nCancel a job\n   scancel jobid\n\n\n\nOrganise your input, output and temporary data. Make use of fast scratch directory ($TMPDIR).\nDon’t run large computation on the login nodes! It negatively impacts all cluster users. Grab a compute node with srun –pty bash option.\n\n\n\nThe SLURM constraint option allows for further control over which nodes your job can be scheduled on in a particular parition/queue. You may require a specific processor family or network interconnect. The features that can be used with the sbatch constraint option are defined by the system administrator and thus vary among HPC sites.\nConstraints available on BAZIS are cpu architecture and gpu. Example (singole constraint):\n#SBATCH --constraint=zen2\nExample combining constraints:\n#SBATCH --constraint=\"zen2|haswell\"\n\n\n\nThe parts of a modern computer we need to understand to apply to running jobs are listed here. (Note: This is way oversimplified and intended to give a basic overview for the purposes of understanding how to request resources from Slurm, there are a lot of resources out there to dig deeper into computer architecture.)\n\n\nA physical motherboard which contains one or more of each of Socket, Memory bus and PCI bus.\n\n\n\nA physical socket on a motherboard which accepts a physical CPU part.\n\n\n\nA physical part that is plugged into a socket.\n\n\n\nA physical CPU core, one of many possible cores, that are part of a CPU.\n\n\n\nA virtual CPU thread, associated with a specific Core. This can be enabled or disabled on a system. On BAZIS hyperthreading is typically enabled. Compute intensive workloads will benefit to disable hyperthreading.\n\n\n\nA communication bus between system memory and a Socket/CPU.\n\n\n\nA communication bus between a Socket/CPU and I/O controllers (disks, networking, graphics,…) in the server.\nSlurm complicates this, however, by using the terms core and cpu interchangeably depending on the context and Slurm command. –cpus-per-taks= for example is actually specifying the number of cores per task.\n\n\n\n\n\n\n#!/bin/bash -l\n#SBATCH -J MyTestJob\n#SBATCH -N 1\n#SBATCH -p defq\n\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\necho \"== Scratch dir. : ${TMPDIR}\"\n\ncd $TMPDIR\n# Your more useful application can be started below!\nhostname"
  },
  {
    "objectID": "topics/bazis.html#workspace",
    "href": "topics/bazis.html#workspace",
    "title": "BAZIS HPC",
    "section": "",
    "text": "Each slurm job will have a fast scratch dir allocated on the nodes which is deleted after finishing the job. use the $TMPDIR virable to use this space for example to store intermediate results or work on many files.\n\n\n\nUse the workspace tools on the beegfs parallel filesystem to create a project space. The project space is a directory, with an associated expiration date, created on behalf of a user, to prevent disks from uncontrolled filling. The beegeefs parallel filesystem is faster than your usual NFS home space, but not backed up, so ideal for data which is easily recreated.\nYour project space lives in the filesystem under: /scratch-shared/ws\nThe project space is managed with the hpc-workspace tooling You can add them to your environment with: module load hpc-workspace\nExample: setup a workspace “MyData” in a batchjob for 10 days.\nSCR=$(ws_allocate MyData 10)\ncd $SCR\nCheck your workspaces\n$ ws_list \nid: MyData\n     workspace directory  : /scratch-shared/ws/username-MyData\n     remaining time       : 9 days 23 hours\n     creation time        : Wed Mar 13 23:51:57 2013\n     expiration date      : Sat Mar 23 23:51:57 2013\n     available extensions : 15\nRelease the project space with\nws_release MyData\nFor user guide see https://github.com/holgerBerger/hpc-workspace/blob/master/user-guide.md"
  },
  {
    "objectID": "topics/bazis.html#python-virtual-environments",
    "href": "topics/bazis.html#python-virtual-environments",
    "title": "BAZIS HPC",
    "section": "",
    "text": "Python has many powerfull powerfull packages. In scientific computing many packages may be used in a single project. To manage many python packages often a package manager as conda is used.\nOn a HPC system we do not prefer conda as it does not use optimised binaries and the cache can take up a lot of space, but we understand it is usefull in some cases and try to help.\nWorking with virtual environments further makes the python environment better to manage\n\nA virtual environment is a named, isolated, working copy of Python that that maintains its own files,\ndirectories, and paths so that you can work with specific versions of libraries or Python itself without affecting other Python projects.\nVirtual environmets make it easy to cleanly separate different projects and avoid problems with different dependencies and version requirements across components.\n\nIn short: - use virtualenv (preferred) or conda - create an isolated environment - Install packages - Activate a virtual environment - Deactivate a virtual environment - Delete a virtual environment\n\n\nPython requirements files are a great way to keep track of the Python modules. It is a simple text file that saves a list of the modules and packages required by your project. By creating a Python requirements.txt file, you save yourself the hassle of having to track down and install all of the required modules manually.\nA reuirements file is a simple text file, which looks like this.\ntensorflow==2.3.1\nuvicorn==0.12.2\nfastapi==0.63.0\nInstalling modules from a requirements file is easy as.\npip install -r requirements.txt\nA requirements file can also be generated with:\npip freeze &gt; requirements.txt\nSee the article referenced below for more information.\n\n\n\nThe first line in a script usually starts the interpreter and is called the Shebang. It is recommended to use /usr/bin/env, which can interpret your $PATH. This makes scripts more portable than hard coded paths..\n#!/usr/local/bin/python\nWill only run your script if python is installed in /usr/local/bin.\n#!/usr/bin/env python\nWill interpret your $PATH, and find python in any directory in your $PATH.\nSo your script is more portable, and will work without modification on systems where python is installed as /usr/bin/python, or /usr/local/bin/python, or even custom directories (that have been added to $PATH), like /opt/local/bin/python.\nFurther Reading * python virtual environments primer * python requirements.txt file"
  },
  {
    "objectID": "topics/bazis.html#r-environment",
    "href": "topics/bazis.html#r-environment",
    "title": "BAZIS HPC",
    "section": "",
    "text": "R has many powerfull scientific packages and a strong community. Installing and maintaining packages for R can be hard. On BAZIS the Bioconductor suite is installed and can be loaded with the appropiate module environment.\nWhen first running R on a Cluster some changes in the workflow are required making the transition from working interactively from a terminal to scripts in batchmode.\n\n\nPretty much all the time we get errors. Errors can be simple e.g.syntax error, R/python version error or more complex e.g. a problem in our data. In either case, please pay attention to what the error says carefully, because often the solution is in that message or at least it is the starting point of the solution while debugging your code. If it is an error you have not seen before, simply google it. Often you will find a solution in websites like stackoverflow.\n\n\n\n\n\nThe png() default device used the X11 driver, which is not avaialble in batch mode or remote operation. Adding the type=“cairo” option to your code solves this issue.\nExample:\npdf(file = \"testR.pdf\", width = 4, height = 4)\nplot(x = 1:10, y = 1:10)\nabline(v = 0 )\ntext(x=0, y=1, labels = \"random text\")\ndev.off()\npng(file = \"testR.png\", type=\"cairo\", width = 4, height = 4)\nplot(x = 1:10, y = 1:10)\nabline(v = 0 )\ntext(x=0, y=1, labels = \"random text\")\ndev.off()\nReferences * How do I produce PNG graphics in batch mode?"
  },
  {
    "objectID": "topics/bazis.html#matlab",
    "href": "topics/bazis.html#matlab",
    "title": "BAZIS HPC",
    "section": "",
    "text": "Matlab has several features to work in batch mode on a HPC cluster. Assuming you know how to create matlab scripts we start simply by executing matlab interactively on a compute node\n\n\nRequest resources (1 node, 1 cpu) in a partition\nsrun -N 1 -p defq --pty /bin/bash\nmodule load matlab/R2023a\ncd your/data/\nHere is an example of a trivial MATLAB script (hello_world.m):\nfprintf('Hello world.\\n')\nRun with matlab using only one computational thread.\n$ matlab -nodisplay -singleCompThread -r hello_world\nHello world.\n&gt;&gt;\nMatlab waits at the end of the script if there is no exit. In an compute job this would keep the job running untill the wallclocklimit so we add an exit at the end. The convenient “-batch” option combines these options.\n-batch MATLAB_command   - Start MATLAB and execute the MATLAB command(s) with no desktop\n                              and certain interactive capabilities disabled. Terminates\n                              upon successful completion of the command and returns exit\n                              code 0. Upon failure, MATLAB terminates with a non-zero exit.\n                              Cannot be combined with -r.\n$ matlab -batch hello_world\n\n\n\nCombining this in a slurm script we can queue matlab workloads.\n    #!/bin/bash -l\n    #SBATCH -J MyMatlab\n    #SBATCH -N 1\n    #SBATCH --cpus-per-task=1\n    #SBATCH -p defq   \n    #SBATCH --output=%x_%j.out\n    #SBATCH --error=%x_%j.err\n    #SBATCH --mail-type=END,FAIL\n    #SBATCH --mail-user=&lt;YOUR EMAIL&gt;\n    \n    # Note: for parallel operations increase cpus-per-task above\n    # Note 2: output and error logs can be given absolute paths \n    \n    echo \"== Starting run at $(date)\"\n    echo \"== Job ID: ${SLURM_JOBID}\"\n    echo \"== Node list: ${SLURM_NODELIST}\"\n    echo \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\n    echo \"== Scratch dir. : ${TMPDIR}\"\n    \n    # cd $TMPDIR\n    # or change to a project folder with matlab file e.g. hello_World.m\n    # cd your/data\n\n    # Load matlab module\n    module load 2022 matlab/R2023a\n    \n    # execute\n    matlab -batch hello_world\n\n\n\n\n\n\n\nmathworks-parpool"
  },
  {
    "objectID": "topics/bazis.html#connect-to-vu-yodairods-with-icommands",
    "href": "topics/bazis.html#connect-to-vu-yodairods-with-icommands",
    "title": "BAZIS HPC",
    "section": "",
    "text": "iRODS/Yoda is a middleware and webinterface to store enriched data. On BAZIS the icommands are available to allow direct access.\nTo use the icommands a configuration file in your home folder is required. https://yoda.vu.nl/site/getting-started/icommands.html#configuration\nAfter configuration use iinit to connect, use a Data Access Password, similar to a WebDAV connection.\nFind an overview of the icommands here: https://docs.irods.org/master/icommands/user/ Check the ipwd, ils, icd, iput, iget en irsync commands for navigation and data transfer.\nYour project folder is located at /vu/home\nicommands Yoda-VU"
  },
  {
    "objectID": "topics/data-management-plan.html",
    "href": "topics/data-management-plan.html",
    "title": "Data Management Plan",
    "section": "",
    "text": "A Data Management Plan (DMP) is a document outlining how research data will be handled throughout the research lifecycle. A DMP is a structured way to address data collection, organization, storage, sharing, and preservation. It also outlines the measures taken to ensure data security and addresses how data will be preserved and made available for future use.\n\n\n\nVU Amsterdam offers the online tool DMPonline for writing Data Management Plans. DMPonline is a platform that offers a range of templates, ensuring that researchers can create DMPs to meet the standards of diverse funders and institutions associated with their projects. DMPonline makes it easy to work on a DMP together with colleagues, advisors, or other stakeholders. VU Amsterdam researchers can use the request feedback function of DMPonline to get their DMP reviewed by a faculty data steward or RDM Support Desk colleague.\nIf you have questions about DMPonline, or encounter problems when using the tool, please get in touch with rdm@vu.nl.\n\n\n\nVarious templates exist in which you can set up your DMP. We strongly recommend that you use the VU template, which is called VU DMP template 2021 (NWO & ZonMw certified) v1.4. Below you’ll find an explanation of how to access this template. If you need to write a DMP for funding agencies NWO, ZonMw or ERC, you can use the VU template as well.\n\n\nYou can find the VU DMP template in DMPonline. It includes concise guidance on how to complete your DMP.\nYou can select the VU template by taking the following steps (see also the picture below).\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nFor the question on primary funding organisation, select the check box on the right, saying that no funder is associated with your plan.\n\nNote: Follow these steps as well if you receive funding from NWO or ZonMw (see also below).\n\nIf you’re aiming to write a full DMP based on the VU DMP template, please make sure you don’t select the GDPR registration form.\n\n\n\n\nWe recommend researchers to use the VU DMP template whenever possible, especially for researchers who work with personal data. The VU DMP template includes questions that serve as input for the GDPR record of processing activities. This means that when you write a DMP based on the VU DMP template, you simultaneousely comply with the VU requirement to register the personal data you use in your research.\nHowever, it is also possible to use other templates in DMPonline. If your funder or partner organization requires you to use a certain template, it is possible to select that template in DMPonline. Please follow the steps below to select a funder’s template.\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nIn the field under Select the primary funding organisation, start typing the name of your funder and select their template.\n\n\nResearchers who don’t work with personal data and who wish to use another DMP template than the VU template, can also follow the steps above.\n\n\n\n\n\n\nIf your research is subject to the GDPR, then you need to register information on your research in a central VU registry. This central registry lists all personal data processing activities carried out at the VU. The registry indicates why and how personal data are processed, and with whom they are shared. The registry helps the VU demonstrate compliance with the GDPR and in the case of a data breach, the registry helps with monitoring and acting swiftly to inform all relevant stakeholders.\nFor research projects, the VU registers data processing via DMPonline. You can create your registration by logging into DMPonline and following the following instructions:\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nFor the question on primary funding organisation, select the check box on the right, saying that no funder is associated with your plan.\n\n\nOnce you get to the two VU templates, you can fill in the VU DMP template 2021 v1.4 if you need to write a DMP anyway; the information you include in this DMP template will be used for the registry. If you don’t need to write a (new) DMP, you can use the separate VU GDPR registration form for research v1.1. Your faculty’s Privacy Champion can help you with your registration.\nIf your research is primarily led by Amsterdam UMC, location VUmc, your research will be registered using their own separate system.\n\n\n\nIf you use personal data in your research, you should register your data processing activities before you start data collection. If you are not sure whether your research data are subject to the GDPR, contact your faculty’s Privacy Champion. Your privacy champion can also assist you if your research is already running, but has not yet been registered.\n\n\n\n\nResearch data is any information that has been collected, observed, generated or created to validate original research findings. Examples of data could be interview recordings, experiment results, physical measurement, notes from focus group’s meetings, notes from fieldwork, observations captured in photographs, film or audio, text files extracted from a corpus, image of archival items or artworks, scraped websites, responses to survey questions. Algorithms, simulations, code, scripts and software are often also considered as research data. There is also physical data: (biological) samples, collections, artifacts etc.\nAdministrative documents, like informed consent forms and key files should be acknowledged as important elements of research data as well.\n\n\n\nAt the VU, we sometimes use the term ‘Data Assets’. You can think of data assets as small ‘parcels’ of data that can change form or format throughout the research. For example, if you’re sending out surveys for your research, the survey responses are considered a data asset. If, in addition to the surveys, you’re also holding focus groups, the data collected from the focus group are also considered a data asset, separate from the survey results. Most projects will have more than one data asset per data stage. It is common to provide data assets based on the data stage such as raw, processed, or analysed. Raw Data refers to original data collected, Processed Data is data that has undergone some level of transformation or organisation. Processing involves cleaning, formatting, and structuring raw data to make them more understandable and suitable for analysis. Analysed Data usually results from statistical methods, detailed examination or interpretation.\nHere are some examples of data assets in research data management:\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nType of data\nFormat\n\n\n\n\nRaw data\nInterviews\nAudio files\nMP3\n\n\n\nSpectographic analysis\nText files\nCSV\n\n\nProcessed data\nTranscription of interviews\nText files\nDocx\n\n\n\nData spreadsheet\nSPSS files\nSAV\n\n\nAnalysed data\nRegression graphic\nGraph\nPNG\n\n\n\nData table\nWord file\nDocx\n\n\nOther\nPoster presentation\nPowerpoint\nPPS\n\n\n\nProject Website\nHTML\n\n\n\n\nAnalysis code\nText files\nPython\n\n\n\nNote that these data assets also change in the different phases of the research! While the interview data are audio files in the raw stage, they are transcribed and become text files in the processed stage.\n\n\n\nThe VU DMP template consists of seven sections with questions. In DMPonline, there is guidance available for all sections, as well as example answers. When you are writing your DMP, you can consult this information directly in DMPonline. Below we provide references to information and support available for various RDM-related aspects.\n\n\nIf you have questions about working with personal data in research, please get in touch with the Privacy Champion of your faculty. The overview of Privacy Champions can be found on the VU website. Make sure to contact your Privacy Champion in the following situations:\n\nIf you need to carry out a DPIA, or if you’re unsure if you need to do one\nIf you work with special category personal data, or otherwise very sensitive data\nIf you are collaborating with other parties\nIf you need software for which no licence is set up on behalf of the VU\nIf you wish to reuse existing data containing personal data\n\nIt is impossible to provide an overview of tasks to be carried out to ensure compliance with the GDPR that fits all research projects. For that reason, it is important to contact your Privacy Champion. They will be able to identify what needs to be arranged to adhere to the GDPR.\nEthical aspects of research should be addressed in the ethics procedure of your faculty. Each faculty has their own ethics committee. The webpages of all committees are listed below. Please go to the page of the ethics committee of your faculty to find instructions for ethical review procedures for your study.\n\nACTA: ACTA Institutional Review Board (IRB)\nBeta: Research ethics review committee Faculty of Science (BETHCIE)\nFGB: Scientific and Ethical Review Board (VCWE)\nFGW: Ethische Toetsingscommissie Onderzoek (EtCO)\nFSW: Research Ethics Review Committee (RERC)\nRCH (Faculty of Law): Ethics Committee\nSBE: Research Ethics and Integrity\nVUmc: METc (Medical Ethical Review Committee)\n\n\n\n\nAn overview of storage facilities at the VU is available in the Data Storage Finder. You can use this as a starting point to navigate storage solutions.\nIf you have questions about data storage and backup, send an email to rdm@vu.nl.\n\n\n\nIf your research data contains personal data and you’re unsure about which data may be published, please contact your Privacy Champion."
  },
  {
    "objectID": "topics/data-management-plan.html#what-is-a-dmp",
    "href": "topics/data-management-plan.html#what-is-a-dmp",
    "title": "Data Management Plan",
    "section": "",
    "text": "A Data Management Plan (DMP) is a document outlining how research data will be handled throughout the research lifecycle. A DMP is a structured way to address data collection, organization, storage, sharing, and preservation. It also outlines the measures taken to ensure data security and addresses how data will be preserved and made available for future use."
  },
  {
    "objectID": "topics/data-management-plan.html#dmponline",
    "href": "topics/data-management-plan.html#dmponline",
    "title": "Data Management Plan",
    "section": "",
    "text": "VU Amsterdam offers the online tool DMPonline for writing Data Management Plans. DMPonline is a platform that offers a range of templates, ensuring that researchers can create DMPs to meet the standards of diverse funders and institutions associated with their projects. DMPonline makes it easy to work on a DMP together with colleagues, advisors, or other stakeholders. VU Amsterdam researchers can use the request feedback function of DMPonline to get their DMP reviewed by a faculty data steward or RDM Support Desk colleague.\nIf you have questions about DMPonline, or encounter problems when using the tool, please get in touch with rdm@vu.nl."
  },
  {
    "objectID": "topics/data-management-plan.html#choosing-the-right-template",
    "href": "topics/data-management-plan.html#choosing-the-right-template",
    "title": "Data Management Plan",
    "section": "",
    "text": "Various templates exist in which you can set up your DMP. We strongly recommend that you use the VU template, which is called VU DMP template 2021 (NWO & ZonMw certified) v1.4. Below you’ll find an explanation of how to access this template. If you need to write a DMP for funding agencies NWO, ZonMw or ERC, you can use the VU template as well.\n\n\nYou can find the VU DMP template in DMPonline. It includes concise guidance on how to complete your DMP.\nYou can select the VU template by taking the following steps (see also the picture below).\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nFor the question on primary funding organisation, select the check box on the right, saying that no funder is associated with your plan.\n\nNote: Follow these steps as well if you receive funding from NWO or ZonMw (see also below).\n\nIf you’re aiming to write a full DMP based on the VU DMP template, please make sure you don’t select the GDPR registration form.\n\n\n\n\nWe recommend researchers to use the VU DMP template whenever possible, especially for researchers who work with personal data. The VU DMP template includes questions that serve as input for the GDPR record of processing activities. This means that when you write a DMP based on the VU DMP template, you simultaneousely comply with the VU requirement to register the personal data you use in your research.\nHowever, it is also possible to use other templates in DMPonline. If your funder or partner organization requires you to use a certain template, it is possible to select that template in DMPonline. Please follow the steps below to select a funder’s template.\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nIn the field under Select the primary funding organisation, start typing the name of your funder and select their template.\n\n\nResearchers who don’t work with personal data and who wish to use another DMP template than the VU template, can also follow the steps above."
  },
  {
    "objectID": "topics/data-management-plan.html#register-your-processing-activities",
    "href": "topics/data-management-plan.html#register-your-processing-activities",
    "title": "Data Management Plan",
    "section": "",
    "text": "If your research is subject to the GDPR, then you need to register information on your research in a central VU registry. This central registry lists all personal data processing activities carried out at the VU. The registry indicates why and how personal data are processed, and with whom they are shared. The registry helps the VU demonstrate compliance with the GDPR and in the case of a data breach, the registry helps with monitoring and acting swiftly to inform all relevant stakeholders.\nFor research projects, the VU registers data processing via DMPonline. You can create your registration by logging into DMPonline and following the following instructions:\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nFor the question on primary funding organisation, select the check box on the right, saying that no funder is associated with your plan.\n\n\nOnce you get to the two VU templates, you can fill in the VU DMP template 2021 v1.4 if you need to write a DMP anyway; the information you include in this DMP template will be used for the registry. If you don’t need to write a (new) DMP, you can use the separate VU GDPR registration form for research v1.1. Your faculty’s Privacy Champion can help you with your registration.\nIf your research is primarily led by Amsterdam UMC, location VUmc, your research will be registered using their own separate system.\n\n\n\nIf you use personal data in your research, you should register your data processing activities before you start data collection. If you are not sure whether your research data are subject to the GDPR, contact your faculty’s Privacy Champion. Your privacy champion can also assist you if your research is already running, but has not yet been registered."
  },
  {
    "objectID": "topics/data-management-plan.html#what-is-data",
    "href": "topics/data-management-plan.html#what-is-data",
    "title": "Data Management Plan",
    "section": "",
    "text": "Research data is any information that has been collected, observed, generated or created to validate original research findings. Examples of data could be interview recordings, experiment results, physical measurement, notes from focus group’s meetings, notes from fieldwork, observations captured in photographs, film or audio, text files extracted from a corpus, image of archival items or artworks, scraped websites, responses to survey questions. Algorithms, simulations, code, scripts and software are often also considered as research data. There is also physical data: (biological) samples, collections, artifacts etc.\nAdministrative documents, like informed consent forms and key files should be acknowledged as important elements of research data as well."
  },
  {
    "objectID": "topics/data-management-plan.html#data-assets",
    "href": "topics/data-management-plan.html#data-assets",
    "title": "Data Management Plan",
    "section": "",
    "text": "At the VU, we sometimes use the term ‘Data Assets’. You can think of data assets as small ‘parcels’ of data that can change form or format throughout the research. For example, if you’re sending out surveys for your research, the survey responses are considered a data asset. If, in addition to the surveys, you’re also holding focus groups, the data collected from the focus group are also considered a data asset, separate from the survey results. Most projects will have more than one data asset per data stage. It is common to provide data assets based on the data stage such as raw, processed, or analysed. Raw Data refers to original data collected, Processed Data is data that has undergone some level of transformation or organisation. Processing involves cleaning, formatting, and structuring raw data to make them more understandable and suitable for analysis. Analysed Data usually results from statistical methods, detailed examination or interpretation.\nHere are some examples of data assets in research data management:\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nType of data\nFormat\n\n\n\n\nRaw data\nInterviews\nAudio files\nMP3\n\n\n\nSpectographic analysis\nText files\nCSV\n\n\nProcessed data\nTranscription of interviews\nText files\nDocx\n\n\n\nData spreadsheet\nSPSS files\nSAV\n\n\nAnalysed data\nRegression graphic\nGraph\nPNG\n\n\n\nData table\nWord file\nDocx\n\n\nOther\nPoster presentation\nPowerpoint\nPPS\n\n\n\nProject Website\nHTML\n\n\n\n\nAnalysis code\nText files\nPython\n\n\n\nNote that these data assets also change in the different phases of the research! While the interview data are audio files in the raw stage, they are transcribed and become text files in the processed stage."
  },
  {
    "objectID": "topics/data-management-plan.html#dmp-elements",
    "href": "topics/data-management-plan.html#dmp-elements",
    "title": "Data Management Plan",
    "section": "",
    "text": "The VU DMP template consists of seven sections with questions. In DMPonline, there is guidance available for all sections, as well as example answers. When you are writing your DMP, you can consult this information directly in DMPonline. Below we provide references to information and support available for various RDM-related aspects.\n\n\nIf you have questions about working with personal data in research, please get in touch with the Privacy Champion of your faculty. The overview of Privacy Champions can be found on the VU website. Make sure to contact your Privacy Champion in the following situations:\n\nIf you need to carry out a DPIA, or if you’re unsure if you need to do one\nIf you work with special category personal data, or otherwise very sensitive data\nIf you are collaborating with other parties\nIf you need software for which no licence is set up on behalf of the VU\nIf you wish to reuse existing data containing personal data\n\nIt is impossible to provide an overview of tasks to be carried out to ensure compliance with the GDPR that fits all research projects. For that reason, it is important to contact your Privacy Champion. They will be able to identify what needs to be arranged to adhere to the GDPR.\nEthical aspects of research should be addressed in the ethics procedure of your faculty. Each faculty has their own ethics committee. The webpages of all committees are listed below. Please go to the page of the ethics committee of your faculty to find instructions for ethical review procedures for your study.\n\nACTA: ACTA Institutional Review Board (IRB)\nBeta: Research ethics review committee Faculty of Science (BETHCIE)\nFGB: Scientific and Ethical Review Board (VCWE)\nFGW: Ethische Toetsingscommissie Onderzoek (EtCO)\nFSW: Research Ethics Review Committee (RERC)\nRCH (Faculty of Law): Ethics Committee\nSBE: Research Ethics and Integrity\nVUmc: METc (Medical Ethical Review Committee)\n\n\n\n\nAn overview of storage facilities at the VU is available in the Data Storage Finder. You can use this as a starting point to navigate storage solutions.\nIf you have questions about data storage and backup, send an email to rdm@vu.nl.\n\n\n\nIf your research data contains personal data and you’re unsure about which data may be published, please contact your Privacy Champion."
  },
  {
    "objectID": "topics/fair-principles.html",
    "href": "topics/fair-principles.html",
    "title": "FAIR Principles",
    "section": "",
    "text": "FAIR Principles\nThe purpose of good data documentation is to facilitate knowledge exchange. The FAIR principles (Wilkinson et al. 2016) are the guidelines for every form of research output, and in making data available to others.\nEvery research group should apply these principles when structuring and preserving their data or other research outputs. Making data FAIR involves many aspects of data collection, organization, availability and storage. FAIR does not mean open or public, it means that there is documentation about how access to the data can be achieved.\n\n\n\n\n\nReferences\n\nWilkinson, Mark D, Michel Dumontier, Ijsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Sci. Data 3 (1): 160018."
  }
]